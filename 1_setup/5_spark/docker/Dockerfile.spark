# Dockerfile to build the image for an app that can be launched via spark-submit or the spark operator
# the code to be executed can either be directly baked into the image or mounted via nfs, s3 or git
# it is a layer on top of the official Spark Image
# The Image has the following add ons
# Driver for
# - Postgres, s3, Delta, Iceberg, Hudi
# Python Packages for
# - Jupyter, Pandas, Numpy, Matplotlib
# Debug Tools for
# - Network, s3


FROM apache/spark-py:v3.3.2
WORKDIR /

# Reset to root to run installation tasks
USER 0

RUN mkdir ${SPARK_HOME}/work-dir/jobs
# copy script that should be submitted into work-dir src, no need when mounting nfs
# COPY spark/jobs ${SPARK_HOME}/work-dir/jobs
RUN apt-get update && \ 
    apt-get install -y procps zip dnsutils s3cmd curl nano

RUN python3 -m pip install --upgrade pip \
    && python3 -m pip install psycopg2-binary \
    && python3 -m pip install chispa \
    && python3 -m pip install jupyter \
    && python3 -m pip install jupyterlab \
    && python3 -m pip install numpy \
    && python3 -m pip install pandas \
    && python3 -m pip install matplotlib \
    && python3 -m pip install delta-spark \
    && python3 -m pip install s3fs


COPY all_jars ${SPARK_HOME}/jars
#COPY dockerfiles/entrypoint.sh /opt/

# start original entypoint srcipt to launch container as spark worker
WORKDIR /opt/spark/work-dir
ENTRYPOINT [ "/opt/entrypoint.sh" ]

# Specify the User that the actual main process will run as
ARG spark_uid=185
USER ${spark_uid}
