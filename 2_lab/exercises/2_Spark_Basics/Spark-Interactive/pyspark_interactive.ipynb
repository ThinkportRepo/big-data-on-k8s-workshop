{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2e7e76d1",
   "metadata": {},
   "source": [
    "# Einfaches Spark Programm\n",
    "Die einzelnen Zellen mit `shift` + `enter` starten und so den Code ausführen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3819c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "\n",
    "# use 95% of the screen for jupyter cell\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container {width:100% !important; }<style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6f1636",
   "metadata": {},
   "outputs": [],
   "source": [
    "appName=\"jupyter-basic\"\n",
    "\n",
    "conf = SparkConf()\n",
    "\n",
    "# CLUSTER MANAGER\n",
    "################################################################################\n",
    "# set Kubernetes Master as Cluster Manager(“k8s://https://” is NOT a typo, this is how Spark knows the “provider” type).\n",
    "conf.setMaster(\"k8s://https://kubernetes.default.svc.cluster.local:443\")\n",
    "\n",
    "# CONFIGURE KUBERNETES\n",
    "################################################################################\n",
    "# set the namespace that will be used for running the driver and executor pods.\n",
    "conf.set(\"spark.kubernetes.namespace\",\"frontend\")\n",
    "# set the docker image from which the Worker pods are created\n",
    "conf.set(\"spark.kubernetes.container.image\", \"thinkportgmbh/workshops:spark-3.3.2\")\n",
    "conf.set(\"spark.kubernetes.container.image.pullPolicy\", \"Always\")\n",
    "\n",
    "# set service account to be used\n",
    "conf.set(\"spark.kubernetes.authenticate.driver.serviceAccountName\", \"spark\")\n",
    "# authentication for service account(required to create worker pods):\n",
    "conf.set(\"spark.kubernetes.authenticate.caCertFile\", \"/var/run/secrets/kubernetes.io/serviceaccount/ca.crt\")\n",
    "conf.set(\"spark.kubernetes.authenticate.oauthTokenFile\", \"/var/run/secrets/kubernetes.io/serviceaccount/token\")\n",
    "\n",
    "\n",
    "# CONFIGURE SPARK\n",
    "################################################################################\n",
    "# set driver host. In this case the ingres service for the spark driver\n",
    "# find name of the driver service with 'kubectl get services' or in the helm chart configuration\n",
    "conf.set(\"spark.driver.host\", \"jupyter-spark-driver.frontend.svc.cluster.local\")\n",
    "# set the port, If this port is busy, spark-shell tries to bind to another port.\n",
    "conf.set(\"spark.driver.port\", \"29413\")\n",
    "# add the postgres driver jars into session\n",
    "\n",
    "# CONFIGURE S3 CONNECTOR\n",
    "conf.set(\"spark.hadoop.fs.s3a.endpoint\", \"minio.minio.svc.cluster.local:9000\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.access.key\", \"trainadm\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.secret.key\", \"train@thinkport\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "conf.set(\"spark.hadoop.fs.s3.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "\n",
    "\n",
    "# CONFIGURE WORKER (Customize based on workload)\n",
    "################################################################################\n",
    "# set number of worker pods\n",
    "conf.set(\"spark.executor.instances\", \"1\")\n",
    "# set memory of each worker pod\n",
    "conf.set(\"spark.executor.memory\", \"1G\")\n",
    "# set cpu of each worker pod\n",
    "conf.set(\"spark.executor.cores\", \"2\")\n",
    "# Number of possible tasks = cores * executores\n",
    "\n",
    "# SPARK SESSION\n",
    "################################################################################\n",
    "# and last, create the spark session and pass it the config object\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .config(conf=conf) \\\n",
    "    .config('spark.sql.session.timeZone', 'Europe/Berlin') \\\n",
    "    .appName(appName)\\\n",
    "    .getOrCreate()\n",
    "\n",
    "# also get the spark context\n",
    "sc=spark.sparkContext\n",
    "# change the log level to warning, to see less output\n",
    "sc.setLogLevel('WARN')\n",
    "\n",
    "# get the configuration object to check all the configurations the session was started with\n",
    "for entry in sc.getConf().getAll():\n",
    "        if entry[0] in [\"spark.app.name\",\"spark.kubernetes.namespace\",\"spark.executor.memory\",\"spark.executor.cores\",\"spark.driver.host\",\"spark.master\"]:\n",
    "            print(entry[0],\"=\",entry[1])\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21cc964",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################\n",
    "# create simple dataframe and show schema and data\n",
    "##################################################################\n",
    "simpleData = [\n",
    "    (\"James\",\"Sales\",\"NY\",90000,34,10000),\n",
    "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000),\n",
    "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000),\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000),\n",
    "    (\"Raman\",\"Finance\",\"CA\",99000,40,24000),\n",
    "    (\"Scott\",\"Finance\",\"NY\",83000,36,19000),\n",
    "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000),\n",
    "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000),\n",
    "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000)\n",
    "    ]\n",
    "\n",
    "schema = [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
    "df = spark.createDataFrame(data=simpleData, schema = schema)\n",
    "\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff4d664",
   "metadata": {},
   "outputs": [],
   "source": [
    "## show distinct departments\n",
    "df.select(\"department\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31aec66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## sum salery by department\n",
    "df.groupBy(\"department\").sum(\"salary\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8db9c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run complex aggregation\n",
    "result=(df\n",
    "    .groupBy(\"department\")\n",
    "    .agg(\n",
    "        f.sum(\"salary\").alias(\"sum_salary\"),\n",
    "        f.avg(\"salary\").alias(\"avg_salary\"),\n",
    "        f.sum(\"bonus\").alias(\"sum_bonus\"),\n",
    "        f.max(\"bonus\").alias(\"max_bonus\")\n",
    "    )\n",
    ")\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2886f22d",
   "metadata": {},
   "source": [
    "Prüfen ob das Bucket `s3://data/` schon existiert. Ansonsten zuerst die Daten von lokal nach s3 landen.   \n",
    "Hierzu im Terminal von VSCode folgende Befehle ausführen\n",
    "```\n",
    "s3 ls \n",
    "s3 mb s3://data\n",
    "s3 put /home/coder/git/2_lab/data/ s3://data --recursive\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb0810e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read singel file from s3 (city_value_histogram)\n",
    "# read simple csv\n",
    "df=(spark\n",
    "    .read\n",
    "    .option(\"delimiter\",\";\")\n",
    "    .csv(\"s3a://data/simple/city_value_histogram.csv\",inferSchema=True,header=True)\n",
    "    )\n",
    "df.show(3)\n",
    "df.describe().show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0322c52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7afac50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
