{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "dbff7e9a",
      "metadata": {},
      "source": [
        "# Spark Streaming Transform Data\n",
        "read data from Kafka topic, filter and reduce and write back to other Kafka Topic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba0572c8",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import SQLContext\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.streaming import StreamingContext\n",
        "import pyspark.sql.functions as f\n",
        "\n",
        "\n",
        "import datetime\n",
        "from datetime import datetime\n",
        "import json\n",
        "\n",
        "\n",
        "# use 95% of the screen for jupyter cell\n",
        "from IPython.core.display import display, HTML\n",
        "display(HTML(\"<style>.container {width:100% !important; }<style>\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c618dd27",
      "metadata": {},
      "outputs": [],
      "source": [
        "appName=\"jupyter-stream\"\n",
        "\n",
        "conf = SparkConf()\n",
        "\n",
        "# CLUSTER MANAGER\n",
        "################################################################################\n",
        "# set Kubernetes Master as Cluster Manager(“k8s://https://” is NOT a typo, this is how Spark knows the “provider” type).\n",
        "conf.setMaster(\"k8s://https://kubernetes.default.svc.cluster.local:443\")\n",
        "\n",
        "# CONFIGURE KUBERNETES\n",
        "################################################################################\n",
        "# set the namespace that will be used for running the driver and executor pods.\n",
        "conf.set(\"spark.kubernetes.namespace\",\"frontend\")\n",
        "# set the docker image from which the Worker pods are created\n",
        "conf.set(\"spark.kubernetes.container.image\", \"thinkportgmbh/workshops:spark-3.3.1\")\n",
        "conf.set(\"spark.kubernetes.container.image.pullPolicy\", \"Always\")\n",
        "\n",
        "# set service account to be used\n",
        "conf.set(\"spark.kubernetes.authenticate.driver.serviceAccountName\", \"spark\")\n",
        "# authentication for service account(required to create worker pods):\n",
        "conf.set(\"spark.kubernetes.authenticate.caCertFile\", \"/var/run/secrets/kubernetes.io/serviceaccount/ca.crt\")\n",
        "conf.set(\"spark.kubernetes.authenticate.oauthTokenFile\", \"/var/run/secrets/kubernetes.io/serviceaccount/token\")\n",
        "\n",
        "\n",
        "# CONFIGURE SPARK\n",
        "################################################################################\n",
        "conf.set(\"spark.sql.adaptive.enabled\", \"False\")\n",
        "# set driver host. In this case the ingres service for the spark driver\n",
        "# find name of the driver service with 'kubectl get services' or in the helm chart configuration\n",
        "conf.set(\"spark.driver.host\", \"jupyter-spark-driver.frontend.svc.cluster.local\")\n",
        "# set the port, If this port is busy, spark-shell tries to bind to another port.\n",
        "conf.set(\"spark.driver.port\", \"29413\")\n",
        "# add the postgres driver jars into session\n",
        "conf.set(\"spark.jars\", \"/opt/spark/jars/spark-sql-kafka-0-10_2.12-3.3.1.jar, /opt/spark/jars/kafka-clients-3.3.1.jar, /opt/spark/jars/spark-avro_2.12-3.3.1.jar\")\n",
        "conf.set(\"spark.driver.extraClassPath\",\"/opt/spark/jars/spark-sql-kafka-0-10_2.12-3.3.1.jar, /opt/spark/jars/kafka-clients-3.3.1.jar, /opt/spark/jars/spark-avro_2.12-3.3.1.jar\")\n",
        "conf.set(\"spark.executor.extraClassPath\",\"/opt/spark/jars/spark-sql-kafka-0-10_2.12-3.3.1.jar, /opt/spark/jars/kafka-clients-3.3.1.jar, /opt/spark/jars/spark-avro_2.12-3.3.1.jar\")\n",
        "#conf.set(\"spark.executor.extraLibrary\",\"/opt/spark/jars/spark-sql-kafka-0-10_2.12-3.3.1.jar, /opt/spark/jars/kafka-clients-3.3.1.jar\")\n",
        "\n",
        "\n",
        "\n",
        "# CONFIGURE S3 CONNECTOR\n",
        "conf.set(\"spark.hadoop.fs.s3a.endpoint\", \"minio.minio.svc.cluster.local:9000\")\n",
        "conf.set(\"spark.hadoop.fs.s3a.access.key\", \"trainadm\")\n",
        "conf.set(\"spark.hadoop.fs.s3a.secret.key\", \"train@thinkport\")\n",
        "conf.set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
        "conf.set(\"spark.hadoop.fs.s3.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
        "conf.set(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
        "conf.set(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
        "\n",
        "\n",
        "\n",
        "# conf.set(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.1\")\n",
        "\n",
        "# CONFIGURE WORKER (Customize based on workload)\n",
        "################################################################################\n",
        "# set number of worker pods\n",
        "conf.set(\"spark.executor.instances\", \"1\")\n",
        "# set memory of each worker pod\n",
        "conf.set(\"spark.executor.memory\", \"1G\")\n",
        "# set cpu of each worker pod\n",
        "conf.set(\"spark.executor.cores\", \"2\")\n",
        "\n",
        "# SPARK SESSION\n",
        "################################################################################\n",
        "# and last, create the spark session and pass it the config object\n",
        "\n",
        "spark = SparkSession\\\n",
        "    .builder\\\n",
        "    .config(conf=conf) \\\n",
        "    .config('spark.sql.session.timeZone', 'Europe/Berlin') \\\n",
        "    .appName(appName)\\\n",
        "    .getOrCreate()\n",
        "\n",
        "# also get the spark context\n",
        "sc=spark.sparkContext\n",
        "ssc = StreamingContext(sc, 2)\n",
        "\n",
        "# change the log level to warning, to see less output\n",
        "sc.setLogLevel('ERROR')\n",
        "\n",
        "# get the configuration object to check all the configurations the session was startet with\n",
        "for entry in sc.getConf().getAll():\n",
        "        if entry[0] in [\"spark.app.name\",\"spark.kubernetes.namespace\",\"spark.executor.memory\",\"spark.executor.cores\",\"spark.driver.host\",\"spark.master\"]:\n",
        "            print(entry[0],\"=\",entry[1])\n",
        "            \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b436a47f",
      "metadata": {},
      "source": [
        "#### Read Stream from Kafka"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31316ce9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# read stream from topic\n",
        "df_step_1 = (spark\n",
        "      .readStream\n",
        "      .format(\"kafka\")\n",
        "      .option(\"kafka.bootstrap.servers\", \"kafka.kafka.svc.cluster.local:9092\")\n",
        "      .option(\"subscribe\", \"twitter-table\")\n",
        "      .option(\"startingOffsets\", \"earliest\")\n",
        "      .load()\n",
        "     )\n",
        "\n",
        "df_step_1.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8dbff5a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# schema des JSON Streams definieren\n",
        "jsonSchema=StructType([\n",
        "    StructField('tweet_id', StringType(), False),\n",
        "    StructField('created_at', TimestampType(), False),\n",
        "    StructField('tweet_message', StringType(), True),\n",
        "    StructField('user_name', StringType(), True),\n",
        "    StructField('user_location', StringType(), True),\n",
        "    StructField('user_follower_count', IntegerType(), True),\n",
        "    StructField('user_friends_count', IntegerType(), True),\n",
        "    StructField('retweet_count', IntegerType(), True),\n",
        "    StructField('language', StringType(), True),\n",
        "    StructField('hashtags', ArrayType(StringType(), True), True)\n",
        "])\n",
        "\n",
        "df_step_2= (df_step_1\n",
        "            # cast binary to string and string with json schema to json object\n",
        "            .select(f.from_json(f.col(\"value\").cast(\"string\"),jsonSchema).alias(\"t\"))\n",
        "            # un nest via\n",
        "            .select(\"t.*\")\n",
        "           )\n",
        "\n",
        "df_step_2.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4c2e5fa",
      "metadata": {},
      "source": [
        "#### For Debugging: write stream to console"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c9ab96e",
      "metadata": {},
      "outputs": [],
      "source": [
        "stream_query_debug=(df_step_2\n",
        "                    .writeStream.format(\"console\")\n",
        "                    .option(\"truncate\", \"false\")\n",
        "                    .outputMode(\"append\")\n",
        "                    .start()\n",
        "                    .awaitTermination()\n",
        "                   )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "119c7409",
      "metadata": {},
      "outputs": [],
      "source": [
        "s3_write_avro=(df_step_2\n",
        "          .writeStream\n",
        "          .format(\"avro\")\n",
        "          .outputMode(\"append\")\n",
        "          .option(\"path\", \"s3a://twitter/avro\")\n",
        "          .option(\"checkpointLocation\", \"/opt/spark/work-dir/\")\n",
        "          .trigger(processingTime='10 seconds')\n",
        "          .start()\n",
        "          .awaitTermination()\n",
        "         )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3916062d-5cde-4156-a712-ffd241dd2319",
      "metadata": {},
      "source": [
        "# BITTE Spark herunterfahren"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76213f4b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Terminate Spark Session\n",
        "# shut down executor pods\n",
        "spark.stop()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (Pyodide)",
      "language": "python",
      "name": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
