{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5765a06e",
   "metadata": {},
   "source": [
    "# Spark Aufgaben\n",
    "1. Importe laden\n",
    "2. Jupyter Spark starten und Twitter-Streams von Avro lesen\n",
    "3. ETL Strecke: Avro Daten einlesen und als Delta Datei wieder raus schreiben\n",
    "4. Analyse-Aufgaben erledigen \n",
    "5. Verlaufsanalyse durchführen\n",
    "6. **Ausschalten der Spark-App**\n",
    "\n",
    "## Wichtige Hinweise\n",
    "1. Führe alle Anweisungen in der vorgegebenen Reihenfolge aus. Die einzelnen Programmierzellen bauen aufeinander auf.\n",
    "2. **Beende unbedingt am Ende die Spark-Anwendung mit dem untersten Befehl \"spark.stop()\" , wenn du aufhörst an den Daten zu arbeiten.**\n",
    "3. Du kannst jederzeit das Notebook wieder hochfahren, wenn du Schritt 1 & 2 (Laden der Imports & Jupyter Spark und seine Konfigurationen hochfahren) ausführen.\n",
    "4. Mit **\"Strg\" + \"Enter\"** führst du einzelne Zellen direkt aus.\n",
    "5. In der oberen Leiste kannst du über **\"Insert\"** weitere Zellen hinzufügen, um weitere Test-Funktionen zu schreiben. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c9d1f3",
   "metadata": {},
   "source": [
    "## 1. Laden der Imports\n",
    "Hier werden alle benötigten Libraries für dieses Lab heruntergeladen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebe3ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import lower, col\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "from delta import *\n",
    "\n",
    "\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "\n",
    "# use 95% of the screen for jupyter cell\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container {width:100% !important; }<style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2579b387",
   "metadata": {},
   "source": [
    "## 2. Jupyter Spark & Konfigurationen hochfahren\n",
    "Hier wird die App jupyter-spark konfiguriert und hochgefahren, welche unsere weiteren Schritte ausführt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d15c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "appName=\"jupyter-analytics\"\n",
    "\n",
    "conf = SparkConf()\n",
    "\n",
    "# CLUSTER MANAGER\n",
    "################################################################################\n",
    "# set Kubernetes Master as Cluster Manager(“k8s://https://” is NOT a typo, this is how Spark knows the “provider” type).\n",
    "conf.setMaster(\"k8s://https://kubernetes.default.svc.cluster.local:443\")\n",
    "\n",
    "# CONFIGURE KUBERNETES\n",
    "################################################################################\n",
    "# set the namespace that will be used for running the driver and executor pods.\n",
    "conf.set(\"spark.kubernetes.namespace\",\"frontend\")\n",
    "# set the docker image from which the Worker pods are created\n",
    "conf.set(\"spark.kubernetes.container.image\", \"thinkportgmbh/workshops:spark-3.3.2\")\n",
    "conf.set(\"spark.kubernetes.container.image.pullPolicy\", \"Always\")\n",
    "\n",
    "# set service account to be used\n",
    "conf.set(\"spark.kubernetes.authenticate.driver.serviceAccountName\", \"spark\")\n",
    "# authentication for service account(required to create worker pods):\n",
    "conf.set(\"spark.kubernetes.authenticate.caCertFile\", \"/var/run/secrets/kubernetes.io/serviceaccount/ca.crt\")\n",
    "conf.set(\"spark.kubernetes.authenticate.oauthTokenFile\", \"/var/run/secrets/kubernetes.io/serviceaccount/token\")\n",
    "\n",
    "\n",
    "# CONFIGURE SPARK\n",
    "################################################################################\n",
    "conf.set(\"spark.sql.session.timeZone\", \"Europe/Berlin\")\n",
    "# set driver host. In this case the ingres service for the spark driver\n",
    "# find name of the driver service with 'kubectl get services' or in the helm chart configuration\n",
    "conf.set(\"spark.driver.host\", \"jupyter-spark-driver.frontend.svc.cluster.local\")\n",
    "# set the port, If this port is busy, spark-shell tries to bind to another port.\n",
    "conf.set(\"spark.driver.port\", \"29413\")\n",
    "# add the postgres driver jars into session\n",
    "conf.set(\"spark.jars\", \"/opt/spark/jars/spark-avro_2.12-3.3.2.jar\")\n",
    "conf.set(\"spark.executor.extraClassPath\",\"/opt/spark/jars/spark-avro_2.12-3.3.2.jar\")\n",
    "#conf.set(\"spark.executor.extraLibrary\",\"/opt/spark/jars/spark-sql-kafka-0-10_2.12-3.3.1.jar, /opt/spark/jars/kafka-clients-3.3.1.jar\")\n",
    "#conf.set(\"spark.driver.extraClassPath\",\"/opt/spark/jars/spark-sql-kafka-0-10_2.12-3.3.1.jar, /opt/spark/jars/kafka-clients-3.3.1.jar, /opt/spark/jars/spark-avro_2.12-3.3.1.jar\")\n",
    "\n",
    "# CONFIGURE S3 CONNECTOR\n",
    "conf.set(\"spark.hadoop.fs.s3a.endpoint\", \"minio.minio.svc.cluster.local:9000\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.access.key\", \"trainadm\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.secret.key\", \"train@thinkport\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "conf.set(\"spark.hadoop.fs.s3.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "\n",
    "#conf.set(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension, org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions, org.apache.spark.sql.hudi.HoodieSparkSessionExtension\")\n",
    "conf.set(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "conf.set(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "# CONFIGURE WORKER (Customize based on workload)\n",
    "################################################################################\n",
    "# set number of worker pods\n",
    "conf.set(\"spark.executor.instances\", \"1\")\n",
    "# set memory of each worker pod\n",
    "conf.set(\"spark.executor.memory\", \"1G\")\n",
    "# set cpu of each worker pod\n",
    "conf.set(\"spark.executor.cores\", \"2\")\n",
    "# Number of possible tasks = cores * executores\n",
    "\n",
    "## Deltalake\n",
    "# conf.set(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "\n",
    "# SPARK SESSION\n",
    "################################################################################\n",
    "# and last, create the spark session and pass it the config object\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .config(conf=conf) \\\n",
    "    .config('spark.sql.session.timeZone', 'Europe/Berlin') \\\n",
    "    .appName(appName)\\\n",
    "    .getOrCreate()\n",
    "\n",
    "# also get the spark context\n",
    "sc=spark.sparkContext\n",
    "# change the log level to warning, to see less output\n",
    "sc.setLogLevel('ERROR')\n",
    "\n",
    "# get the configuration object to check all the configurations the session was startet with\n",
    "for entry in sc.getConf().getAll():\n",
    "        if entry[0] in [\"spark.app.name\",\"spark.kubernetes.namespace\",\"spark.executor.memory\",\"spark.executor.cores\",\"spark.driver.host\",\"spark.master\"]:\n",
    "            print(entry[0],\"=\",entry[1])\n",
    "            \n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f42ed16",
   "metadata": {},
   "source": [
    "## 3. Einlesen und Schreiben von Daten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cbac52",
   "metadata": {},
   "source": [
    "### 3.1 Einlesen der Avro Dateien von S3\n",
    "Laden der Daten aus dem Bucket \"s3a://twitter/avro\" in einen DataFrame, um auf den Daten zu arbeiten. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b65543",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avro=(spark\n",
    "    .read.format(\"avro\")\n",
    "    # Pfad zu Bucket\n",
    "    .load(\"s3a://twitter/avro\")\n",
    "    # repartition auf 20 um optimierter mit den wenigen cpu zu arbeiten\n",
    "    .repartition(20)\n",
    "   ).cache()\n",
    "\n",
    "\n",
    " # nur Tweets mit dem Hashtag BigData weiter verwenden\n",
    "df = df_avro.filter(f.array_contains(f.col(\"hashtags\"),\"BigData\")==True)\n",
    "\n",
    "print(\"Anzahl aller Tweets: \",df_avro.count())\n",
    "print(\"Anzahl Tweets mit BigData: \",df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f31f3ee",
   "metadata": {},
   "source": [
    "Kurz anschauen was da drin istErste Ausgabe der Daten in Form eines DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dec3929",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Anzahl aller Tweets: \",df_avro.count())\n",
    "print(\"Anzahl Tweets mit BigData: \",df.count())\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518be64b",
   "metadata": {},
   "source": [
    "## 4. Analyse-Aufgaben\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b79d70",
   "metadata": {},
   "source": [
    "### 4.1 Tweets anschauen und den Aufbau des Dataframes\n",
    "Schau dir den Datensatz einmal genau an. Welche Spalten gibt es? Welche Datentypen sind vorhanden?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406525a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.<<EIGENER CODE>>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4223d1",
   "metadata": {},
   "source": [
    "### 4.2  Das Schema des Datensatzes anzeigen \n",
    "<br>\n",
    "<code> df.printSchema()</code> gibt das Schema des Datensatzes aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196c1696",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc238b9",
   "metadata": {},
   "source": [
    "### 4.3 Zählen der Tweets pro Stunde\n",
    "Schreibe eine Abfrage, die **die Anzahl an Tweets pro Stunde** zählt.  \n",
    "Hilfreiche Dokumentation: https://spark.apache.org/docs/latest/sql-ref-syntax-qry-select-groupby.html\n",
    "<details hidden>\n",
    "<summary> &#8964 Tipp </summary>\n",
    "<code>df_hourly=(&ltdataFrame&gt\n",
    "            .withColumn(&ltrename_column&gt, f.hour(f.col(\"&ltcolumn&gt\")))\n",
    "            .groupBy(\"&ltrename_column&gt\")\n",
    "            .&ltfunctionToCount()&gt\n",
    "            .withColumnRenamed(\"count\",\"total\")\n",
    "            .sort(\"&ltrename_codf_top_user=(df\n",
    "                ...\n",
    "                )\n",
    "df_top_user.show()\n",
    "lumn&gt\")\n",
    "          )\n",
    "df_hourly.show(20)</code>\n",
    "</details>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1185327b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hourly=(df  \n",
    "            ...\n",
    "          )\n",
    "\n",
    "df_hourly.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0d5a8f",
   "metadata": {},
   "source": [
    "### 4.4 Top 10 User nach Tweet-Anzahl\n",
    "Schreibe eine Abfrage, die die **Top User** nach ihrer **Anzahl an Tweets** ausgibt. Bedenke dabei, deine Ausgabe auf **10** Einträge zu limitieren.\n",
    "<details hidden>\n",
    "<summary> &#8964 Tipp</summary>\n",
    "\n",
    "<code>df_top_user=(&ltdataFrame&gt\n",
    "                .groupBy(\"&ltcolumnA&gt\")\n",
    "                .agg(\n",
    "                    f.count(\"&ltcolumnA&gt\").alias(\"numberOfTweets\")\n",
    "                    )\n",
    "                .orderBy(f.col(\"&ltaggregatedColumn&gt\").desc())\n",
    "                .&ltfunctionTolimit(10)&gt\n",
    "                )\n",
    "df_top_user.show()</code>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a6ca1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top_user=(df\n",
    "                ...\n",
    "                )\n",
    "df_top_user.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75828392",
   "metadata": {},
   "source": [
    "### 4.5 Umgang mit Arrays\n",
    "Für die folgenden Aufgabe wird die <code>explode</code>-Funktion benötigt. Schreibe eine Abfrage die das Hashtag-array mit <code>explode</code> teilt. Gebe dabei die Spalten \"user_name\", \"tweet_id\"und die explodierte\"hashtags\"- Spalte mit einem Limit von 20 Zeilen aus. \n",
    "\n",
    "https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.functions.explode.html\n",
    "<details hidden>\n",
    "<summary> &#8964 Tipp </summary>\n",
    "<p>\n",
    "<code>df_hash=(df\n",
    "         .withColumn(\"&ltcolumnC&gt\",explode(\"&ltcolumnC&gt\"))\n",
    "        .limit(20)\n",
    "        .select(\"&ltcolumnA&gt\", \"&ltcolumnB&gt\", \"&ltcolumnC&gt\")\n",
    "        )\n",
    "df_hash.show()</code>\n",
    "</details>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd44fd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hash=(df\n",
    "         ...\n",
    "        )\n",
    "df_hash.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58765db1",
   "metadata": {},
   "source": [
    "### 4.6 Top 5 Hashtags der Top 10 User\n",
    "Schreibe eine Abfrage, die die **Top 5 der Hashtags** der **10 User** mit den **meisten Tweets** ausgibt.\n",
    "<br>\n",
    "<br>\n",
    "<details>\n",
    "<summary> &#8964 Hinweis </summary>\n",
    "<p>\n",
    "<code>df_top5_per_user=(&ltdataFrameA&gt\n",
    "            # filter via join\n",
    "            .join(&dataFrameBC&gt,[&ltdataFrameA&gt.&ltcolumnA&gt==&ltDataFrameB&gt.&ltcolumnB&gt],how=\"left\")\n",
    "            # hashtags array in Zeilen Einträge exploden\n",
    "            .withColumn(\"&ltcolumnC&gt\",explode(\"&ltcolumnC&gt\"))\n",
    "            # hashtags lowercase schreiben um Doppelungen zu entfernen\n",
    "            .withColumn(\"&ltcolumnC&gt\", lower(col('&ltcolumnC&gt')))\n",
    "            # groupieren und counten by hashtag\n",
    "            .&ltfunctionToGroup&gt(\"&ltcolumnC&gt\").agg(f.count(\"&ltcolumnC&gt\"))\n",
    "            # rückwärts sortieren\n",
    "            .&ltfunctionToSort&gt(f.col(\"count(&ltcolumnC&gt)\").desc())\n",
    "            # top 5 selectieren\n",
    "            .limit(5) \n",
    "                 )\n",
    "df_top5_per_user.show()</code>\n",
    "</details>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bd4d64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_top5_per_user=(df_top_user\n",
    "                ...\n",
    "                 )\n",
    "df_top5_per_user.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb68e6c",
   "metadata": {
    "tags": []
   },
   "source": [
    " ### 4.7 Top 10 Influencer (User mit #BigData-tweets mit den meisten Followern) \n",
    " Schreibe eine Abfrage, die die **Top 10 Influencer** mit den **meisten Follower** zählt und sortiert anzeigt.\n",
    " <br>\n",
    "<br>\n",
    "<details>\n",
    "<summary> &#8964 Hinweis </summary>\n",
    "<p>\n",
    "<code>df_top_influencer=(df\n",
    "                .groupBy(\"&ltcolumnA&gt\")\n",
    "                .agg(\n",
    "                    f.&ltfunctionForMaximum&gt(\"&ltcolumnB&gt\").alias(\"follower\")\n",
    "                    )\n",
    "                .&ltfunctionToOrder&gt(f.col(\"follower\").desc())\n",
    "                )\n",
    "df_top_influencer.show(10)</code>\n",
    "</details>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b5113b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top_influencer=(df\n",
    "                ...\n",
    "                )\n",
    "df_top_influencer.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935e6148",
   "metadata": {},
   "source": [
    "### 4.8 Top 10 Influencer und ihre Anzahl an tweets\n",
    "Schreibe eine Abfrage, die die **Top 10 Influencer**, ihre Follower und die **Anzahl ihrer Tweets** ausgibt. außeredem soll es sortiert nach den Anzahl ihrer Follower sein. \n",
    "<br>\n",
    "<br>\n",
    "<details>\n",
    "<summary> &#8964 Hinweis </summary>\n",
    "<p>\n",
    "<code>df_withRetweets=(&ltdataFrameA&gt\n",
    "            # filter via join auf die Top 10 Influencer\n",
    "            .join(&ltdataFrameB&gt, [&ltdataFrameA&gt.&ltcolumnA&gt==&ltdataFrameB&gt.&ltcolumnB&gt],how=\"&lthowToJoin&gt\")\n",
    "            .orderBy(f.col(\"&ltcolumnC&gt\").desc())\n",
    "            .limit(10)\n",
    "            .drop(\"&ltdoubledColumn&gt\")\n",
    "            .select(\"&ltcolumnA&gt\",\"&ltcolumnC&gt\",\"&ltcolumnD&gt\")\n",
    "    )\n",
    "df_withRetweets.show()</code>\n",
    "</details>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47a7c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_withRetweets=(df_top_user\n",
    "            ....\n",
    "    )\n",
    "\n",
    "df_withRetweets.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13676250",
   "metadata": {},
   "source": [
    "### Bonusaufgabe: Filter nach den Top 10 Locations und ihrem Top Hashtag\n",
    "Schreibe eine Abfrage, die die **Top 10 häufigsten Locations** ausgibt und das am **zweitmeisten verwendete Hashtag** dort. Da alle unsere Daten das Hashtag #BigData beinhalten. \n",
    "<br>\n",
    "<br>\n",
    "<details>\n",
    "<summary> &#8964 Hinweis </summary>\n",
    "<p>\n",
    "<code>df3=(df\n",
    "    .select(\"&ltcolumnA&gt\")\n",
    "    .where(~f.col(\"&ltcolumnC&gt\").isin(<FilterWord1,...,FilterWordN>))\n",
    "    .groupBy(\"&ltcolumnC&gt\")\n",
    "    .&ltfunctionToCount()&gt\n",
    "    .withColumnRenamed(\"count\",\"location_total\")\n",
    "    .orderBy(f.col(\"location_total\").desc())\n",
    "    .limit(10)\n",
    "    )</code>\n",
    "    \n",
    "<code>df4=(df\n",
    "    .select(\"&ltcolumnA&gt\",\"&ltcolumnB&gt\")\n",
    "    .withColumn(\"singletag\",f.explode(f.col(\"&ltcolumnB&gt\")))\n",
    "    .g&ltfunctiontoGroup&gt(\"&ltcolumnA&gt\",\"singletag\")\n",
    "    .&ltfunctiontoCount()&gt\n",
    "    .withColumnRenamed(\"count\",\"tags_total\")\n",
    "        )</code>\n",
    "    \n",
    "<code>df5=(&ltdataFrameA&gt.alias(\"a\")\n",
    "    .&ltfunctiontoJoin&gt(f.broadcast(&ltdataFrameB&gt.alias(\"b\")),[&ltdataFrameB&gt.&ltcolumnA&gt==&ltdataFrameB&gt.&ltcolumnB&gt],how=\"left\")\n",
    "    .select(\"a.&ltcolumnB&gt\",\"a.location_total\",\"b.singletag\",\"b.tags_total\")      \n",
    "    .withColumn(\"rank\",f.row_number().over(Window.partitionBy(\"a.&ltcolumnB&gt\")\n",
    "    .&ltfunctionToOrder&gt(f.col(\"b.tags_total\").desc())))\n",
    "    .&ltfunctionToFilter&gt(f.col(\"rank\")==2)\n",
    "    .&ltfunctionToSort&gt(f.col(\"location_total\").&ltdescending&gt())\n",
    "    .limit(10)\n",
    "    )\n",
    "df5.show()</code>\n",
    "</details>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5df865",
   "metadata": {},
   "source": [
    "# 6. Ausschalten der Spark-App\n",
    "**Bitte schließe am Ende die Spark-App wieder mit dem folgenden Befehl `spark.stop()`, wenn du fertig mit der Bearbeitung der Aufgaben bist.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdb31e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
