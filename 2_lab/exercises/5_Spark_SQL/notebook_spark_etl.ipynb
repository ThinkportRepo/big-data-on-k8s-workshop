{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5765a06e",
   "metadata": {},
   "source": [
    "# Spark ETL Aufgaben\n",
    "1. Daten von Avro laden und nach Delta rausschreiben\n",
    "\n",
    "\n",
    "## Wichtige Hinweise\n",
    "1. Führe alle Anweisungen in der vorgegebenen Reihenfolge aus. Die einzelnen Programmierzellen bauen aufeinander auf.\n",
    "2. **Beende unbedingt am Ende die Spark-Anwendung mit dem untersten Befehl \"spark.stop()\" , wenn du aufhörst an den Daten zu arbeiten, damit die Resourcen (cpu und memory) wieder freigegeben werden**\n",
    "3. Du kannst jederzeit das Notebook wieder hochfahren, wenn du Schritt 1 & 2 (Laden der Imports & Jupyter Spark und seine Konfigurationen hochfahren) ausführen.\n",
    "4. Mit **\"Strg\" + \"Enter\"** führst du einzelne Zellen direkt aus.\n",
    "5. In der oberen Leiste kannst du über **\"Insert\"** weitere Zellen hinzufügen, um weitere Test-Funktionen zu schreiben. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "03c9d1f3",
   "metadata": {},
   "source": [
    "## 1. Module Laden\n",
    "Hier werden alle benötigten Libraries für dieses Lab heruntergeladen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebe3ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "from delta import *\n",
    "\n",
    "\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "\n",
    "# use 95% of the screen for jupyter cell\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container {width:100% !important; }<style>\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2579b387",
   "metadata": {},
   "source": [
    "## 2.  Spark starten\n",
    "Hier wird die App jupyter-spark konfiguriert und hochgefahren, welche unsere weiteren Schritte ausführt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d15c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "appName=\"jupyter-etl\"\n",
    "\n",
    "conf = SparkConf()\n",
    "\n",
    "# CLUSTER MANAGER\n",
    "################################################################################\n",
    "# set Kubernetes Master as Cluster Manager(“k8s://https://” is NOT a typo, this is how Spark knows the “provider” type).\n",
    "conf.setMaster(\"k8s://https://kubernetes.default.svc.cluster.local:443\")\n",
    "\n",
    "# CONFIGURE KUBERNETES\n",
    "################################################################################\n",
    "# set the namespace that will be used for running the driver and executor pods.\n",
    "conf.set(\"spark.kubernetes.namespace\",\"frontend\")\n",
    "# set the docker image from which the Worker pods are created\n",
    "conf.set(\"spark.kubernetes.container.image\", \"thinkportgmbh/workshops:spark-3.3.2\")\n",
    "conf.set(\"spark.kubernetes.container.image.pullPolicy\", \"Always\")\n",
    "\n",
    "# set service account to be used\n",
    "conf.set(\"spark.kubernetes.authenticate.driver.serviceAccountName\", \"spark\")\n",
    "# authentication for service account(required to create worker pods):\n",
    "conf.set(\"spark.kubernetes.authenticate.caCertFile\", \"/var/run/secrets/kubernetes.io/serviceaccount/ca.crt\")\n",
    "conf.set(\"spark.kubernetes.authenticate.oauthTokenFile\", \"/var/run/secrets/kubernetes.io/serviceaccount/token\")\n",
    "\n",
    "\n",
    "# CONFIGURE SPARK\n",
    "################################################################################\n",
    "conf.set(\"spark.sql.session.timeZone\", \"Europe/Berlin\")\n",
    "# set driver host. In this case the ingres service for the spark driver\n",
    "# find name of the driver service with 'kubectl get services' or in the helm chart configuration\n",
    "conf.set(\"spark.driver.host\", \"jupyter-spark-driver.frontend.svc.cluster.local\")\n",
    "# set the port, If this port is busy, spark-shell tries to bind to another port.\n",
    "conf.set(\"spark.driver.port\", \"29413\")\n",
    "# add the postgres driver jars into session\n",
    "conf.set(\"spark.jars\", \"/opt/spark/jars/spark-avro_2.12-3.3.2.jar\")\n",
    "conf.set(\"spark.executor.extraClassPath\",\"/opt/spark/jars/spark-avro_2.12-3.3.2.jar\")\n",
    "\n",
    "# CONFIGURE S3 CONNECTOR\n",
    "conf.set(\"spark.hadoop.fs.s3a.endpoint\", \"minio.minio.svc.cluster.local:9000\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.access.key\", \"trainadm\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.secret.key\", \"train@thinkport\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "conf.set(\"spark.hadoop.fs.s3.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "\n",
    "#conf.set(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension, org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions, org.apache.spark.sql.hudi.HoodieSparkSessionExtension\")\n",
    "conf.set(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "conf.set(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "# CONFIGURE WORKER (Customize based on workload)\n",
    "################################################################################\n",
    "# set number of worker pods\n",
    "conf.set(\"spark.executor.instances\", \"3\")\n",
    "# set memory of each worker pod\n",
    "conf.set(\"spark.executor.memory\", \"1G\")\n",
    "# set cpu of each worker pod\n",
    "conf.set(\"spark.executor.cores\", \"1\")\n",
    "# Number of possible tasks = cores * executores\n",
    "\n",
    "## Deltalake\n",
    "# conf.set(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "\n",
    "# SPARK SESSION\n",
    "################################################################################\n",
    "# and last, create the spark session and pass it the config object\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .config(conf=conf) \\\n",
    "    .config('spark.sql.session.timeZone', 'Europe/Berlin') \\\n",
    "    .appName(appName)\\\n",
    "    .getOrCreate()\n",
    "\n",
    "# also get the spark context\n",
    "sc=spark.sparkContext\n",
    "# change the log level to warning, to see less output\n",
    "sc.setLogLevel('ERROR')\n",
    "\n",
    "# get the configuration object to check all the configurations the session was startet with\n",
    "for entry in sc.getConf().getAll():\n",
    "        if entry[0] in [\"spark.app.name\",\"spark.kubernetes.namespace\",\"spark.executor.memory\",\"spark.executor.cores\",\"spark.driver.host\",\"spark.master\"]:\n",
    "            print(entry[0],\"=\",entry[1])\n",
    "            \n",
    "spark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9f42ed16",
   "metadata": {},
   "source": [
    "## 3. ETL: Einlesen - Transformieren - Schreiben"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "93cbac52",
   "metadata": {},
   "source": [
    "### 3.1 Einlesen der Avro Daten (Extract)\n",
    "Lade die Daten aus dem Bucket `s3a://twitter/avro` in einen DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc875f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avro=(\n",
    "    #<< HIER CODE EINFÜGEN >>\n",
    "    )\n",
    "\n",
    "# Count und Inhalt anzeigen\n",
    "print(\"Anzahl aller Tweets: \",df_avro.count())\n",
    "df_avro.show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "067a22f8",
   "metadata": {},
   "source": [
    "### 3.2 Transformieren des Dataframes (Transform)\n",
    "\n",
    "1. Filter nur auf die Zeilen, die das Hashtag \"BigData\" enthalten, mit Hilfe der passenden **Array Function** (https://spark.apache.org/docs/latest/sql-ref-functions-builtin.html#array-functions)\n",
    "2. Benenne folgende Spalten mit kürzeren Namen um, damti später weniger getippt werden muss.\n",
    "   - `user_name` -> `user`\n",
    "   - `user_location` -> `country`\n",
    "   - `user_follower_count` -> `follower`\n",
    "   - `user_friends_count` -> `friends`\n",
    "   - `retweet_count` -> `retweets´\n",
    "3. Füge eine neue Spalte `hastag_count` hinzu in der die Anzahl der Hashtags steht\n",
    "2. Entferne die Spalte `tweet_message` aus dem Resultset, da diese als langer String viel Speicher benötigt aber für die weiteren Analyse nicht benötigt wird\n",
    "\n",
    "\n",
    "Dokumentation: https://spark.apache.org/docs/3.1.1/api/python/reference/pyspark.sql.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08800479",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed = (df_avro\n",
    "                  # filter die Zeilen mit Hashtag BigData raus\n",
    "                  #<< HIER CODE EINFÜGEN >>\n",
    "                  # benenne die Spalten mit kürzeren Namen um\n",
    "                  #<< HIER CODE EINFÜGEN >>\n",
    "                  # füge eine neue Spalte mit der Anzahl der Hasthags hinzu\n",
    "                  #<< HIER CODE EINFÜGEN >>\n",
    "                  # falls nicht schon entfernt, entferne die Spalte `tweet_message`\n",
    "                  #<< HIER CODE EINFÜGEN >>\n",
    "              )\n",
    "\n",
    "\n",
    "# Dataframe anzeigen\n",
    "print(\"Anzahl aller Tweets: \",df_avro.count())\n",
    "print(\"Anzahl nach Transformation: \",df_transformed.count())\n",
    "df_transformed.show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7da71da4",
   "metadata": {},
   "source": [
    "### 3.3 Rausschreiben der Daten in das Deltaformat (Load)\n",
    "Schreibe die Daten im Delta Format in das Bucke S3-Bucket `s3a://twitter/delta`   \n",
    "verwende herbei folgende Spezifikationen:\n",
    "- partitioniere die Daten nach der Spalte `language` \n",
    "- setze den Mode auf `append` (nicht `overwrite`)\n",
    "- setzt die Option `overwriteSchema`auf `true`\n",
    "- füge einen Kommentar `load from Spark` in den Metadaten hinzu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c13f331",
   "metadata": {},
   "outputs": [],
   "source": [
    "write=(df_transformed\n",
    "        .write\n",
    "        \n",
    "        # partitioniere nach Spalte `language`\n",
    "        \n",
    "        # schreibe mit mode=append\n",
    "        \n",
    "        # Füge Optionen und Kommentar dazu\n",
    "        \n",
    "        \n",
    "        .save(\"WOHIN?\")\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "07465e47",
   "metadata": {},
   "source": [
    "## 4. Ergebnis überprüfen\n",
    "Überprüfe mit einer Methode deiner Wahl ob die Daten erfolgreich auf s3 angekommen sind und ob sie Partitioniert wurden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e7a12b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5d5df865",
   "metadata": {},
   "source": [
    "# 6. Ausschalten der Spark-App\n",
    "**Bitte schließe am Ende die Spark-App wieder mit dem folgenden Befehl `spark.stop()`, wenn du fertig mit der Bearbeitung der Aufgaben bist.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdb31e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
