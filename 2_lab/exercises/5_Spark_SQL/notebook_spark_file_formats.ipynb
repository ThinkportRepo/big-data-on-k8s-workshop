{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "610d267c",
   "metadata": {},
   "source": [
    "# Aufgaben zu Big Data File Formats\n",
    "\n",
    "Folgende Aufgaben haben zum Ziel mit den verschiedenen Dateiformaten vertraut zu werden und insbesondere die speziellen Eigenschaften und Funktionen der Formate zu verstehen\n",
    "\n",
    "### CSV and JSON\n",
    "Klassische Datei Formate für Datenverarbeitung  \n",
    "**Typische Eigenschaften:** Einfache Struktur, human-readable, Zeilenformat\n",
    "\n",
    "### Avro, ORC, Parquet\n",
    "Big Data optimierte Formate um schnell große Datenmengen zu lesen und zu schreiben  \n",
    "**Typische Eigenschaften:** teilbar in kleine Dateien (splittable), komprimierbar (compressible), überspringbar (skippable), selbsterklärend (self describing with schema), Schema erweiterbar (Schema Evolution), Schema erzwingend (Schema Enforcment), Filter Pushdown\n",
    "\n",
    "### Delta, Iceberg, Hudi\n",
    "Erweiterte Big Data Formate um die ACID und Tracing Eigenschaften einer klassichen SQL Datenbank zu erfüllen  \n",
    "**Typische Eigenschaften:** Erweiterung um zusätzliche Metadaten und spezielle Treiber zum lesen/schreiben, Time Travel Funktion, Merge und Update Funktionen, Audit Log Funktionalitäten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592e56dd",
   "metadata": {},
   "source": [
    "###  Import Python Modules\n",
    "Hier werden alle benötigten Libraries für dieses Lab heruntergeladen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0aa4fda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container {width:100% !important; }<style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "from delta import *\n",
    "\n",
    "import datetime\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import json\n",
    "import csv\n",
    "\n",
    "import boto3\n",
    "from botocore.client import Config\n",
    "\n",
    "# use 95% of the screen for jupyter cell\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container {width:100% !important; }<style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3ba21e",
   "metadata": {},
   "source": [
    "### Launch Spark Jupyter and Configuration\n",
    "\n",
    "#### Configure a Spark session for Kubernetes cluster with S3 support\n",
    "### CLUSTER MANAGER\n",
    "- set the Kubernetes master URL as Cluster Manager(“k8s://https://” is NOT a typo, this is how Spark knows the “provider” type)\n",
    "\n",
    "### KUBERNETES\n",
    "- set the namespace that will be used for running the driver and executor pods\n",
    "- set the docker image from which the Worker/Exectutor pods are created\n",
    "- set the Kubernetes service account name and provide the authentication details for the service account (required to create worker pods)\n",
    "\n",
    "### SPARK\n",
    "- set the driver host and the driver port (find name of the driver service with 'kubectl get services' or in the helm chart configuration)\n",
    "- enable Delta Lake, Iceberg, and Hudi support by setting the spark.sql.extensions\n",
    "- configure Hive catalog for Iceberg\n",
    "- enable S3 connector\n",
    "- set the number of worker pods, their memory and cores (HINT: number of possible tasks = cores * executores)\n",
    "\n",
    "### SPARK SESSION\n",
    "- create the Spark session using the SparkSession.builder object\n",
    "- get the Spark context from the created session and set the log level to \"ERROR\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1faf0688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f9f9068",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/29 13:13:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.kubernetes.namespace = frontend\n",
      "spark.master = k8s://https://kubernetes.default.svc.cluster.local:443\n",
      "spark.app.name = jupyter-spark\n",
      "spark.executor.memory = 1G\n",
      "spark.executor.cores = 2\n",
      "spark.sql.extensions = org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\n",
      "spark.driver.host = jupyter-spark-driver.frontend.svc.cluster.local\n"
     ]
    }
   ],
   "source": [
    "appName=\"jupyter-spark\"\n",
    "\n",
    "conf = SparkConf()\n",
    "\n",
    "# CLUSTER MANAGER\n",
    "\n",
    "conf.setMaster(\"k8s://https://kubernetes.default.svc.cluster.local:443\")\n",
    "\n",
    "# CONFIGURE KUBERNETES\n",
    "\n",
    "conf.set(\"spark.kubernetes.namespace\",\"frontend\")\n",
    "conf.set(\"spark.kubernetes.container.image\", \"thinkportgmbh/workshops:spark-3.3.2\")\n",
    "conf.set(\"spark.kubernetes.container.image.pullPolicy\", \"Always\")\n",
    "\n",
    "conf.set(\"spark.kubernetes.authenticate.driver.serviceAccountName\", \"spark\")\n",
    "conf.set(\"spark.kubernetes.authenticate.caCertFile\", \"/var/run/secrets/kubernetes.io/serviceaccount/ca.crt\")\n",
    "conf.set(\"spark.kubernetes.authenticate.oauthTokenFile\", \"/var/run/secrets/kubernetes.io/serviceaccount/token\")\n",
    "\n",
    "# CONFIGURE SPARK\n",
    "\n",
    "conf.set(\"spark.sql.session.timeZone\", \"Europe/Berlin\")\n",
    "conf.set(\"spark.driver.host\", \"jupyter-spark-driver.frontend.svc.cluster.local\")\n",
    "conf.set(\"spark.driver.port\", \"29413\")\n",
    "\n",
    "conf.set(\"spark.jars\", \"/opt/spark/jars/spark-avro_2.12-3.3.2.jar\")\n",
    "conf.set(\"spark.driver.extraClassPath\",\"/opt/spark/jars/spark-avro_2.12-3.3.2.jar\")\n",
    "conf.set(\"spark.executor.extraClassPath\",\"/opt/spark/jars/spark-avro_2.12-3.3.2.jar\")\n",
    "\n",
    "conf.set(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension, org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions, org.apache.spark.sql.hudi.HoodieSparkSessionExtension\")\n",
    "\n",
    "######## Hive als Metastore einbinden\n",
    "#conf.set(\"hive.metastore.uris\", \"thrift://hive-metastore.hive.svc.cluster.local:9083\") \n",
    "\n",
    "######## Iceberg configs\n",
    "conf.set(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\")\n",
    "conf.set(\"spark.sql.catalog.ice\",\"org.apache.iceberg.spark.SparkCatalog\") \n",
    "conf.set(\"spark.sql.catalog.ice.type\",\"hive\") \n",
    "conf.set(\"spark.sql.catalog.ice.uri\",\"thrift://hive-metastore.hive.svc.cluster.local:9083\") \n",
    "\n",
    "####### Hudi configs\n",
    "conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "\n",
    "# CONFIGURE S3 CONNECTOR\n",
    "conf.set(\"spark.hadoop.fs.s3a.endpoint\", \"minio.minio.svc.cluster.local:9000\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.access.key\", \"trainadm\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.secret.key\", \"train@thinkport\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "conf.set(\"spark.hadoop.fs.s3.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "\n",
    "# CONFIGURE WORKER (Customize based on workload)\n",
    "\n",
    "conf.set(\"spark.executor.instances\", \"1\")\n",
    "conf.set(\"spark.executor.memory\", \"1G\")\n",
    "conf.set(\"spark.executor.cores\", \"2\")\n",
    "\n",
    "# SPARK SESSION\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .config(conf=conf) \\\n",
    "    .config('spark.sql.session.timeZone', 'Europe/Berlin') \\\n",
    "    .appName(appName)\\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "sc=spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "# get the configuration object to check all the configurations the session was startet with\n",
    "for entry in sc.getConf().getAll():\n",
    "        if entry[0] in [\"spark.app.name\",\"spark.kubernetes.namespace\",\"spark.executor.memory\",\"spark.executor.cores\",\"spark.driver.host\",\"spark.master\",\"spark.sql.extensions\"]:\n",
    "            print(entry[0],\"=\",entry[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdaa9936",
   "metadata": {},
   "source": [
    "### Configure Boto3 \n",
    "for simple s3 operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2a9d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hilfsfunktionen um mit einfachen Befehlen auf s3 zu arbeiten \n",
    "# WICHTIG: Falls das bucket s3://fileformats noch nicht existiert muss dieses über das Terminal erst erzeugt werden\n",
    "# Command: s3 mb s3://fileformats\n",
    "\n",
    "# Bucket, muss zuerst in Minio oder via Terminal Befehl erstellt werden\n",
    "bucket = \"fileformats\"\n",
    "bucket_path=\"s3://\"+bucket\n",
    "\n",
    "options = {\n",
    "    'endpoint_url': 'http://minio.minio.svc.cluster.local:9000',\n",
    "    'aws_access_key_id': 'trainadm',\n",
    "    'aws_secret_access_key': 'train@thinkport',\n",
    "    'config': Config(signature_version='s3v4'),\n",
    "    'verify': False}\n",
    "\n",
    "s3_resource = boto3.resource('s3', **options)  \n",
    "s3_client = boto3.client('s3', **options)\n",
    "\n",
    "# show files on s3 bucket/prefix\n",
    "def ls(bucket,prefix):\n",
    "    '''List objects from bucket/prefix'''\n",
    "    try:\n",
    "        for obj in s3_resource.Bucket(bucket).objects.filter(Prefix=prefix):\n",
    "            print(obj.key)\n",
    "    except Exception as e: \n",
    "        print(e)\n",
    "    \n",
    "# show file content in files\n",
    "def cat(bucket,prefix,binary=False):\n",
    "    '''Show content of one or several files with same prefix/wildcard'''\n",
    "    try:\n",
    "        for obj in s3_resource.Bucket(bucket).objects.filter(Prefix=prefix):\n",
    "            print(\"File:\",obj.key)\n",
    "            print(\"----------------------\")\n",
    "            if binary==True:\n",
    "                print(obj.get()['Body'].read())\n",
    "            else: \n",
    "                print(obj.get()['Body'].read().decode())\n",
    "            print(\"######################\")\n",
    "    except Exception as e: \n",
    "        print(e)\n",
    "\n",
    "# delete files from bucket\n",
    "def rm(bucket,prefix):\n",
    "    '''Delete everything from bucket/prefix'''\n",
    "    for object in s3_resource.Bucket(bucket).objects.filter(Prefix=prefix):\n",
    "        print(object.key)\n",
    "        s3_client.delete_object(Bucket=bucket, Key=object.key)\n",
    "    print(f\"Deleted files from {bucket}/{prefix}*\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cc004e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show everything in bucket\n",
    "ls(bucket,\"\")\n",
    "print(\"#############################\")\n",
    "# show folder\n",
    "ls(bucket,\"csv\")\n",
    "print(\"#############################\")\n",
    "# show subfolder\n",
    "ls(bucket,\"delta/_delta_log/\")\n",
    "print(\"#############################\")\n",
    "print(\"\")\n",
    "# show content of one or several files with same prefix/wildcard\n",
    "cat(bucket,'csv/part')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27495f26",
   "metadata": {},
   "source": [
    "### Create sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1b8b7c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++ create new dataframe and show schema and data\n",
      "################################################\n",
      "++ start data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------+-------+\n",
      "|id |account|dt_transaction|balance|\n",
      "+---+-------+--------------+-------+\n",
      "|1  |alex   |2019-01-01    |1000   |\n",
      "|2  |alex   |2019-02-01    |1500   |\n",
      "|4  |maria  |2020-01-01    |5000   |\n",
      "|3  |alex   |2019-03-01    |1700   |\n",
      "+---+-------+--------------+-------+\n",
      "\n",
      "++ update row and add row\n",
      "+---+-------+--------------+-------+\n",
      "|id |account|dt_transaction|balance|\n",
      "+---+-------+--------------+-------+\n",
      "|1  |alex   |2019-03-01    |3300   |\n",
      "|2  |peter  |2021-01-01    |100    |\n",
      "+---+-------+--------------+-------+\n",
      "\n",
      "++ add new column\n",
      "+---+-------+--------------+-------+-------------+\n",
      "|id |account|dt_transaction|balance|new          |\n",
      "+---+-------+--------------+-------+-------------+\n",
      "|1  |otto   |2019-10-01    |4444   |neue Spalte 1|\n",
      "+---+-------+--------------+-------+-------------+\n",
      "\n",
      "++ add new row with wrong schema (id)\n",
      "+---+-------+--------------+-------+\n",
      "|id |account|dt_transaction|balance|\n",
      "+---+-------+--------------+-------+\n",
      "|5  |markus |2019-09-01    |555    |\n",
      "+---+-------+--------------+-------+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- account: string (nullable = true)\n",
      " |-- dt_transaction: date (nullable = true)\n",
      " |-- balance: long (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- account: string (nullable = true)\n",
      " |-- dt_transaction: date (nullable = true)\n",
      " |-- balance: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# initial Daten\n",
    "account_data1 = [\n",
    "    (1,\"alex\",\"2019-01-01\",1000),\n",
    "    (2,\"alex\",\"2019-02-01\",1500),\n",
    "    (3,\"alex\",\"2019-03-01\",1700),\n",
    "    (4,\"maria\",\"2020-01-01\",5000)\n",
    "    ]\n",
    "\n",
    "# Datensatz mit einem Update und einer neuen Zeile\n",
    "account_data2 = [\n",
    "    (1,\"alex\",\"2019-03-01\",3300),\n",
    "    (2,\"peter\",\"2021-01-01\",100)\n",
    "    ]\n",
    "\n",
    "# Datensatz mit neuer Zeile und neuer Spalte\n",
    "account_data3 = [\n",
    "    (1,\"otto\",\"2019-10-01\",4444,\"neue Spalte 1\")\n",
    "]\n",
    "\n",
    "# Datensatz mit neuer Zeile und neuer Spalte\n",
    "account_data4 = [\n",
    "    (5,\"markus\",\"2019-09-01\",555)\n",
    "]\n",
    "\n",
    "schema = [\"id\",\"account\",\"dt_transaction\",\"balance\"]\n",
    "schema3 = [\"id\",\"account\",\"dt_transaction\",\"balance\",\"new\"]\n",
    "\n",
    "df1 = spark.createDataFrame(data=account_data1, schema = schema).withColumn(\"dt_transaction\",f.col(\"dt_transaction\").cast(\"date\")).repartition(3)\n",
    "df2 = spark.createDataFrame(data=account_data2, schema = schema).withColumn(\"dt_transaction\",f.col(\"dt_transaction\").cast(\"date\")).repartition(2)\n",
    "df3 = spark.createDataFrame(data=account_data3, schema = schema3).withColumn(\"dt_transaction\",f.col(\"dt_transaction\").cast(\"date\")).repartition(1)\n",
    "df4 = spark.createDataFrame(data=account_data4, schema = schema).withColumn(\"dt_transaction\",f.col(\"dt_transaction\").cast(\"date\")).withColumn(\"id\",f.col(\"id\").cast(\"string\")).repartition(1)\n",
    "\n",
    "\n",
    "print(\"++ create new dataframe and show schema and data\")\n",
    "print(\"################################################\")\n",
    "\n",
    "# df1.printSchema()\n",
    "print(\"++ start data\")\n",
    "df1.show(truncate=False)\n",
    "print(\"++ update row and add row\")\n",
    "df2.show(truncate=False)\n",
    "print(\"++ add new column\")\n",
    "df3.show(truncate=False)\n",
    "print(\"++ add new row with wrong schema (id)\")\n",
    "df4.show(truncate=False)\n",
    "df1.printSchema()\n",
    "df4.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf0c3d1",
   "metadata": {},
   "source": [
    "<hr style=\"height: 3px; background: gray;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6fe705",
   "metadata": {},
   "source": [
    "## CSV\n",
    "\n",
    "### Aufgabe:\n",
    "Schreibe die Daten als CSV mit der Overwrite und Append Funktion\n",
    "\n",
    "1. Datenset 1 als csv schreiben (.format(\"csv\") und Pfad= .save(f\"s3://{bucket}/csv\"))\n",
    "2. Dateien und Inhalt anzeigen, vestehen was da passiert ist\n",
    "3. Daten wieder einlese und checken ob es ein Schema und Spaltennamen erhalten wurden\n",
    "4. Datenset 3 anfügen mit weiterer Spalte anfügen (append)\n",
    "5. Daten wieder einlesen und checken was mit der neuen Spalte passiert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916ad0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of Partitions:\", df1.rdd.getNumPartitions())\n",
    "\n",
    "# Schreibe Datenset 1 als CSV Datei\n",
    "write_csv=(df1\n",
    "           .write\n",
    "           .format(\"csv\")\n",
    "           .mode(\"overwrite\") # append\n",
    "           .save(f\"s3://{bucket}/csv\")\n",
    "          )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebbc9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anzeigen der Dateien im Bucket/Prefix\n",
    "ls(bucket,\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fb51b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anzeigen der Inhalte jeder Datei im Bucket/Prefix\n",
    "cat(bucket,\"csv/part\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f31d31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lese die csv Datei wieder ein und prüfe das Schema\n",
    "read_csv=spark.read.format(\"csv\").load(f\"s3://{bucket}/csv\")\n",
    "\n",
    "read_csv.printSchema()\n",
    "read_csv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5a5979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# schreibe Datenset 3 (neue Spalte) in die gleiche Tabelle dazu\n",
    "write_csv=(df3\n",
    "           .write\n",
    "           .format(\"csv\")\n",
    "           .mode(\"append\")\n",
    "           .save(f\"s3://{bucket}/csv\")\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a771362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anzeigen der Inhalte jeder Datei im Bucket/Prefix\n",
    "ls(bucket,\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349331cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anzeigen der Inhalte jeder Datei im Bucket/Prefix\n",
    "cat(bucket,\"csv/part\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fd06f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# und lese alles nochmal ein um zu schauen ob die neue Spalte richtig erkannt wurde\n",
    "read_csv=spark.read.format(\"csv\").load(f\"s3://{bucket}/csv\")\n",
    "\n",
    "read_csv.printSchema()\n",
    "read_csv.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1b3038",
   "metadata": {},
   "source": [
    "#### Erkenntnisse CSV\n",
    "* In wieviele Dateien wird das Datenset aufgeteilt und warum?\n",
    "* Bleibt das Schema erhalten (Selbsterklärend)\n",
    "* Können neue Spalten angefügt werden (Schema Evolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148b0337",
   "metadata": {},
   "source": [
    "<hr style=\"height: 3px; background: gray;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55bcbfa",
   "metadata": {},
   "source": [
    "## JSON\n",
    "\n",
    "### Aufgabe:\n",
    "Wiederhole die gleichen Schritte mit dem JSON Format und schaue wie sich hier Schema und neue Spalten verhalten\n",
    "\n",
    "1. Datenset 1 als json schreiben (.format(\"json\") und Pfad= .save(f\"s3://{bucket}/json\"))\n",
    "2. Dateien und Inhalt anzeigen, vestehen was da passiert ist\n",
    "3. Daten wieder einlese und checken ob es ein Schema und Spaltennamen gibt\n",
    "4. Datenset 3 anfügen (append)\n",
    "5. Daten wieder einlesen und checken was mit der neuen Spalte passiert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e368413b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of Partitions:\", df1.rdd.getNumPartitions())\n",
    "\n",
    "# Schreibe Datenset 1 als JSON Datei\n",
    "\n",
    "# HIER EIGENE CODE EINFÜGEN - in den pfad s3://{bucket}/json schreiben"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23edb01",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> &#8964 Lösung Datenset 1 als JSON Datei schreiben</summary>\n",
    "<p>\n",
    "<code>write_json=(df1\n",
    "           .write\n",
    "           .format(\"json\")\n",
    "           .mode(\"overwrite\") # append\n",
    "           .save(f\"s3://{bucket}/json\")\n",
    "          )</code>\n",
    "</details>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9898f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls(bucket,\"json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba7f5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat(bucket,\"json/part\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe6715f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daten wieder einlese und checken ob es ein Schema und Spaltennamen gibt\n",
    "\n",
    "# HIER EIGENE CODE EINFÜGEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7506f71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# schreibe Datenset 3 (neue Spalte) in die gleiche Tabelle dazu (!! append NOT overwrite)\n",
    "\n",
    "# HIER EIGENE CODE EINFÜGEN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046d241e",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> &#8964 Lösung Datenset 3 als JSON Datei anfügen</summary>\n",
    "<p>\n",
    "<code>write_json=(df3\n",
    "           .write\n",
    "           .format(\"json\")\n",
    "           .mode(\"append\") # append\n",
    "           .save(f\"s3://{bucket}/json\")\n",
    "          )</code>\n",
    "</details>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba940d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alles nochmal einlesen und schauen ob die neue Spalte und die Schemas richtig erkannt wurden\n",
    "\n",
    "read_json=# HIER EIGENE CODE EINFÜGEN\n",
    "\n",
    "read_json.printSchema()\n",
    "read_json.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd549ced",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> &#8964 Lösung JSON wieder einlesen</summary>\n",
    "<p>\n",
    "<code>\n",
    "read_json=spark.read.format(\"json\").load(f\"s3://{bucket}/json\")\n",
    "read_json.printSchema()\n",
    "read_json.show()\n",
    "    </code>\n",
    "</details>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad580792",
   "metadata": {},
   "source": [
    "#### Erkenntnisse JSON\n",
    "* Bleibt das Schema erhalten (Selbsterklärend)\n",
    "* Können neue Spalten angefügt werden (Schema Evolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094e8bb0",
   "metadata": {},
   "source": [
    "## AVRO\n",
    "Avro ist ein Zeilenformat was für das schnelle Schreiben im Streaming Kontext optimiert ist.\n",
    "Avro ist selbsterklärend, hat ein Schema und unterstützt Schema Evolution\n",
    "\n",
    "### Aufgabe:\n",
    "Wiederhole die gleichen Schritte mit dem AVRO Format und schaue wie sich hier Schema und neue Spalten verhalten\n",
    "\n",
    "1. Datenset 1 als avro schreiben (.format(\"avro\") und Pfad= .save(f\"s3://{bucket}/avro\"))\n",
    "2. Dateien und Inhalt anzeigen, vestehen was da passiert ist\n",
    "3. Metadaten in Datei identifizieren\n",
    "3. Daten wieder einlese und checken ob es ein Schema und Spaltennamen gibt\n",
    "4. Schema Evolutiuon: Datenset 3 anfügen mit neuer Spalte anfügen\n",
    "5. Daten wieder einlesen und checken was mit der neuen Spalte passiert\n",
    "6. Schema Enforcement: Datentyp in bestehender Spalte ändern und schauen ob und wie dies gehandhabt wird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44777375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schreibe Datenset 1 als AVRO Datei\n",
    "\n",
    "# <<HIER EIGENE CODE EINFÜGEN>> - in den Pfad s3://{bucket}/avro schreiben\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c89ea1",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> &#8964 Lösung Datenset 1 als AVRO Datei schreiben</summary>\n",
    "<p>\n",
    "<code>write_avro=(df1\n",
    "           .write\n",
    "           .format(\"avro\")\n",
    "           .mode(\"overwrite\") # append\n",
    "           .save(f\"s3://{bucket}/avro\")\n",
    "          )</code>\n",
    "</details>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ebc8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sind die Daten auf s3 angekommen\n",
    "# <<HIER EIGENE CODE EINFÜGEN>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82f2e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finde in der Darstellung der Datei die Metadaten und die eigentlichen Daten\n",
    "# Da Avro ein Binärformat ist muss hier in cat die Flag auf True gesetzt werden\n",
    "cat(bucket,\"avro/part\",True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac72f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_avro=# HIER EIGENE CODE EINFÜGEN\n",
    "\n",
    "read_avro.printSchema()\n",
    "read_avro.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1e0ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# schreibe Datenset 3 (neue Spalte) in die gleiche Tabelle dazu (!! append NOT overwrite)\n",
    "\n",
    "# HIER EIGENE CODE EINFÜGEN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d9a03f",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> &#8964 Lösung Datenset 3 als AVRO Datei anfügen</summary>\n",
    "<p>\n",
    "<code>write_avro=(df3\n",
    "           .write\n",
    "           .format(\"avro\")\n",
    "           .mode(\"append\") # append\n",
    "           .save(f\"s3://{bucket}/avro\")\n",
    "          )</code>\n",
    "</details>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70570799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alles nochmal einlesen und schauen ob die neue Spalte und die Schemas richtig erkannt wurden\n",
    "# wiederholt sich langsam gell?\n",
    "\n",
    "# <<HIER EIGENE CODE EINFÜGEN>>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e98de2e",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> &#8964 Lösung AVRO wieder einlesen</summary>\n",
    "<p>\n",
    "<code>\n",
    "read_avro=spark.read.format(\"avro\").load(f\"s3://{bucket}/json\")\n",
    "read_avro.printSchema()\n",
    "read_avro.show()\n",
    "    </code>\n",
    "</details>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572f0409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Füge eine Zeile (df2) zu der AVRO Tabelle hinzu aber ändere den Datentyp für die id von long zu string\n",
    "print(\"Schema vorher:\")\n",
    "df2.printSchema()\n",
    "\n",
    "\n",
    "df2a=(df2\n",
    "      # nur die Zeile Peter aus df2\n",
    "      .where(f.col(\"account\")==\"peter\")\n",
    "      # ID als string statt als long\n",
    "      .withColumn(\"id\", f.col(\"id\").cast(\"int\"))\n",
    "     )\n",
    "\n",
    "print(\"Schema nachher:\")\n",
    "df2a.printSchema()\n",
    "\n",
    "\n",
    "write_avro=(df2a\n",
    "            .write\n",
    "            .format(\"avro\")\n",
    "            .mode(\"append\")\n",
    "            .save(f\"s3://{bucket}/avro\")\n",
    "           )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d39ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# probiere das Verzeichnis mit verschiedenen Datentypen einzulesen\n",
    "read_avro=(spark\n",
    "           .read\n",
    "           .format(\"avro\")\n",
    "           .load(f\"s3://{bucket}/avro\"))\n",
    "\n",
    "\n",
    "read_avro.printSchema()\n",
    "read_avro.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b1054d",
   "metadata": {},
   "source": [
    "#### Erkenntnisse AVRO\n",
    "* Werden Spaltennamen erhalten? \n",
    "* Gibt es ein Schema?\n",
    "* Schema Evolution: Kann das Schema erweitert werden, also eine neue Spalte angefügt werden?\n",
    "* Schema Enforcement on write: Kann eine Spalte mit falschem Datetyp einfach beim schreiben hinzugefügt werden? \n",
    "* Schema Enforcement on read: Kann ein Verzeichnis mit mehreren Avro Dateien bei der eine Spalte ein anderes Schema hat gelesen werden?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cd7f3e",
   "metadata": {},
   "source": [
    "<hr style=\"height: 3px; background: gray;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45de295",
   "metadata": {},
   "source": [
    "## Parquet\n",
    "\n",
    "### Aufgabe:\n",
    "Wiederhole die gleichen Schritte mit dem PARQUET Format und schaue wie sich hier Schema und neue Spalten verhalten\n",
    "\n",
    "1. Datenset 1 als parquet schreiben (.format(\"parquet\") und Pfad= .save(f\"s3://{bucket}/parquet\"))\n",
    "2. Dateien und Inhalt anzeigen, vestehen was da passiert ist\n",
    "3. Metadaten in Datei identifizieren\n",
    "3. Daten wieder einlese und checken ob es ein Schema und Spaltennamen gibt\n",
    "4. Schema Evolutiuon: Datenset 3 anfügen mit neuer Spalte anfügen\n",
    "5. Daten wieder einlesen und checken was mit der neuen Spalte passiert\n",
    "6. Partion & Pushdown Filter: Execution Plan für verschiedene Filter anzeigen\n",
    "6. Schema Enforcement: Datentyp in bestehender Spalte ändern und schauen ob und wie dies gehandhabt wird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb546000",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of Partitions:\", df1.rdd.getNumPartitions())\n",
    "\n",
    "write_parquet=(df1\n",
    "           .write\n",
    "           # Fachliche Partitionierung beim Schreiben\n",
    "           .partitionBy(\"account\")\n",
    "           .format(\"parquet\")\n",
    "           .mode(\"overwrite\") # append\n",
    "           .save(f\"s3://{bucket}/parquet\")\n",
    "          )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9e08a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sind die Daten auf s3 angekommen\n",
    "# <<HIER EIGENE CODE EINFÜGEN>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f5468e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# schaue eine Datei im Detail an und finde die Metadaten\n",
    "# <<HIER EIGENE CODE EINFÜGEN>>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f538dc",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> &#8964 Lösung PARQUET Metadaten anzeigen</summary>\n",
    "<p>\n",
    "<code>\n",
    "cat(bucket,\"parquet/account=maria\",True)\n",
    "</code>\n",
    "</details>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba707c84",
   "metadata": {},
   "source": [
    "### Parquet: Filter Pushdown\n",
    "Da das Parquet Format spalten basiert ist und für jede Spalte Metadaten vorhällt, können Programme die diese Dateien einlesen vor der Serialisierung (dem kompletten Einlesen in den Arbeitsspeicher) erst die Header scannen und entscheiden welche Dateien tatsächlich benötigt werden.  \n",
    "Dies nennt man Attribut oder Filter Pushdown.  \n",
    "Spark kann außerdem, wenn die Daten in Partition im Format `PartitionKey=value` diese automatisch erkennen und wenn ein Filter auf die Partition gelegt ist nur diesen Unterordner einlesen.  \n",
    "\n",
    "**Aufgabe:** Untersuche den Execution Plan der Filter Operation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00a1e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parquet Datei mit PartitionFilter laden\n",
    "read_parquet=(spark\n",
    "              .read.format(\"parquet\")\n",
    "              .load(f\"s3://{bucket}/parquet\")\n",
    "              # Filter auf die Spalte über die partitioniert wurde\n",
    "              .filter(f.col(\"account\")==\"alex\")\n",
    "             )\n",
    "\n",
    "# Parquet mit normalem Filter laden\n",
    "read_parquet2=(spark\n",
    "              .read.format(\"parquet\")\n",
    "              .load(f\"s3://{bucket}/parquet\")\n",
    "              # Filter auf die Spalte über eine normale Spalte\n",
    "              .filter(f.col(\"balance\")>1500)\n",
    "             )\n",
    "\n",
    "# Anzeigen des physischen Execution Plans um zu sehen welche Filter ins Dateisystem bzw. in die Parquet Datei gepusht werden\n",
    "print(\"Partition Filter\")\n",
    "read_parquet.explain(\"simple\")\n",
    "print(\"Pushdow Filter\")\n",
    "read_parquet2.explain(\"simple\")\n",
    "\n",
    "\n",
    "#read_parquet.printSchema()\n",
    "#read_parquet.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1114d68f",
   "metadata": {},
   "source": [
    "### Parquet: Schema Evolution\n",
    "Schema Evolution ermöglicht es das Schema der Tabelle zu erweitern.  \n",
    "Der Spark Parquet reader bietet verschiedenen Möglichkeiten mit Schemaerweiterungen umzugehen  \n",
    "https://spark.apache.org/docs/latest/sql-data-sources-parquet.html#schema-merging\n",
    "\n",
    "**Aufgabe:** Lese die Daten so ein, dass das Schema korrekt erweitert wird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74705fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zeile mit neuer Spalte anfügen\n",
    "write_parquet=(df3\n",
    "           .write\n",
    "           .format(\"parquet\")\n",
    "           .mode(\"append\") # append\n",
    "           # schreibe ohne zu Partitionieren direkt in ein neues Unterverzeichnis\n",
    "           .save(f\"s3://{bucket}/parquet/account=otto\")\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c8e59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# einlesen mit der mergeSchema Option\n",
    "read_parquet=(spark\n",
    "              .read.format(\"parquet\")\n",
    "              # setzte die mergeSchema auf true/false um den Unterschied beim Einlesen zu sehen\n",
    "              # Vegleiche: https://spark.apache.org/docs/latest/sql-data-sources-parquet.html#schema-merging\n",
    "              #.option(\"mergeSchema\", \"false\")\n",
    "              .load(f\"s3://{bucket}/parquet\")\n",
    "             )\n",
    "\n",
    "read_parquet.printSchema()\n",
    "read_parquet.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244785ff",
   "metadata": {},
   "source": [
    "### Parquet: Schema Enforcement\n",
    "Schema Enforcement sorgt dafür, dass in ein bestehendes Schema keine Daten mit falschen Typen geschrieben werden können"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3dad2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datensatz mit falschem Datentyp anfügen\n",
    "df2a=(df2.where(f.col(\"account\")==\"peter\").withColumn(\"id\", f.col(\"id\").cast(\"string\")))\n",
    "\n",
    "\n",
    "# Zeile mit falschem Typ anfügen\n",
    "write_parquet=(df2a\n",
    "           .write\n",
    "           .partitionBy(\"account\")\n",
    "           .format(\"parquet\")\n",
    "           .mode(\"append\") # append\n",
    "           .save(f\"s3://{bucket}/parquet\")\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660c9fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# einlesen mit der mergeSchema Option\n",
    "read_parquet=(spark\n",
    "              .read.format(\"parquet\")\n",
    "              # setzte die mergeSchema auf true/false um den Unterschied beim Einlesen zu sehen\n",
    "              .option(\"mergeSchema\", \"false\")\n",
    "              .load(f\"s3://{bucket}/parquet\")\n",
    "             )\n",
    "\n",
    "read_parquet.printSchema()\n",
    "read_parquet.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab7dd65",
   "metadata": {},
   "source": [
    "#### Erkenntnisse Parquet\n",
    "* Sind Parquet Dateien selbsterklärend (haben ein Spalten und Typenschema )\n",
    "* Partitioning and Partion Discovery: werden die Daten in Verzeichnisse geschriebe und wieder als Partitionen erkannt?\n",
    "* Schema Evolution: Kann das Schema erweitert werden, also eine neue Spalte angefügt werden?\n",
    "* Schema Enforcement on write: Kann eine Spalte mit falschem Datetyp einfach beim schreiben hinzugefügt werden? \n",
    "* Schema Enforcement on read: Kann ein Verzeichnis mit mehreren Parquet Dateien bei der eine Spalte ein anderes Schema hat gelesen werden?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80ea102",
   "metadata": {},
   "source": [
    "<hr style=\"height: 3px; background: gray;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eadd53c",
   "metadata": {},
   "source": [
    "# Delta\n",
    "Das Delta Format fügt Parquet Dateien einen zusätzlichen Layer an Metadatan hinzu und erfüllt mit dem entsprechenden Treiber alle ACID Eigenschaften einer Datenbank und mehr. \n",
    "\n",
    "A = **Atomic** heißt alle Datenänderungen werden wie eine einzige Operation verarbeitet. Dies bedeutet, dass entweder alle Änderungen durchgeführt werden oder keine. Wenn das schreiben also mitten drin Fehlschlägt werden alle Daten dieser Schreiboperation die bereits geschrieben wurden wieder entfernt, bzw. nicht als erfolgreich geschrieben markiert.  \n",
    "T = **Consistency** bedeutet, wenn eine Transaktion beginnt und wenn eine Transaktion endet, befinden sich die Daten in einem konsistenten Zustand.  \n",
    "I = **Isolation** und bedeutet, dass der Übergangszustand einer Transaktion für andere Transaktionen nicht sichtbar ist. Dies führt dazu, dass Transaktionen, die gleichzeitig ablaufen, sich nicht gegenseitig beeinflussen oder blockieren.  \n",
    "D =**Durability** heißt, das die Datenänderungen nach erfolgreich abgeschlossener Transaktion erhalten bleiben und werden nicht rückgängig gemacht werden, selbst wenn ein Systemausfall auftritt.  \n",
    "\n",
    "**Time Travel** bei Delta bedeutet, dass jede Datenänderung als eigene Version aufgezeichnet wird und jederzeit zu einer alten Version zurück gegegangen werden kann.\n",
    "\n",
    "### Aufgabe:\n",
    "Wiederhole die gleichen Schritte mit dem DELTA Format und schaue wie sich hier Schema und neue Spalten verhalten\n",
    "\n",
    "1. Datenset 1 als DELTA schreiben (.format(\"delta\") und Pfad= .save(f\"s3://{bucket}/delta\"))\n",
    "2. Dateien und Inhalt anzeigen, vestehen was da passiert ist\n",
    "3. Metadaten und Deltalog in Datei verstehen\n",
    "3. Daten wieder einlese und checken ob es ein Schema und Spaltennamen gibt\n",
    "4. Schema Evolutiuon: Datenset 3 anfügen mit neuer Spalte anfügen\n",
    "5. Daten wieder einlesen und checken was mit der neuen Spalte passiert\n",
    "6. Partion & Pushdown Filter: Execution Plan für verschiedene Filter anzeigen\n",
    "6. Schema Enforcement: Datentyp in bestehender Spalte ändern und schauen ob und wie dies gehandhabt wird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513d213c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schreibe die Daten df1 als Delta Datei in den Pfad bucket/delta\n",
    "write_delta=(df1\n",
    "           .write\n",
    "           .format(\"delta\")\n",
    "           #.option(\"mergeSchema\", \"false\")\n",
    "           .mode(\"overwrite\") \n",
    "           .save(f\"s3://{bucket}/delta\")\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cd6f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls(bucket,\"delta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da732ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls(bucket,\"delta/_delta_log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3dbb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# untersuche die Dateien des Deltalogs und versuche zu verstehen wie die Versionierung und Schema Validierung damit funktioniert\n",
    "cat(bucket,\"delta/_delta_log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a714c074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lese die gerade geschriebenen Delta Tabelle wieder ein und zeige sie an, überprüfe das Schema\n",
    "# <<HIER EIGENE CODE EINFÜGEN>>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba9e1f8",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> &#8964 Lösung DELTA einlesen</summary>\n",
    "<p>\n",
    "<code>\n",
    "read_delta=spark.read.format(\"delta\").load(f\"s3://{bucket}/delta\")\n",
    "read_delta.show()\n",
    "</code>\n",
    "</details>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649e7d11",
   "metadata": {},
   "source": [
    "### Delta: Schema Evolution\n",
    "**Aufgabe:** Verstehe die Option *mergeSchema* on write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d72dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zeile mit zusätzlicher Spalte anfügen (df3)\n",
    "write_delta=(df3\n",
    "           .write\n",
    "           .format(\"delta\")\n",
    "           # Bei Delta kann bein Schreiben gesetzt werden ob die Tabelle erweitert werden soll oder nicht, Default ist false. \n",
    "           # Führe den Code zuerst ohne diese Option aus und schaue das Ergebnis, an \n",
    "           #.option(\"mergeSchema\", \"false\")\n",
    "           .mode(\"append\") # append\n",
    "           .save(f\"s3://{bucket}/delta\")\n",
    "          )\n",
    "\n",
    "\n",
    "# überprüfe ob die neue Spalte korrekt angefügt wurde\n",
    "read_delta=spark.read.format(\"delta\").load(f\"s3://{bucket}/delta\")\n",
    "read_delta.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bf92d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat(bucket,\"delta/_delta_log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae03a75",
   "metadata": {},
   "source": [
    "### Delta: Schema Enforcement\n",
    "Schema Enforcement bedeutet soll garantieren, dass keine Daten mit falschen Datentyp der Tabelle abgefügt werden  \n",
    "\n",
    "**Aufgabe:** verstehe die Option *mergeSchema* im Kontext von Datentyp Änderungen bei bestehenden Spalten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca43040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# füge eine Zeile mit falschen Datetyp für eine bestehenden Spalte an\n",
    "write_delta=(df2\n",
    "           # Eine Zeile aus dem df2 filtern\n",
    "           .where(f.col(\"account\")==\"peter\")\n",
    "           # Bestehenden Spaltentyp ändern\n",
    "           .withColumn(\"id\", f.col(\"id\").cast(\"string\"))\n",
    "           .write\n",
    "           .format(\"delta\")\n",
    "           # Bei Delta kann bein Schreiben gesetzt werden ob die Tabelle erweitert werden kann oder nicht, Default ist false\n",
    "           #.option(\"mergeSchema\", \"false\")\n",
    "           .mode(\"overwrite\") # append\n",
    "           .save(f\"s3://{bucket}/delta\")\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11007f7c",
   "metadata": {},
   "source": [
    "### Delta: Schema Replacement\n",
    "Delta bieten die Möglichkeit das Schema einer Tabelle zu ändern, also z.B. eine bestehende Spalte umzubenennen und deren Datentyp zu ändern.   \n",
    "Dokumentation mit Beispielen: https://docs.delta.io/latest/delta-batch.html#replace-table-schema  \n",
    "\n",
    "**Aufgabe:** Ändere in der bestehenden Tabelle den Datentyp der Spalte `id` auf `string` und den Spaltennamen `new` zu `comment`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8608f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update das Schema der bestehenen Delta Dateien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d0905b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee87f0a1",
   "metadata": {},
   "source": [
    "### Delta: History und Metadaten\n",
    "Im Delta Log werde alle Transaktionen mit zahlreichen Metadaten gespeichert. Das Spark Modul, der Treiber, um diese Daten auszulesen bietet zahlreiche Möglichkeiten diese Daten zu analysieren\n",
    "\n",
    "**Aufgabe:** Lese den History Log ein und verstehe was in den einzelnen Attributen steht. Treffe eine Auswahl der interessanten Informationen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb3480c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erzeuge ein DeltaTable Objekt was alle Zusatzeigenschaften von Delta bereitstellt\n",
    "deltaTable = DeltaTable.forPath(spark, f\"s3://{bucket}/delta\")\n",
    "\n",
    "# Historie aus den Delta Logs erzeugen\n",
    "fullHistoryDF = deltaTable.history() \n",
    "\n",
    "# Alle verfügbaren Spalten anzeigen\n",
    "fullHistoryDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac186df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wähle die wichtigen Felder aus der Historie aus und zeige sie an\n",
    "# fullHistoryDF.select(<<CODE EINFÜGEN>>).show(truncate=True)\n",
    "\n",
    "# Löse die genested Spalten in eigene Spalten auf und zeige folgende Attribute in eigenen Spalten `mode`, `numFiles` und `numOutputRows` \n",
    "#fullHistoryDF.select(<<CODE EINFÜGEN>>).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e5243a",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> &#8964 Lösung Historie analysieren</summary>\n",
    "<p>\n",
    "<code>\n",
    "deltaTable = DeltaTable.forPath(spark, f\"s3://{bucket}/delta\")\n",
    "fullHistoryDF = deltaTable.history() \n",
    "fullHistoryDF.select(\"version\",\"readVersion\",\"timestamp\",\"userId\",\"operation\",\"operationParameters\",\"operationMetrics\",\"userMetadata\").show(truncate=False)\n",
    "\n",
    "# Zugriff auf genested struc Objecte mit `spalte.attributname`\n",
    "fullHistoryDF.select(\"version\",\"readVersion\",\"timestamp\",\"operation\",\"operationParameters.mode\",\"operationMetrics.numFiles\",\"operationMetrics.numOutputRows\").show(truncate=False)\n",
    "read_json.printSchema()\n",
    "read_json.show()\n",
    "    </code>\n",
    "</details>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f89cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erkunde die Metadaten aus der Funktion deltaTable.detail()\n",
    "\n",
    "#<<CODE EINFÜGEN>>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd35736",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> &#8964 Lösung Details anezigen</summary>\n",
    "<p>\n",
    "<code>\n",
    "deltaTable = DeltaTable.forPath(spark, f\"s3://{bucket}/delta\")\n",
    "detailsDF = deltaTable.detail()\n",
    "detailsDF.show()\n",
    "detailsDF.select(\"location\",\"numFiles\",\"tableFeatures\").show(truncate=False)\n",
    "</code>\n",
    "</details>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc61efc",
   "metadata": {},
   "source": [
    "### Delta: Time Travel\n",
    "Die Time Travel Funktion von Delta ermöglicht es den Zustand der Datentabelle zu einem bestimmten Zeitpunkt in der Vergangenheit wiederherzustellen oder Änderungen zu verfolgen.   \n",
    "Time Travel ermöglicht es, vorherige Versionen der Daten abzufragen und historische Analysen durchzuführen, ohne auf separate Backups oder Snapshots angewiesen zu sein.\n",
    "\n",
    "**Aufgabe:** lese verschiedenen Datenstände nach Versionsnummer oder Timestap ein.  \n",
    "Weitere Informationen hierzu finden sich in https://delta.io/blog/2023-02-01-delta-lake-time-travel/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6617afc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.format(\"delta\").option(<<CODE EINFÜGEN>>).load(f\"s3://{bucket}/delta\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55d249d",
   "metadata": {},
   "source": [
    "### Delta: Merge (Upsert)\n",
    "Delta bietet die Möglichkeiten auf bestehenden Dateien ein Daten Upsert durchzuführen.   \n",
    "Ubsert oder Merge bedeutet zu prüfen ob es die neue Zeile für einen bestimmten Schlüssel schon in den Daten gibt und wenn ja diese zu updaten und wenn nein sie neu hinzuzufügen\n",
    "\n",
    "**Aufgabe:** Merge den Datensatz df2 mit dem der Änderung der `balance` für `Alex` und überprüfe ob das Update korrekt war"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aea3fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "deltaTable2 = DeltaTable.forPath(spark, f\"s3://{bucket}/delta\")\n",
    "\n",
    "# Spalte anfügen, da merge nur funktioniert wenn das Schema stimmt\n",
    "df2a=df2.withColumn(\"new\",f.lit(\"test\"))\n",
    "\n",
    "print(\"++ Datensatz der auf bestehende Daten upserted/merged werden soll\")\n",
    "df2a.show()\n",
    "print(\"++ Bestehender Datensatz auf s3\")\n",
    "deltaTable2.toDF().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1688108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verwendung der merge Funktion (es gibt auch eine update() oder delete() Funktion)\n",
    "dt3=(deltaTable2.alias(\"oldData\")\n",
    "      .merge(df2a.alias(\"newData\"),\n",
    "            \"oldData.account = newData.account AND oldData.dt_transaction = newData.dt_transaction\")\n",
    "            .whenMatchedUpdateAll()\n",
    "            .whenNotMatchedInsertAll()\n",
    "      .execute()\n",
    "    )\n",
    "\n",
    "deltaTable2.toDF().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da9f192",
   "metadata": {},
   "source": [
    "### Delta: Roleback\n",
    "Delta bietet die Möglichkeiten direkt auf den Datenstand einer bestimmten Version oder eines Zeitpunktes zurück zu gehen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225593d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verwende die DeltaTable Funktion restoreToVersion(0) oder restoreToTimestamp(\"yyyy-mm-dd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85fcee1",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> &#8964 Hilfreiche Befehle</summary>\n",
    "<p>\n",
    "<code>\n",
    "deltaTable2.toDF().show()\n",
    "\n",
    "deltaTable2.restoreToVersion(0)\n",
    "#restoreToTimestamp\n",
    "#isDeltaTable\n",
    "deltaTable2.toDF().show()\n",
    "spark.read.format(\"delta\").load(f\"s3://{bucket}/delta\").show()\n",
    "\n",
    "</code>\n",
    "</details>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe344799",
   "metadata": {},
   "source": [
    "#### Erkenntnisse Delta\n",
    "* Wie funktioniert das Metadatenmanagement und was steht im Delta Log?\n",
    "* Schema Evolution: Kann das Schema erweitert werden, also eine neue Spalte angefügt werden?\n",
    "* Schema Enforcement on write: Kann eine Spalte mit falschem Datetyp einfach beim schreiben hinzugefügt werden? \n",
    "* Schema Enforcement on read: Kann ein Verzeichnis mit mehreren Parquet Dateien bei der eine Spalte ein anderes Schema hat gelesen werden? (um die Ecke Denk Frage)\n",
    "* Was ermöglichen mir die Metadaten (Historie, Audit etc)\n",
    "* Was ist der Vorteil der Merge Funktion? Wie müsste ich sonst Dateibasiert einen Merge/Update durchführen?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5956eb",
   "metadata": {},
   "source": [
    "<hr style=\"height: 3px; background: gray;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b85012",
   "metadata": {},
   "source": [
    "## Iceberg\n",
    "Das Iceberg Format Ist vergleichbar mit dem Delta Format Parquet, Avro oder ORC Dateien einen zusätzlichen Layer an Metadatan hinzu und erfüllt mit dem entsprechenden Treiber alle ACID Eigenschaften einer Datenbank.   \n",
    "**Unterschied zu Delta:**  \n",
    "* Das Delta Format verwendet ein Transaktionsprotokoll, um inkrementelle Updates zu erfassen und eine vollständige Versionierung der Daten zu ermöglichen. Dies bedeutet, dass Delta Änderungen an den Daten in einem einzelnen Parquet-Dateisystem erfasst. Delta ist eng mit Spark und der Delta Lake Plattform von Databricks verbunden und verwedet als darunterliegendes Datenformat immer Parqeut Dateien\n",
    "* Iceberg hat ebenfalls eine append-only-Architektur, bei der Daten in sogenannten \"Snapshot-Dateien\" gespeichert werden. Iceberg ist älter und unabhängig von einer speziellen Plattform. Daher es gibt wesentlich mehr Big Data Engines die Iceberg voll unterstützen (Spark, Hive, Trino, Drill, Presto).\n",
    "\n",
    "**Naming Unterschiede:**  \n",
    "Bei Delta spricht man von `Versionen` und `Time Travel`. \n",
    "Bei Iceberg heißt dies `Snapshots` und `Snapshot roleback`  \n",
    "\n",
    "Eine weitere Besonderheit bei Iceberg ist, dass es für Daten die Möglichkeiten von Branching und Tagging gibt. Es können also Variationen der Daten in einem z.B. Development Branch gepflegt und bearbeitet weiter entwickelt werden (https://iceberg.apache.org/docs/latest/branching/)\n",
    "\n",
    "Im Unterchied zu Delta benötigt Spark zum arbeiten mit Iceberg immer einen Catalog oder Metastore um Details über die Dateien abzulegen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db30780",
   "metadata": {},
   "source": [
    "### Iceberg: Catalog und Metastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cb24f4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|      catalog|\n",
      "+-------------+\n",
      "|          ice|\n",
      "|spark_catalog|\n",
      "+-------------+\n",
      "\n",
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "|  iceberg|\n",
      "+---------+\n",
      "\n",
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "|  iceberg|  iceberg|      false|\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Anzeigen der in Spark eingebundenen Metadaten Cataloge\n",
    "# Diese Spark Session wurde mit einer Anbindung an den Hive Metastore gestartet\n",
    "## Konfiguration aus für die Spark Session erzeugt einen Catalog ice vom Typ hive\n",
    "## conf.set(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\")\n",
    "## conf.set(\"spark.sql.catalog.ice\",\"org.apache.iceberg.spark.SparkCatalog\") \n",
    "## conf.set(\"spark.sql.catalog.ice.type\",\"hive\") \n",
    "## conf.set(\"spark.sql.catalog.ice.uri\",\"thrift://hive-metastore.hive.svc.cluster.local:9083\") \n",
    "\n",
    "\n",
    "spark.sql(\"SHOW catalogs\").show()\n",
    "spark.sql(\"SHOW databases from ice\").show()\n",
    "spark.sql(\"show tables from ice.iceberg\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7377b8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zunächst muss in dem Ice Catalog eine Datenbankabstraktion angelegt werden mit dem s3 Pfad wo die Daten zu dieser Tabellen liegen\n",
    "# create a Database(name=<db_name>, locationUri='s3a://<bucket>/')\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS ice.iceberg LOCATION 's3a://{bucket}/'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b8232c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jetzt erscheint diese Datenbank im Iceberg Catalog\n",
    "spark.sql(\"SHOW databases from ice\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5010f061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Es gibt aber noch keine Tabellen in dieser Datenbankabstraktion\n",
    "spark.sql(\"show tables from ice.iceberg\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cba4b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Die Tabelle kann ach wieder aus dem Hive Metastore, aus dem Catalog gelöscht werden\n",
    "#spark.sql(\"drop table iceberg.iceberg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eafa7b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Um Daten unter Verwendung des Catalogs nach s3 zu schreiben wird nicht der direkte Pfad verwendet sondern die Datenbank Referenz wo der Pfad hinterlegt ist\n",
    "# Die Datenbank iceberg -> s3://bucket/\n",
    "# die Tabelle soll jetzt nach ->s3://bucket/iceberg\n",
    "# geschrieben wird mit der Methode `saveAsTable(\"datenbank.tabelle\")` und der Tabellennamen wir dann als Prefix auf s3 verwendet\n",
    "write_iceberg=(df1\n",
    "                  .write\n",
    "                  .format(\"iceberg\")\n",
    "                  .mode(\"overwrite\")\n",
    "                  .saveAsTable(\"ice.iceberg.iceberg\")\n",
    "               )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494fb421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Versuche auf dem Dateisystem mit ls und cat die Struktur der Metadaten zu verstehen\n",
    "# Dieser Artikel kann dabei helfen https://medium.com/snowflake/understanding-iceberg-table-metadata-b1209fbcc7c3\n",
    "ls(bucket,\"iceberg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c844c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zeige die Daten wieder an\n",
    "iceberg_df = spark.read.table(\"ice.iceberg.iceberg\")\n",
    "iceberg_df.show()\n",
    "\n",
    "# Da es sich bereits um eine Tabellenabstraktion handelt, kann auch direkt mit Spark SQL gelesen werden\n",
    "spark.sql(\"SELECT * FROM ice.iceberg.iceberg;\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a6911c",
   "metadata": {},
   "source": [
    "### Iceberg: Schema Evolution\n",
    "**Aufgabe:** Verstehe wie eine Spaltenerweiterung bei Iceberg gemacht wird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92013c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Füge Datensatz 2 mit einer zusätzlichen Spalte hinzu\n",
    "write_iceberg=(df3\n",
    "               .write\n",
    "               .format(\"iceberg\")\n",
    "               #.option(\"mergeSchema\",\"true\")\n",
    "               .mode(\"append\")\n",
    "               .saveAsTable(\"ice.iceberg.iceberg\")\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3946b31c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Um eine neue Spalte hinzuzufügen muss das Schema wie bei einer SQL Tabelle zuerst geändert werden. Es gibt keine automatische Anpassung wie bei Delta via die Option mergeSchema\n",
    "# Ändere das Schema mit folgendem Befehl und führe dann obige Schreiboperation nochmal durch\n",
    "spark.sql(\"ALTER TABLE ice.iceberg.iceberg ADD COLUMNS (new VARCHAR(50))\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a31c068c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------+-------+-------------+\n",
      "| id|account|dt_transaction|balance|          new|\n",
      "+---+-------+--------------+-------+-------------+\n",
      "|  1|   otto|    2019-10-01|   4444|neue Spalte 1|\n",
      "|  1|   alex|    2019-01-01|   1000|         null|\n",
      "|  2|   alex|    2019-02-01|   1500|         null|\n",
      "|  4|  maria|    2020-01-01|   5000|         null|\n",
      "|  3|   alex|    2019-03-01|   1700|         null|\n",
      "+---+-------+--------------+-------+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM ice.iceberg.iceberg;\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab84274",
   "metadata": {},
   "source": [
    "### Iceberg: Schema Enforcement\n",
    "\n",
    "**Aufgabe:** verstehe wie Schema Enforcement bei Iceberg funktioniert. Korrigiere den Code bis die Zeile korrekt angefügt werden kann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e813db3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# füge eine Zeile mit falschen Datetyp für eine bestehenden Spalte an\n",
    "try:\n",
    "    write_iceberg=(df2\n",
    "           # Eine Zeile aus dem df2 filtern\n",
    "           .where(f.col(\"account\")==\"peter\")\n",
    "           # Bestehenden Spaltentyp ändern\n",
    "           .withColumn(\"id\", f.col(\"id\").cast(\"string\"))\n",
    "           #.withColumn(\"new\", f.lit(\"peter\").cast(\"string\"))\n",
    "           .write\n",
    "           .format(\"iceberg\")\n",
    "           .mode(\"append\") # append\n",
    "           .saveAsTable(\"ice.iceberg.iceberg\")\n",
    "          )\n",
    "    print(\"Zeile angefügt\")\n",
    "except Exception as error:\n",
    "    print(\"ERROR Writing to Iceberg\")\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75633ab2",
   "metadata": {},
   "source": [
    "In Iceberg kann der Typ einer bestehenden Tabelle nicht geändert werden.   \n",
    "Einzige Möglichkeit ist es die Tabelle einzulesen, neu zu casten und in einen neue Tabelle zu schreiben.   \n",
    "Diese kann dann anschließend wieder umbenannt werden auf den alten Namen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fbbf4d",
   "metadata": {},
   "source": [
    "### Iceberg: History und Metadaten\n",
    "Iceberg bietet zahlreiche Funktionen um die verschiedenen Metadaten auszulesen.   \n",
    "Die Funktionen werden immer an die Tabelle angefügt und funktionieren sowohl in der Dataframe als auch in der SQL Syntax\n",
    "```\n",
    "spark.read.table(\"ice.iceberg.iceberg.history\").show()\n",
    "spark.sql(\"SELECT * FROM ice.iceberg.iceberg.history;\").show()\n",
    "```\n",
    "\n",
    "**Aufgabe:** Untersuche und verstehe die Metadaten folgender Methoden:  \n",
    "`history`, `files`, `snapshots`, `manifests`, `partitions`\n",
    "\n",
    "Weitere Informationen zu den Metadaten und dem Zugrif darauf finden sich in der Dokumentation\n",
    "https://iceberg.apache.org/docs/latest/spark-queries/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74d837af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "|     made_current_at|        snapshot_id|          parent_id|is_current_ancestor|\n",
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "|2023-06-29 15:19:...|4422470565428377895|               null|               true|\n",
      "|2023-06-29 15:19:...|7483311904055840977|4422470565428377895|               true|\n",
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# << EIGENEN CODE EINFÜGEN >>\n",
    "spark.read.table(\"ice.iceberg.iceberg.history\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dca46af",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> &#8964 Lösung Historie analysieren</summary>\n",
    "<p>\n",
    "<code>\n",
    "spark.sql(\"SELECT * FROM iceberg.iceberg.history;\").show()\n",
    "spark.sql(\"SELECT * FROM iceberg.iceberg.files;\").show()\n",
    "spark.sql(\"SELECT * FROM iceberg.iceberg.snapshots;\").show()\n",
    "\n",
    "## alternative syntax example:\n",
    "spark.read.format(\"iceberg\").load(\"iceberg.iceberg.files\").show()\n",
    "</code>\n",
    "</details>\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6db241",
   "metadata": {},
   "source": [
    "### Iceberg: Iceberg: Time Travel\n",
    "Bei Iceberg gibt es statt Versionen Snapshots und einige fein granularere Möglichkeiten in der Daten Historie zurück zu gehen.\n",
    "z.B. können alle Time Travel Operationne auch auf die Metadaten angewendet werden. Es kann also nachgeschaut werden wie die Metadaten zu einem bestimmten Zeitpunkt/Snapshot ausgesehen haben.\n",
    "\n",
    "- ```snapshot-id``` selects a specific table snapshot\n",
    "- ```as-of-timestamp``` selects the current snapshot at a timestamp, in milliseconds\n",
    "- ```branch``` selects the head snapshot of the specified branch. Note that currently branch cannot be combined with as-of-timestamp.\n",
    "- ```tag``` selects the snapshot associated with the specified tag. Tags cannot be combined with as-of-timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c2d137e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------+-------+\n",
      "| id|account|dt_transaction|balance|\n",
      "+---+-------+--------------+-------+\n",
      "|  1|   alex|    2019-01-01|   1000|\n",
      "|  2|   alex|    2019-02-01|   1500|\n",
      "|  4|  maria|    2020-01-01|   5000|\n",
      "|  3|   alex|    2019-03-01|   1700|\n",
      "+---+-------+--------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from the results of iceberg_table.snapshots get the snapshots IDs\n",
    "snapshot1 = spark.read \\\n",
    "                 .option(\"snapshot-id\", \"<<Valide Snapshot ID eintragen>>\") \\\n",
    "                 .format(\"iceberg\") \\\n",
    "                 .load(\"ice.iceberg.iceberg\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da46097d",
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot2 = spark.read \\\n",
    "                 .option(\"snapshot-id\", \"<<Valide Snapshot ID eintragen>>\") \\\n",
    "                 .format(\"iceberg\") \\\n",
    "                 .load(\"ice.iceberg.iceberg\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0654a9e",
   "metadata": {},
   "source": [
    "<hr style=\"height: 3px; background: gray;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fcb4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bac1ce7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
