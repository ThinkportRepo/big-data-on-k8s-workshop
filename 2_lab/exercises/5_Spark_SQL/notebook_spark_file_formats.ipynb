{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "610d267c",
   "metadata": {},
   "source": [
    "# Aufgaben zu Big Data File Formats\n",
    "\n",
    "Folgende Aufgaben haben zum Ziel mit den verschiedenen Dateiformaten vertraut zu werden und insbesondere die speziellen Eigenschaften und Funktionen der Formate zu verstehen\n",
    "\n",
    "### CSV and JSON\n",
    "Klassische Datei Formate für Datenverarbeitung  \n",
    "**Typische Eigenschaften:** Einfache Struktur, human-readable, Zeilenformat\n",
    "\n",
    "### Avro, ORC, Parquet\n",
    "Big Data optimierte Formate um schnell große Datenmengen zu lesen und zu schreiben  \n",
    "**Typische Eigenschaften:** teilbar in kleine Dateien (splittable), komprimierbar (compressible), überspringbar (skippable), selbsterklärend (self describing with schema), Schema erweiterbar (Schema Evolution), Filter Pushdown\n",
    "\n",
    "### Delta, Iceberg, Hudi\n",
    "Erweiterte Big Data Formate um die ACID und Tracing Eigenschaften einer klassichen SQL Datenbank zu erfüllen  \n",
    "**Typische Eigenschaften:** Erweiterung um zusätzliche Metadaten und spezielle Treiber zum lesen/schreiben, Time Travel Funktion, Merge und Update Funktionen, Audit Log Funktionalitäten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592e56dd",
   "metadata": {},
   "source": [
    "## 1. Import Python Modules\n",
    "Hier werden alle benötigten Libraries für dieses Lab heruntergeladen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0aa4fda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container {width:100% !important; }<style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "from delta import *\n",
    "\n",
    "import datetime\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import json\n",
    "import csv\n",
    "\n",
    "# use 95% of the screen for jupyter cell\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container {width:100% !important; }<style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3ba21e",
   "metadata": {},
   "source": [
    "## 2. Launch Spark Jupyter and Configuration\n",
    "\n",
    "#### Configure a Spark session for Kubernetes cluster with S3 support\n",
    "### CLUSTER MANAGER\n",
    "- set the Kubernetes master URL as Cluster Manager(“k8s://https://” is NOT a typo, this is how Spark knows the “provider” type)\n",
    "\n",
    "### KUBERNETES\n",
    "- set the namespace that will be used for running the driver and executor pods\n",
    "- set the docker image from which the Worker/Exectutor pods are created\n",
    "- set the Kubernetes service account name and provide the authentication details for the service account (required to create worker pods)\n",
    "\n",
    "### SPARK\n",
    "- set the driver host and the driver port (find name of the driver service with 'kubectl get services' or in the helm chart configuration)\n",
    "- enable Delta Lake, Iceberg, and Hudi support by setting the spark.sql.extensions\n",
    "- configure Hive catalog for Iceberg\n",
    "- enable S3 connector\n",
    "- set the number of worker pods, their memory and cores (HINT: number of possible tasks = cores * executores)\n",
    "\n",
    "### SPARK SESSION\n",
    "- create the Spark session using the SparkSession.builder object\n",
    "- get the Spark context from the created session and set the log level to \"ERROR\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1faf0688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f9f9068",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.kubernetes.namespace = frontend\n",
      "spark.sql.extensions = io.delta.sql.DeltaSparkSessionExtension, org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions, org.apache.spark.sql.hudi.HoodieSparkSessionExtension\n",
      "spark.master = k8s://https://kubernetes.default.svc.cluster.local:443\n",
      "spark.app.name = jupyter-spark\n",
      "spark.executor.memory = 1G\n",
      "spark.executor.cores = 2\n",
      "spark.driver.host = jupyter-spark-driver.frontend.svc.cluster.local\n"
     ]
    }
   ],
   "source": [
    "appName=\"jupyter-spark\"\n",
    "\n",
    "conf = SparkConf()\n",
    "\n",
    "# CLUSTER MANAGER\n",
    "\n",
    "conf.setMaster(\"k8s://https://kubernetes.default.svc.cluster.local:443\")\n",
    "\n",
    "# CONFIGURE KUBERNETES\n",
    "\n",
    "conf.set(\"spark.kubernetes.namespace\",\"frontend\")\n",
    "conf.set(\"spark.kubernetes.container.image\", \"thinkportgmbh/workshops:spark-3.3.2\")\n",
    "conf.set(\"spark.kubernetes.container.image.pullPolicy\", \"Always\")\n",
    "\n",
    "conf.set(\"spark.kubernetes.authenticate.driver.serviceAccountName\", \"spark\")\n",
    "conf.set(\"spark.kubernetes.authenticate.caCertFile\", \"/var/run/secrets/kubernetes.io/serviceaccount/ca.crt\")\n",
    "conf.set(\"spark.kubernetes.authenticate.oauthTokenFile\", \"/var/run/secrets/kubernetes.io/serviceaccount/token\")\n",
    "\n",
    "# CONFIGURE SPARK\n",
    "\n",
    "conf.set(\"spark.sql.session.timeZone\", \"Europe/Berlin\")\n",
    "conf.set(\"spark.driver.host\", \"jupyter-spark-driver.frontend.svc.cluster.local\")\n",
    "conf.set(\"spark.driver.port\", \"29413\")\n",
    "\n",
    "conf.set(\"spark.jars\", \"/opt/spark/jars/spark-avro_2.12-3.3.2.jar\")\n",
    "conf.set(\"spark.driver.extraClassPath\",\"/opt/spark/jars/spark-avro_2.12-3.3.2.jar\")\n",
    "conf.set(\"spark.executor.extraClassPath\",\"/opt/spark/jars/spark-avro_2.12-3.3.2.jar\")\n",
    "\n",
    "conf.set(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension, org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions, org.apache.spark.sql.hudi.HoodieSparkSessionExtension\")\n",
    "\n",
    "######## Hive als Metastore einbinden\n",
    "conf.set(\"hive.metastore.uris\", \"thrift://hive-metastore.hive.svc.cluster.local:9083\") \n",
    "\n",
    "######## Iceberg configs\n",
    "conf.set(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\")\n",
    "conf.set(\"spark.sql.catalog.ice\",\"org.apache.iceberg.spark.SparkCatalog\") \n",
    "conf.set(\"spark.sql.catalog.ice.type\",\"hive\") \n",
    "conf.set(\"spark.sql.catalog.ice.uri\",\"thrift://hive-metastore.hive.svc.cluster.local:9083\") \n",
    "\n",
    "\n",
    "####### Hudi configs\n",
    "conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "\n",
    "# CONFIGURE S3 CONNECTOR\n",
    "conf.set(\"spark.hadoop.fs.s3a.endpoint\", \"minio.minio.svc.cluster.local:9000\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.access.key\", \"trainadm\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.secret.key\", \"train@thinkport\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "conf.set(\"spark.hadoop.fs.s3.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "\n",
    "# CONFIGURE WORKER (Customize based on workload)\n",
    "\n",
    "conf.set(\"spark.executor.instances\", \"1\")\n",
    "conf.set(\"spark.executor.memory\", \"1G\")\n",
    "conf.set(\"spark.executor.cores\", \"2\")\n",
    "\n",
    "# SPARK SESSION\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .config(conf=conf) \\\n",
    "    .config('spark.sql.session.timeZone', 'Europe/Berlin') \\\n",
    "    .appName(appName)\\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "sc=spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "# get the configuration object to check all the configurations the session was startet with\n",
    "for entry in sc.getConf().getAll():\n",
    "        if entry[0] in [\"spark.app.name\",\"spark.kubernetes.namespace\",\"spark.executor.memory\",\"spark.executor.cores\",\"spark.driver.host\",\"spark.master\",\"spark.sql.extensions\"]:\n",
    "            print(entry[0],\"=\",entry[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27495f26",
   "metadata": {},
   "source": [
    "## Create sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "e1b8b7c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++ create new dataframe and show schema and data\n",
      "################################################\n",
      "+---+-------+--------------+-------+\n",
      "|id |account|dt_transaction|balance|\n",
      "+---+-------+--------------+-------+\n",
      "|1  |alex   |2019-01-01    |1000   |\n",
      "|2  |alex   |2019-02-01    |1500   |\n",
      "|4  |maria  |2020-01-01    |5000   |\n",
      "|3  |alex   |2019-03-01    |1700   |\n",
      "+---+-------+--------------+-------+\n",
      "\n",
      "+---+-------+--------------+-------+\n",
      "|id |account|dt_transaction|balance|\n",
      "+---+-------+--------------+-------+\n",
      "|1  |alex   |2019-03-01    |3300   |\n",
      "|2  |peter  |2021-01-01    |100    |\n",
      "+---+-------+--------------+-------+\n",
      "\n",
      "+---+-------+--------------+-------+-------------+\n",
      "|id |account|dt_transaction|balance|new          |\n",
      "+---+-------+--------------+-------+-------------+\n",
      "|1  |otto   |2019-10-01    |4444   |neue Spalte 1|\n",
      "+---+-------+--------------+-------+-------------+\n",
      "\n",
      "+---+-------+--------------+-------+\n",
      "|id |account|dt_transaction|balance|\n",
      "+---+-------+--------------+-------+\n",
      "|5  |markus |2019-09-01    |555    |\n",
      "+---+-------+--------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# initial Daten\n",
    "account_data1 = [\n",
    "    (1,\"alex\",\"2019-01-01\",1000),\n",
    "    (2,\"alex\",\"2019-02-01\",1500),\n",
    "    (3,\"alex\",\"2019-03-01\",1700),\n",
    "    (4,\"maria\",\"2020-01-01\",5000)\n",
    "    ]\n",
    "\n",
    "# Datensatz mit einem Update und einer neuen Zeile\n",
    "account_data2 = [\n",
    "    (1,\"alex\",\"2019-03-01\",3300),\n",
    "    (2,\"peter\",\"2021-01-01\",100)\n",
    "    ]\n",
    "\n",
    "# Datensatz mit neuer Zeile und neuer Spalte\n",
    "account_data3 = [\n",
    "    (1,\"otto\",\"2019-10-01\",4444,\"neue Spalte 1\")\n",
    "]\n",
    "\n",
    "# Datensatz mit neuer Zeile und neuer Spalte\n",
    "account_data4 = [\n",
    "    (5,\"markus\",\"2019-09-01\",555)\n",
    "]\n",
    "\n",
    "schema = [\"id\",\"account\",\"dt_transaction\",\"balance\"]\n",
    "schema3 = [\"id\",\"account\",\"dt_transaction\",\"balance\",\"new\"]\n",
    "\n",
    "df1 = spark.createDataFrame(data=account_data1, schema = schema).withColumn(\"dt_transaction\",col(\"dt_transaction\").cast(\"date\")).repartition(3)\n",
    "df2 = spark.createDataFrame(data=account_data2, schema = schema).withColumn(\"dt_transaction\",col(\"dt_transaction\").cast(\"date\")).repartition(2)\n",
    "df3 = spark.createDataFrame(data=account_data3, schema = schema3).withColumn(\"dt_transaction\",col(\"dt_transaction\").cast(\"date\")).repartition(1)\n",
    "df4 = spark.createDataFrame(data=account_data4, schema = schema).withColumn(\"dt_transaction\",col(\"dt_transaction\").cast(\"date\")).withColumn(\"id\",col(\"id\").cast(\"string\")).repartition(1)\n",
    "\n",
    "\n",
    "print(\"++ create new dataframe and show schema and data\")\n",
    "print(\"################################################\")\n",
    "\n",
    "# df1.printSchema()\n",
    "df1.show(truncate=False)\n",
    "df2.show(truncate=False)\n",
    "df3.show(truncate=False)\n",
    "df4.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379a3b6a",
   "metadata": {},
   "source": [
    "## Configure boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d161c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hilfsfunktionen um mit einfachen Befehlen auf s3 zu arbeiten (s3 mb s3://fileformats)\n",
    "import boto3\n",
    "from botocore.client import Config\n",
    "\n",
    "# Bucket, muss zuerst in Minio oder via Terminal Befehl erstellt werden\n",
    "bucket = \"fileformats\"\n",
    "bucket_path=\"s3://\"+bucket\n",
    "\n",
    "options = {\n",
    "    'endpoint_url': 'http://minio.minio.svc.cluster.local:9000',\n",
    "    'aws_access_key_id': 'trainadm',\n",
    "    'aws_secret_access_key': 'train@thinkport',\n",
    "    'config': Config(signature_version='s3v4'),\n",
    "    'verify': False}\n",
    "\n",
    "s3_resource = boto3.resource('s3', **options)  \n",
    "s3_client = boto3.client('s3', **options)\n",
    "\n",
    "# show files on s3 bucket/prefix\n",
    "def ls(bucket,prefix):\n",
    "    '''List objects from bucket/prefix'''\n",
    "    try:\n",
    "        for obj in s3_resource.Bucket(bucket).objects.filter(Prefix=prefix):\n",
    "            print(obj.key)\n",
    "    except Exception as e: \n",
    "        print(e)\n",
    "    \n",
    "# show file content in files\n",
    "def cat(bucket,prefix,binary=False):\n",
    "    '''Show content of one or several files with same prefix/wildcard'''\n",
    "    try:\n",
    "        for obj in s3_resource.Bucket(bucket).objects.filter(Prefix=prefix):\n",
    "            print(\"File:\",obj.key)\n",
    "            print(\"----------------------\")\n",
    "            if binary==True:\n",
    "                print(obj.get()['Body'].read())\n",
    "            else: \n",
    "                print(obj.get()['Body'].read().decode())\n",
    "            print(\"######################\")\n",
    "    except Exception as e: \n",
    "        print(e)\n",
    "\n",
    "# delete files from bucket\n",
    "def rm(bucket,prefix):\n",
    "    '''Delete everything from bucket/prefix'''\n",
    "    for object in s3_resource.Bucket(bucket).objects.filter(Prefix=prefix):\n",
    "        print(object.key)\n",
    "        s3_client.delete_object(Bucket=bucket, Key=object.key)\n",
    "    print(f\"Deleted files from {bucket}/{prefix}*\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e07436ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#############################\n",
      "#############################\n",
      "#############################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show everything in bucket\n",
    "ls(bucket,\"\")\n",
    "print(\"#############################\")\n",
    "# show folder\n",
    "ls(bucket,\"csv\")\n",
    "print(\"#############################\")\n",
    "# show subfolder\n",
    "ls(bucket,\"delta/_delta_log/\")\n",
    "print(\"#############################\")\n",
    "print(\"\")\n",
    "# show content of one or several files with same prefix/wildcard\n",
    "cat(bucket,'csv/part')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6fe705",
   "metadata": {},
   "source": [
    "## CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "916ad0c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Partitions: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"Number of Partitions:\", df1.rdd.getNumPartitions())\n",
    "\n",
    "# Schreibe Datenset 1 als CSV Datei\n",
    "write_csv=(df1\n",
    "           .write\n",
    "           .format(\"csv\")\n",
    "           .mode(\"overwrite\") # append\n",
    "           .save(f\"s3://{bucket}/csv\")\n",
    "          )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cebbc9fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv/_SUCCESS\n",
      "csv/part-00000-f2c3a348-3c01-472f-8f2a-cfd618a219cf-c000.csv\n",
      "csv/part-00001-f2c3a348-3c01-472f-8f2a-cfd618a219cf-c000.csv\n",
      "csv/part-00002-f2c3a348-3c01-472f-8f2a-cfd618a219cf-c000.csv\n"
     ]
    }
   ],
   "source": [
    "ls(bucket,\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82fb51b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: csv/part-00000-f2c3a348-3c01-472f-8f2a-cfd618a219cf-c000.csv\n",
      "----------------------\n",
      "1,alex,2019-01-01,1000\n",
      "\n",
      "######################\n",
      "File: csv/part-00001-f2c3a348-3c01-472f-8f2a-cfd618a219cf-c000.csv\n",
      "----------------------\n",
      "2,alex,2019-02-01,1500\n",
      "4,maria,2020-01-01,5000\n",
      "\n",
      "######################\n",
      "File: csv/part-00002-f2c3a348-3c01-472f-8f2a-cfd618a219cf-c000.csv\n",
      "----------------------\n",
      "3,alex,2019-03-01,1700\n",
      "\n",
      "######################\n"
     ]
    }
   ],
   "source": [
    "cat(bucket,\"csv/part\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f31d31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      "\n",
      "+---+-----+----------+----+\n",
      "|_c0|  _c1|       _c2| _c3|\n",
      "+---+-----+----------+----+\n",
      "|  2| alex|2019-02-01|1500|\n",
      "|  4|maria|2020-01-01|5000|\n",
      "|  1| alex|2019-01-01|1000|\n",
      "|  3| alex|2019-03-01|1700|\n",
      "+---+-----+----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lese csv Datei wieder ein und schaue mir Spaltennamen und Schema an\n",
    "read_csv=spark.read.format(\"csv\").load(f\"s3://{bucket}/csv\")\n",
    "\n",
    "read_csv.printSchema()\n",
    "read_csv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c5a5979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# schreibe Datenset 3 (neue Spalte) in die gleiche Tabelle dazu\n",
    "write_csv=(df3\n",
    "           .write\n",
    "           .format(\"csv\")\n",
    "           .mode(\"append\") # append\n",
    "           .save(f\"s3://{bucket}/csv\")\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a771362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv/_SUCCESS\n",
      "csv/part-00000-7e095edf-5062-4284-8dce-b3e018d3289c-c000.csv\n",
      "csv/part-00000-f2c3a348-3c01-472f-8f2a-cfd618a219cf-c000.csv\n",
      "csv/part-00001-7e095edf-5062-4284-8dce-b3e018d3289c-c000.csv\n",
      "csv/part-00001-f2c3a348-3c01-472f-8f2a-cfd618a219cf-c000.csv\n",
      "csv/part-00002-f2c3a348-3c01-472f-8f2a-cfd618a219cf-c000.csv\n"
     ]
    }
   ],
   "source": [
    "ls(bucket,\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "349331cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: csv/part-00000-7e095edf-5062-4284-8dce-b3e018d3289c-c000.csv\n",
      "----------------------\n",
      "\n",
      "######################\n",
      "File: csv/part-00000-f2c3a348-3c01-472f-8f2a-cfd618a219cf-c000.csv\n",
      "----------------------\n",
      "1,alex,2019-01-01,1000\n",
      "\n",
      "######################\n",
      "File: csv/part-00001-7e095edf-5062-4284-8dce-b3e018d3289c-c000.csv\n",
      "----------------------\n",
      "1,otto,2019-10-01,4444,neue Spalte 1\n",
      "\n",
      "######################\n",
      "File: csv/part-00001-f2c3a348-3c01-472f-8f2a-cfd618a219cf-c000.csv\n",
      "----------------------\n",
      "2,alex,2019-02-01,1500\n",
      "4,maria,2020-01-01,5000\n",
      "\n",
      "######################\n",
      "File: csv/part-00002-f2c3a348-3c01-472f-8f2a-cfd618a219cf-c000.csv\n",
      "----------------------\n",
      "3,alex,2019-03-01,1700\n",
      "\n",
      "######################\n"
     ]
    }
   ],
   "source": [
    "cat(bucket,\"csv/part\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "31fd06f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      "\n",
      "+---+-----+----------+----+\n",
      "|_c0|  _c1|       _c2| _c3|\n",
      "+---+-----+----------+----+\n",
      "|  2| alex|2019-02-01|1500|\n",
      "|  4|maria|2020-01-01|5000|\n",
      "|  1| otto|2019-10-01|4444|\n",
      "|  1| alex|2019-01-01|1000|\n",
      "|  3| alex|2019-03-01|1700|\n",
      "+---+-----+----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# und lese alles nochmal ein um zu schauen ob die neue Spalte richtig erkannt wurde\n",
    "read_csv=spark.read.format(\"csv\").load(f\"s3://{bucket}/csv\")\n",
    "\n",
    "read_csv.printSchema()\n",
    "read_csv.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1b3038",
   "metadata": {},
   "source": [
    "#### Erkenntnisse CSV\n",
    "* Datenset wird in mehrere Dateien aufgeteilt die der Anzahl der Partitionen ensprechen \n",
    "* kein Schema vorhanden (Typen)\n",
    "* kein Anfügen neuer Spalten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55bcbfa",
   "metadata": {},
   "source": [
    "## JSON\n",
    "\n",
    "### Aufgabe:\n",
    "Wiederhole die gleichen Schritte mit dem JSON Format und schaue wie sich hier Schema und neue Spalten verhalten\n",
    "\n",
    "1. Datenset 1 als json schreiben (.format(\"json\") und Pfad= .save(f\"s3://{bucket}/json\"))\n",
    "2. Dateien und Inhalt anzeigen, vestehen was da passiert ist\n",
    "3. Daten wieder einlese und checken ob es ein Schema und Spaltennamen gibt\n",
    "4. Datenset 3 anfügen (append)\n",
    "5. Daten wieder einlesen und checken was mit der neuen Spalte passiert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e368413b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Partitions: 3\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Partitions:\", df1.rdd.getNumPartitions())\n",
    "\n",
    "# Schreibe Datenset 1 als JSON Datei\n",
    "\n",
    "# HIER EIGENE CODE EINFÜGEN - in den pfad s3://{bucket}/json schreiben"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aecbd80",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> &#8964 Lösung Datenset 1 als JSON Datei schreiben</summary>\n",
    "<p>\n",
    "<code>write_json=(df1\n",
    "           .write\n",
    "           .format(\"json\")\n",
    "           .mode(\"overwrite\") # append\n",
    "           .save(f\"s3://{bucket}/json\")\n",
    "          )</code>\n",
    "</details>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "da9898f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "json/_SUCCESS\n",
      "json/part-00000-00811c1d-25e8-4a3a-b8ef-692095840568-c000.json\n",
      "json/part-00001-00811c1d-25e8-4a3a-b8ef-692095840568-c000.json\n",
      "json/part-00002-00811c1d-25e8-4a3a-b8ef-692095840568-c000.json\n"
     ]
    }
   ],
   "source": [
    "ls(bucket,\"json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3ba7f5c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: json/part-00000-00811c1d-25e8-4a3a-b8ef-692095840568-c000.json\n",
      "----------------------\n",
      "{\"id\":1,\"account\":\"alex\",\"dt_transaction\":\"2019-01-01\",\"balance\":1000}\n",
      "\n",
      "######################\n",
      "File: json/part-00001-00811c1d-25e8-4a3a-b8ef-692095840568-c000.json\n",
      "----------------------\n",
      "{\"id\":2,\"account\":\"alex\",\"dt_transaction\":\"2019-02-01\",\"balance\":1500}\n",
      "{\"id\":4,\"account\":\"maria\",\"dt_transaction\":\"2020-01-01\",\"balance\":5000}\n",
      "\n",
      "######################\n",
      "File: json/part-00002-00811c1d-25e8-4a3a-b8ef-692095840568-c000.json\n",
      "----------------------\n",
      "{\"id\":3,\"account\":\"alex\",\"dt_transaction\":\"2019-03-01\",\"balance\":1700}\n",
      "\n",
      "######################\n"
     ]
    }
   ],
   "source": [
    "cat(bucket,\"json/part\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb7d899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daten wieder einlese und checken ob es ein Schema und Spaltennamen gibt\n",
    "\n",
    "# HIER EIGENE CODE EINFÜGEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7506f71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# schreibe Datenset 3 (neue Spalte) in die gleiche Tabelle dazu (!! append NOT overwrite)\n",
    "\n",
    "# HIER EIGENE CODE EINFÜGEN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64de1474",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> &#8964 Lösung Datenset 3 als JSON Datei anfügen</summary>\n",
    "<p>\n",
    "<code>write_json=(df3\n",
    "           .write\n",
    "           .format(\"json\")\n",
    "           .mode(\"append\") # append\n",
    "           .save(f\"s3://{bucket}/json\")\n",
    "          )</code>\n",
    "</details>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ba940d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alles nochmal einlesen und schauen ob die neue Spalte und die Schemas richtig erkannt wurden\n",
    "\n",
    "read_json=# HIER EIGENE CODE EINFÜGEN\n",
    "\n",
    "read_json.printSchema()\n",
    "read_json.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4f57c6",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> &#8964 Lösung JSON wieder einlesen</summary>\n",
    "<p>\n",
    "<code>read_json=spark.read.format(\"json\").load(f\"s3://{bucket}/json\")\n",
    "\n",
    "read_json.printSchema()\n",
    "read_json.show()\n",
    "    </code>\n",
    "</details>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad580792",
   "metadata": {},
   "source": [
    "#### Erkenntnisse CSV\n",
    "* Werden Spaltennamen erhalten? \n",
    "* Gibt es ein Schema?\n",
    "* Können neue Spalten agefügt und verarbeitet werden?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094e8bb0",
   "metadata": {},
   "source": [
    "## AVRO\n",
    "Avro ist ein Zeilenformat was für das schnelle Schreiben im Streaming Kontext optimiert ist.\n",
    "Avro ist selbsterklärend, hat ein Schema und unterstützt Schema Evolution\n",
    "\n",
    "### Aufgabe:\n",
    "Wiederhole die gleichen Schritte mit dem AVRO Format und schaue wie sich hier Schema und neue Spalten verhalten\n",
    "\n",
    "1. Datenset 1 als avro schreiben (.format(\"avro\") und Pfad= .save(f\"s3://{bucket}/avro\"))\n",
    "2. Dateien und Inhalt anzeigen, vestehen was da passiert ist\n",
    "3. Metadaten in Datei identifizieren\n",
    "3. Daten wieder einlese und checken ob es ein Schema und Spaltennamen gibt\n",
    "4. Schema Evolutiuon: Datenset 3 anfügen mit neuer Spalte anfügen\n",
    "5. Daten wieder einlesen und checken was mit der neuen Spalte passiert\n",
    "6. Schema Enforcement: Datentyp in bestehender Spalte ändern und schauen ob und wie dies gehandhabt wird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "44777375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schreibe Datenset 1 als AVRO Datei\n",
    "\n",
    "# HIER EIGENE CODE EINFÜGEN - in den Pfad s3://{bucket}/avro schreiben\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10290dc0",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> &#8964 Lösung Datenset 1 als AVRO Datei schreiben</summary>\n",
    "<p>\n",
    "<code>write_json=(df1\n",
    "           .write\n",
    "           .format(\"avro\")\n",
    "           .mode(\"overwrite\") # append\n",
    "           .save(f\"s3://{bucket}/avro\")\n",
    "          )</code>\n",
    "</details>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a7ebc8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avro/_SUCCESS\n",
      "avro/part-00000-aa567ac2-db28-465e-bd22-686ea9ba20ac-c000.avro\n",
      "avro/part-00001-aa567ac2-db28-465e-bd22-686ea9ba20ac-c000.avro\n",
      "avro/part-00002-aa567ac2-db28-465e-bd22-686ea9ba20ac-c000.avro\n"
     ]
    }
   ],
   "source": [
    "ls(bucket,\"avro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d82f2e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finde in der Darstellung der Datei die Metadaten und die eigentlichen Daten\n",
    "# Da Avro ein Binärformat ist muss hier in cat die Flag auf True gesetzt werden\n",
    "cat(bucket,\"avro/part\",True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8ac72f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_avro=# HIER EIGENE CODE EINFÜGEN\n",
    "\n",
    "read_avro.printSchema()\n",
    "read_avro.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1b1e0ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# schreibe Datenset 3 (neue Spalte) in die gleiche Tabelle dazu (!! append NOT overwrite)\n",
    "\n",
    "# HIER EIGENE CODE EINFÜGEN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b17731",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> &#8964 Lösung Datenset 3 als AVRO Datei anfügen</summary>\n",
    "<p>\n",
    "<code>write_avro=(df3\n",
    "           .write\n",
    "           .format(\"avro\")\n",
    "           .mode(\"append\") # append\n",
    "           .save(f\"s3://{bucket}/avro\")\n",
    "          )</code>\n",
    "</details>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "70570799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alles nochmal einlesen und schauen ob die neue Spalte und die Schemas richtig erkannt wurden\n",
    "# wiederholt sich langsam gell?\n",
    "\n",
    "read_avro=# HIER EIGENE CODE EINFÜGEN\n",
    "\n",
    "read_avro.printSchema()\n",
    "read_avro.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448ca431",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> &#8964 Lösung JSON wieder einlesen</summary>\n",
    "<p>\n",
    "<code>read_json=spark.read.format(\"json\").load(f\"s3://{bucket}/json\")\n",
    "\n",
    "read_json.printSchema()\n",
    "read_json.show()\n",
    "    </code>\n",
    "</details>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "720ff1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema vorher:\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- account: string (nullable = true)\n",
      " |-- dt_transaction: date (nullable = true)\n",
      " |-- balance: long (nullable = true)\n",
      "\n",
      "Schema nachher:\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- account: string (nullable = true)\n",
      " |-- dt_transaction: date (nullable = true)\n",
      " |-- balance: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Füge eine Zeile (df2) zu der AVRO Tabelle hinzu aber ändere den Datentyp für die id von long zu string\n",
    "print(\"Schema vorher:\")\n",
    "df2.printSchema()\n",
    "\n",
    "\n",
    "df2a=(df2\n",
    "           # nur die Zeile Peter aus df2\n",
    "           .where(f.col(\"account\")==\"peter\")\n",
    "           # ID als string statt als long\n",
    "           .withColumn(\"id\", f.col(\"id\").cast(\"int\"))\n",
    "          )\n",
    "\n",
    "print(\"Schema nachher:\")\n",
    "df2a.printSchema()\n",
    "\n",
    "\n",
    "write_avro=(df2a\n",
    "            .write\n",
    "            .format(\"avro\")\n",
    "            #.option(\"mergeSchema\",\"true\")\n",
    "            .mode(\"append\")\n",
    "            .save(f\"s3://{bucket}/avro\")\n",
    "           )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "1fc2f400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: float (nullable = true)\n",
      " |-- account: string (nullable = true)\n",
      " |-- dt_transaction: date (nullable = true)\n",
      " |-- balance: long (nullable = true)\n",
      "\n",
      "23/06/28 20:59:52 ERROR TaskSetManager: Task 0 in stage 121.0 failed 4 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1573.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 121.0 failed 4 times, most recent failure: Lost task 0.3 in stage 121.0 (TID 221) (10.244.1.7 executor 1): org.apache.spark.sql.avro.IncompatibleSchemaException: Cannot convert Avro type {\"type\":\"record\",\"name\":\"topLevelRecord\",\"fields\":[{\"name\":\"id\",\"type\":[\"long\",\"null\"]},{\"name\":\"account\",\"type\":[\"string\",\"null\"]},{\"name\":\"dt_transaction\",\"type\":[{\"type\":\"int\",\"logicalType\":\"date\"},\"null\"]},{\"name\":\"balance\",\"type\":[\"long\",\"null\"]}]} to SQL type STRUCT<id: FLOAT, account: STRING, dt_transaction: DATE, balance: BIGINT>.\n\tat org.apache.spark.sql.avro.AvroDeserializer.liftedTree1$1(AvroDeserializer.scala:101)\n\tat org.apache.spark.sql.avro.AvroDeserializer.<init>(AvroDeserializer.scala:73)\n\tat org.apache.spark.sql.avro.AvroFileFormat$$anon$1.<init>(AvroFileFormat.scala:143)\n\tat org.apache.spark.sql.avro.AvroFileFormat.$anonfun$buildReader$1(AvroFileFormat.scala:136)\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:154)\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:139)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:209)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:270)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.base/java.lang.Thread.run(Unknown Source)\nCaused by: org.apache.spark.sql.avro.IncompatibleSchemaException: Cannot convert Avro field 'id' to SQL field 'id' because schema is incompatible (avroType = \"long\", sqlType = FLOAT)\n\tat org.apache.spark.sql.avro.AvroDeserializer.newWriter(AvroDeserializer.scala:343)\n\tat org.apache.spark.sql.avro.AvroDeserializer.$anonfun$getRecordWriter$1(AvroDeserializer.scala:374)\n\tat scala.collection.immutable.List.map(List.scala:293)\n\tat org.apache.spark.sql.avro.AvroDeserializer.getRecordWriter(AvroDeserializer.scala:371)\n\tat org.apache.spark.sql.avro.AvroDeserializer.liftedTree1$1(AvroDeserializer.scala:83)\n\t... 26 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2238)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2259)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2278)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:506)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:459)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3868)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2863)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2863)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3084)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:288)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:327)\n\tat jdk.internal.reflect.GeneratedMethodAccessor220.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Unknown Source)\nCaused by: org.apache.spark.sql.avro.IncompatibleSchemaException: Cannot convert Avro type {\"type\":\"record\",\"name\":\"topLevelRecord\",\"fields\":[{\"name\":\"id\",\"type\":[\"long\",\"null\"]},{\"name\":\"account\",\"type\":[\"string\",\"null\"]},{\"name\":\"dt_transaction\",\"type\":[{\"type\":\"int\",\"logicalType\":\"date\"},\"null\"]},{\"name\":\"balance\",\"type\":[\"long\",\"null\"]}]} to SQL type STRUCT<id: FLOAT, account: STRING, dt_transaction: DATE, balance: BIGINT>.\n\tat org.apache.spark.sql.avro.AvroDeserializer.liftedTree1$1(AvroDeserializer.scala:101)\n\tat org.apache.spark.sql.avro.AvroDeserializer.<init>(AvroDeserializer.scala:73)\n\tat org.apache.spark.sql.avro.AvroFileFormat$$anon$1.<init>(AvroFileFormat.scala:143)\n\tat org.apache.spark.sql.avro.AvroFileFormat.$anonfun$buildReader$1(AvroFileFormat.scala:136)\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:154)\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:139)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:209)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:270)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\t... 1 more\nCaused by: org.apache.spark.sql.avro.IncompatibleSchemaException: Cannot convert Avro field 'id' to SQL field 'id' because schema is incompatible (avroType = \"long\", sqlType = FLOAT)\n\tat org.apache.spark.sql.avro.AvroDeserializer.newWriter(AvroDeserializer.scala:343)\n\tat org.apache.spark.sql.avro.AvroDeserializer.$anonfun$getRecordWriter$1(AvroDeserializer.scala:374)\n\tat scala.collection.immutable.List.map(List.scala:293)\n\tat org.apache.spark.sql.avro.AvroDeserializer.getRecordWriter(AvroDeserializer.scala:371)\n\tat org.apache.spark.sql.avro.AvroDeserializer.liftedTree1$1(AvroDeserializer.scala:83)\n\t... 26 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[118], line 9\u001b[0m\n\u001b[1;32m      2\u001b[0m read_avro\u001b[38;5;241m=\u001b[39m(spark\n\u001b[1;32m      3\u001b[0m            \u001b[38;5;241m.\u001b[39mread\n\u001b[1;32m      4\u001b[0m            \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mavro\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m            \u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms3://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbucket\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/avro\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      8\u001b[0m read_avro\u001b[38;5;241m.\u001b[39mprintSchema()\n\u001b[0;32m----> 9\u001b[0m \u001b[43mread_avro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.py:606\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    603\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameter \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a bool\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    605\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 606\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1573.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 121.0 failed 4 times, most recent failure: Lost task 0.3 in stage 121.0 (TID 221) (10.244.1.7 executor 1): org.apache.spark.sql.avro.IncompatibleSchemaException: Cannot convert Avro type {\"type\":\"record\",\"name\":\"topLevelRecord\",\"fields\":[{\"name\":\"id\",\"type\":[\"long\",\"null\"]},{\"name\":\"account\",\"type\":[\"string\",\"null\"]},{\"name\":\"dt_transaction\",\"type\":[{\"type\":\"int\",\"logicalType\":\"date\"},\"null\"]},{\"name\":\"balance\",\"type\":[\"long\",\"null\"]}]} to SQL type STRUCT<id: FLOAT, account: STRING, dt_transaction: DATE, balance: BIGINT>.\n\tat org.apache.spark.sql.avro.AvroDeserializer.liftedTree1$1(AvroDeserializer.scala:101)\n\tat org.apache.spark.sql.avro.AvroDeserializer.<init>(AvroDeserializer.scala:73)\n\tat org.apache.spark.sql.avro.AvroFileFormat$$anon$1.<init>(AvroFileFormat.scala:143)\n\tat org.apache.spark.sql.avro.AvroFileFormat.$anonfun$buildReader$1(AvroFileFormat.scala:136)\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:154)\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:139)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:209)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:270)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.base/java.lang.Thread.run(Unknown Source)\nCaused by: org.apache.spark.sql.avro.IncompatibleSchemaException: Cannot convert Avro field 'id' to SQL field 'id' because schema is incompatible (avroType = \"long\", sqlType = FLOAT)\n\tat org.apache.spark.sql.avro.AvroDeserializer.newWriter(AvroDeserializer.scala:343)\n\tat org.apache.spark.sql.avro.AvroDeserializer.$anonfun$getRecordWriter$1(AvroDeserializer.scala:374)\n\tat scala.collection.immutable.List.map(List.scala:293)\n\tat org.apache.spark.sql.avro.AvroDeserializer.getRecordWriter(AvroDeserializer.scala:371)\n\tat org.apache.spark.sql.avro.AvroDeserializer.liftedTree1$1(AvroDeserializer.scala:83)\n\t... 26 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2238)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2259)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2278)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:506)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:459)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3868)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2863)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2863)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3084)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:288)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:327)\n\tat jdk.internal.reflect.GeneratedMethodAccessor220.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Unknown Source)\nCaused by: org.apache.spark.sql.avro.IncompatibleSchemaException: Cannot convert Avro type {\"type\":\"record\",\"name\":\"topLevelRecord\",\"fields\":[{\"name\":\"id\",\"type\":[\"long\",\"null\"]},{\"name\":\"account\",\"type\":[\"string\",\"null\"]},{\"name\":\"dt_transaction\",\"type\":[{\"type\":\"int\",\"logicalType\":\"date\"},\"null\"]},{\"name\":\"balance\",\"type\":[\"long\",\"null\"]}]} to SQL type STRUCT<id: FLOAT, account: STRING, dt_transaction: DATE, balance: BIGINT>.\n\tat org.apache.spark.sql.avro.AvroDeserializer.liftedTree1$1(AvroDeserializer.scala:101)\n\tat org.apache.spark.sql.avro.AvroDeserializer.<init>(AvroDeserializer.scala:73)\n\tat org.apache.spark.sql.avro.AvroFileFormat$$anon$1.<init>(AvroFileFormat.scala:143)\n\tat org.apache.spark.sql.avro.AvroFileFormat.$anonfun$buildReader$1(AvroFileFormat.scala:136)\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:154)\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:139)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:209)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:270)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\t... 1 more\nCaused by: org.apache.spark.sql.avro.IncompatibleSchemaException: Cannot convert Avro field 'id' to SQL field 'id' because schema is incompatible (avroType = \"long\", sqlType = FLOAT)\n\tat org.apache.spark.sql.avro.AvroDeserializer.newWriter(AvroDeserializer.scala:343)\n\tat org.apache.spark.sql.avro.AvroDeserializer.$anonfun$getRecordWriter$1(AvroDeserializer.scala:374)\n\tat scala.collection.immutable.List.map(List.scala:293)\n\tat org.apache.spark.sql.avro.AvroDeserializer.getRecordWriter(AvroDeserializer.scala:371)\n\tat org.apache.spark.sql.avro.AvroDeserializer.liftedTree1$1(AvroDeserializer.scala:83)\n\t... 26 more\n"
     ]
    }
   ],
   "source": [
    "# probiere das Verzeichnis mit verschiedenen Datentypen einzulesen\n",
    "read_avro=(spark\n",
    "           .read\n",
    "           .format(\"avro\")\n",
    "           .load(f\"s3://{bucket}/avro\"))\n",
    "\n",
    "\n",
    "read_avro.printSchema()\n",
    "read_avro.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b1054d",
   "metadata": {},
   "source": [
    "#### Erkenntnisse AVRO\n",
    "* Werden Spaltennamen erhalten? \n",
    "* Gibt es ein Schema?\n",
    "* Schema Evolution: Kann das Schema erweitert werden, also eine neue Spalte angefügt werden?\n",
    "* Schema Enforcement on write: Kann eine Spalte mit falschem Datetyp einfach beim schreiben hinzugefügt werden? \n",
    "* Schema Enforcement on read: Kann ein Verzeichnis mit mehreren Avro Dateien bei der eine Spalte ein anderes Schema hat gelesen werden?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45de295",
   "metadata": {},
   "source": [
    "## Parquet\n",
    "\n",
    "### Aufgabe:\n",
    "Wiederhole die gleichen Schritte mit dem PARQUET Format und schaue wie sich hier Schema und neue Spalten verhalten\n",
    "\n",
    "1. Datenset 1 als parquet schreiben (.format(\"parquet\") und Pfad= .save(f\"s3://{bucket}/parquet\"))\n",
    "2. Dateien und Inhalt anzeigen, vestehen was da passiert ist\n",
    "3. Metadaten in Datei identifizieren\n",
    "3. Daten wieder einlese und checken ob es ein Schema und Spaltennamen gibt\n",
    "4. Schema Evolutiuon: Datenset 3 anfügen mit neuer Spalte anfügen\n",
    "5. Daten wieder einlesen und checken was mit der neuen Spalte passiert\n",
    "6. Partion & Pushdown Filter: Execution Plan für verschiedene Filter anzeigen\n",
    "6. Schema Enforcement: Datentyp in bestehender Spalte ändern und schauen ob und wie dies gehandhabt wird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "fb546000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Partitions: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"Number of Partitions:\", df1.rdd.getNumPartitions())\n",
    "\n",
    "write_parquet=(df1\n",
    "           .write\n",
    "           # Fachliche Partitionierung beim Schreiben\n",
    "           .partitionBy(\"account\")\n",
    "           .format(\"parquet\")\n",
    "           .mode(\"overwrite\") # append\n",
    "           .save(f\"s3://{bucket}/parquet\")\n",
    "          )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "6c9e08a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ls(bucket,\"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "49b6d692",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat(bucket,\"parquet/account=maria\",True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc84f12a",
   "metadata": {},
   "source": [
    "#### Filter Pushdown "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "e00a1e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition Filter\n",
      "== Physical Plan ==\n",
      "*(1) ColumnarToRow\n",
      "+- FileScan parquet [id#1296L,dt_transaction#1297,balance#1298L,account#1299] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[s3://fileformats/parquet], PartitionFilters: [isnotnull(account#1299), (account#1299 = alex)], PushedFilters: [], ReadSchema: struct<id:bigint,dt_transaction:date,balance:bigint>\n",
      "\n",
      "\n",
      "Pushdow Filter\n",
      "== Physical Plan ==\n",
      "*(1) Filter (isnotnull(balance#1307L) AND (balance#1307L > 1500))\n",
      "+- *(1) ColumnarToRow\n",
      "   +- FileScan parquet [id#1305L,dt_transaction#1306,balance#1307L,account#1308] Batched: true, DataFilters: [isnotnull(balance#1307L), (balance#1307L > 1500)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[s3://fileformats/parquet], PartitionFilters: [], PushedFilters: [IsNotNull(balance), GreaterThan(balance,1500)], ReadSchema: struct<id:bigint,dt_transaction:date,balance:bigint>\n",
      "\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- dt_transaction: date (nullable = true)\n",
      " |-- balance: long (nullable = true)\n",
      " |-- account: string (nullable = true)\n",
      "\n",
      "+---+--------------+-------+-------+\n",
      "| id|dt_transaction|balance|account|\n",
      "+---+--------------+-------+-------+\n",
      "|  1|    2019-01-01|   1000|   alex|\n",
      "|  2|    2019-02-01|   1500|   alex|\n",
      "|  3|    2019-03-01|   1700|   alex|\n",
      "+---+--------------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parquet Datei mit PartitionFilter laden\n",
    "read_parquet=(spark\n",
    "              .read.format(\"parquet\")\n",
    "              .load(f\"s3://{bucket}/parquet\")\n",
    "              # Filter auf die Spalte über die partitioniert wurde\n",
    "              .filter(col(\"account\")==\"alex\")\n",
    "             )\n",
    "\n",
    "# Parquet mit normalem Filter laden\n",
    "read_parquet2=(spark\n",
    "              .read.format(\"parquet\")\n",
    "              .load(f\"s3://{bucket}/parquet\")\n",
    "              # Filter auf die Spalte über eine normale Spalte\n",
    "              .filter(col(\"balance\")>1500)\n",
    "             )\n",
    "\n",
    "# Anzeigen des physischen Execution Plans um zu sehen welche Filter ins Dateisystem bzw. in die Parquet Datei gepusht werden\n",
    "print(\"Partition Filter\")\n",
    "read_parquet.explain(\"simple\")\n",
    "print(\"Pushdow Filter\")\n",
    "read_parquet2.explain(\"simple\")\n",
    "\n",
    "\n",
    "read_parquet.printSchema()\n",
    "read_parquet.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37f5387",
   "metadata": {},
   "source": [
    "#### Schema Evolution (Spalte anfügen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "94347c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Zeile mit neuer Spalte anfügen\n",
    "write_parquet=(df3\n",
    "           .write\n",
    "           .format(\"parquet\")\n",
    "           .mode(\"append\") # append\n",
    "           # schreibe ohne zu Partitionieren direkt in ein neues Unterverzeichnis\n",
    "           .save(f\"s3://{bucket}/parquet/account=otto\")\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "562a110e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- dt_transaction: date (nullable = true)\n",
      " |-- balance: long (nullable = true)\n",
      " |-- account: string (nullable = true)\n",
      "\n",
      "+---+--------------+-------+-------+\n",
      "| id|dt_transaction|balance|account|\n",
      "+---+--------------+-------+-------+\n",
      "|  1|    2019-10-01|   4444|   otto|\n",
      "|  1|    2019-01-01|   1000|   alex|\n",
      "|  2|    2019-02-01|   1500|   alex|\n",
      "|  3|    2019-03-01|   1700|   alex|\n",
      "|  4|    2020-01-01|   5000|  maria|\n",
      "+---+--------------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# einlesen mit der mergeSchema Option\n",
    "read_parquet=(spark\n",
    "              .read.format(\"parquet\")\n",
    "              # setzte die mergeSchema auf true/false um den Unterschied beim Einlesen zu sehen\n",
    "              # Vegleiche: https://spark.apache.org/docs/latest/sql-data-sources-parquet.html#schema-merging\n",
    "              .option(\"mergeSchema\", \"false\")\n",
    "              .load(f\"s3://{bucket}/parquet\")\n",
    "             )\n",
    "\n",
    "read_parquet.printSchema()\n",
    "read_parquet.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dc969b",
   "metadata": {},
   "source": [
    "#### Schema Enforcement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "20d1aa9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Datensatz mit falschem Datentyp anfügen\n",
    "df2a=(df2.where(f.col(\"account\")==\"peter\").withColumn(\"id\", f.col(\"id\").cast(\"string\")))\n",
    "\n",
    "\n",
    "# Zeile mit falschem Typ anfügen\n",
    "write_parquet=(df2a\n",
    "           .write\n",
    "           .partitionBy(\"account\")\n",
    "           .format(\"parquet\")\n",
    "           .option(\"mergeSchema\", \"true\")\n",
    "           .mode(\"append\") # append\n",
    "           .save(f\"s3://{bucket}/parquet\")\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "e852d9fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- dt_transaction: date (nullable = true)\n",
      " |-- balance: long (nullable = true)\n",
      " |-- account: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 185:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/28 21:25:43 ERROR TaskSetManager: Task 0 in stage 185.0 failed 4 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1744.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 185.0 failed 4 times, most recent failure: Lost task 0.3 in stage 185.0 (TID 320) (10.244.1.7 executor 1): org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file s3://fileformats/parquet/account=peter/part-00001-6326d72d-c04c-42cd-88b1-b6d1c11b3c40.c000.snappy.parquet. Column: [id], Expected: bigint, Found: BINARY\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:724)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:278)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:561)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.base/java.lang.Thread.run(Unknown Source)\nCaused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1125)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:187)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:316)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:212)\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:274)\n\t... 20 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2238)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2259)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2278)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:506)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:459)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3868)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2863)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2863)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3084)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:288)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:327)\n\tat jdk.internal.reflect.GeneratedMethodAccessor220.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Unknown Source)\nCaused by: org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file s3://fileformats/parquet/account=peter/part-00001-6326d72d-c04c-42cd-88b1-b6d1c11b3c40.c000.snappy.parquet. Column: [id], Expected: bigint, Found: BINARY\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:724)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:278)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:561)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\t... 1 more\nCaused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1125)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:187)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:316)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:212)\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:274)\n\t... 20 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[146], line 10\u001b[0m\n\u001b[1;32m      2\u001b[0m read_parquet\u001b[38;5;241m=\u001b[39m(spark\n\u001b[1;32m      3\u001b[0m               \u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m               \u001b[38;5;66;03m# setzte die mergeSchema auf true/false um den Unterschied beim Einlesen zu sehen\u001b[39;00m\n\u001b[1;32m      5\u001b[0m               \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmergeSchema\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfalse\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m               \u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms3://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbucket\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m              )\n\u001b[1;32m      9\u001b[0m read_parquet\u001b[38;5;241m.\u001b[39mprintSchema()\n\u001b[0;32m---> 10\u001b[0m \u001b[43mread_parquet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.py:606\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    603\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameter \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a bool\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    605\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 606\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1744.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 185.0 failed 4 times, most recent failure: Lost task 0.3 in stage 185.0 (TID 320) (10.244.1.7 executor 1): org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file s3://fileformats/parquet/account=peter/part-00001-6326d72d-c04c-42cd-88b1-b6d1c11b3c40.c000.snappy.parquet. Column: [id], Expected: bigint, Found: BINARY\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:724)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:278)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:561)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.base/java.lang.Thread.run(Unknown Source)\nCaused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1125)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:187)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:316)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:212)\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:274)\n\t... 20 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2238)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2259)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2278)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:506)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:459)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3868)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2863)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2863)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3084)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:288)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:327)\n\tat jdk.internal.reflect.GeneratedMethodAccessor220.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Unknown Source)\nCaused by: org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file s3://fileformats/parquet/account=peter/part-00001-6326d72d-c04c-42cd-88b1-b6d1c11b3c40.c000.snappy.parquet. Column: [id], Expected: bigint, Found: BINARY\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:724)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:278)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:561)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\t... 1 more\nCaused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1125)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:187)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:316)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:212)\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:274)\n\t... 20 more\n"
     ]
    }
   ],
   "source": [
    "# einlesen mit der mergeSchema Option\n",
    "read_parquet=(spark\n",
    "              .read.format(\"parquet\")\n",
    "              # setzte die mergeSchema auf true/false um den Unterschied beim Einlesen zu sehen\n",
    "              .option(\"mergeSchema\", \"false\")\n",
    "              .load(f\"s3://{bucket}/parquet\")\n",
    "             )\n",
    "\n",
    "read_parquet.printSchema()\n",
    "read_parquet.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfe1fae",
   "metadata": {},
   "source": [
    "#### Erkenntnisse Parquet\n",
    "* Sind Parquet Dateien selbsterklärend (haben ein Spalten und Typenschema )\n",
    "* Partitioning and Partion Discovery: werden die Daten in Verzeichnisse geschriebe und wieder als Partitionen erkannt?\n",
    "* Schema Evolution: Kann das Schema erweitert werden, also eine neue Spalte angefügt werden?\n",
    "* Schema Enforcement on write: Kann eine Spalte mit falschem Datetyp einfach beim schreiben hinzugefügt werden? \n",
    "* Schema Enforcement on read: Kann ein Verzeichnis mit mehreren Parquet Dateien bei der eine Spalte ein anderes Schema hat gelesen werden?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eadd53c",
   "metadata": {},
   "source": [
    "## Delta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7d3e7e",
   "metadata": {},
   "source": [
    "- a **storage layer** that runs on top of existing data lakes\n",
    "- supports ACID transactions and data versioning\n",
    "- allows data lineage tracking\n",
    "- provides optimization for streaming workloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "513d213c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "write_delta=(df1\n",
    "           .write\n",
    "           .format(\"delta\")\n",
    "           #.option(\"mergeSchema\", \"false\")\n",
    "           .mode(\"overwrite\") \n",
    "           .save(f\"s3://{bucket}/delta\")\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "70cd6f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta/_delta_log/00000000000000000000.json\n",
      "delta/part-00000-71e8256a-4ae9-4a30-82df-1df4300074fa-c000.snappy.parquet\n",
      "delta/part-00001-740db38d-df85-4d5f-b675-785b8e5a1ff6-c000.snappy.parquet\n",
      "delta/part-00002-751ee4f7-7ea7-4731-9df7-9a881d3a97ac-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "ls(bucket,\"delta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2da732ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta/_delta_log/00000000000000000000.json\n"
     ]
    }
   ],
   "source": [
    "ls(bucket,\"delta/_delta_log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0c3dbb33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: delta/_delta_log/00000000000000000000.json\n",
      "----------------------\n",
      "{\"protocol\":{\"minReaderVersion\":1,\"minWriterVersion\":2}}\n",
      "{\"metaData\":{\"id\":\"cc178cae-6af7-410b-9996-f81ccdeb1ab5\",\"format\":{\"provider\":\"parquet\",\"options\":{}},\"schemaString\":\"{\\\"type\\\":\\\"struct\\\",\\\"fields\\\":[{\\\"name\\\":\\\"id\\\",\\\"type\\\":\\\"long\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"account\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"dt_transaction\\\",\\\"type\\\":\\\"date\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"balance\\\",\\\"type\\\":\\\"long\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}}]}\",\"partitionColumns\":[],\"configuration\":{},\"createdTime\":1684095467225}}\n",
      "{\"add\":{\"path\":\"part-00000-71e8256a-4ae9-4a30-82df-1df4300074fa-c000.snappy.parquet\",\"partitionValues\":{},\"size\":1236,\"modificationTime\":1684095469000,\"dataChange\":true,\"stats\":\"{\\\"numRecords\\\":1,\\\"minValues\\\":{\\\"id\\\":1,\\\"account\\\":\\\"alex\\\",\\\"dt_transaction\\\":\\\"2019-01-01\\\",\\\"balance\\\":1000},\\\"maxValues\\\":{\\\"id\\\":1,\\\"account\\\":\\\"alex\\\",\\\"dt_transaction\\\":\\\"2019-01-01\\\",\\\"balance\\\":1000},\\\"nullCount\\\":{\\\"id\\\":0,\\\"account\\\":0,\\\"dt_transaction\\\":0,\\\"balance\\\":0}}\"}}\n",
      "{\"add\":{\"path\":\"part-00001-740db38d-df85-4d5f-b675-785b8e5a1ff6-c000.snappy.parquet\",\"partitionValues\":{},\"size\":1254,\"modificationTime\":1684095469000,\"dataChange\":true,\"stats\":\"{\\\"numRecords\\\":2,\\\"minValues\\\":{\\\"id\\\":2,\\\"account\\\":\\\"alex\\\",\\\"dt_transaction\\\":\\\"2019-02-01\\\",\\\"balance\\\":1500},\\\"maxValues\\\":{\\\"id\\\":4,\\\"account\\\":\\\"maria\\\",\\\"dt_transaction\\\":\\\"2020-01-01\\\",\\\"balance\\\":5000},\\\"nullCount\\\":{\\\"id\\\":0,\\\"account\\\":0,\\\"dt_transaction\\\":0,\\\"balance\\\":0}}\"}}\n",
      "{\"add\":{\"path\":\"part-00002-751ee4f7-7ea7-4731-9df7-9a881d3a97ac-c000.snappy.parquet\",\"partitionValues\":{},\"size\":1236,\"modificationTime\":1684095473000,\"dataChange\":true,\"stats\":\"{\\\"numRecords\\\":1,\\\"minValues\\\":{\\\"id\\\":3,\\\"account\\\":\\\"alex\\\",\\\"dt_transaction\\\":\\\"2019-03-01\\\",\\\"balance\\\":1700},\\\"maxValues\\\":{\\\"id\\\":3,\\\"account\\\":\\\"alex\\\",\\\"dt_transaction\\\":\\\"2019-03-01\\\",\\\"balance\\\":1700},\\\"nullCount\\\":{\\\"id\\\":0,\\\"account\\\":0,\\\"dt_transaction\\\":0,\\\"balance\\\":0}}\"}}\n",
      "{\"commitInfo\":{\"timestamp\":1684095478099,\"operation\":\"WRITE\",\"operationParameters\":{\"mode\":\"Overwrite\",\"partitionBy\":\"[]\"},\"isolationLevel\":\"Serializable\",\"isBlindAppend\":false,\"operationMetrics\":{\"numFiles\":\"3\",\"numOutputRows\":\"4\",\"numOutputBytes\":\"3726\"},\"engineInfo\":\"Apache-Spark/3.3.1 Delta-Lake/2.1.1\",\"txnId\":\"e821496b-88e7-46d6-a713-b91a791957ed\"}}\n",
      "\n",
      "######################\n"
     ]
    }
   ],
   "source": [
    "cat(bucket,\"delta/_delta_log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "a714c074",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------+-------+\n",
      "| id|account|dt_transaction|balance|\n",
      "+---+-------+--------------+-------+\n",
      "|  2|   alex|    2019-02-01|   1500|\n",
      "|  4|  maria|    2020-01-01|   5000|\n",
      "|  3|   alex|    2019-03-01|   1700|\n",
      "|  1|   alex|    2019-01-01|   1000|\n",
      "+---+-------+--------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "read_delta=spark.read.format(\"delta\").load(f\"s3://{bucket}/delta\")\n",
    "read_delta.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce95dd2",
   "metadata": {},
   "source": [
    "#### Schema Evolution must be explicitely set \n",
    "Aufgabe: spiele mit der Optiob mergeSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "e8d72dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "write_delta=(df3\n",
    "           .write\n",
    "           .format(\"delta\")\n",
    "           # Bei Delta kann bein Schreiben gesetzt werden ob die Tabelle erweitert werden kann oder nicht, Default ist false \n",
    "           .option(\"mergeSchema\", \"true\")\n",
    "           .mode(\"append\") # append\n",
    "           .save(f\"s3://{bucket}/delta\")\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "a5bf92d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat(bucket,\"delta/_delta_log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "85c970d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------+-------+-------------+\n",
      "| id|account|dt_transaction|balance|          new|\n",
      "+---+-------+--------------+-------+-------------+\n",
      "|  1|   otto|    2019-10-01|   4444|neue Spalte 1|\n",
      "|  2|   alex|    2019-02-01|   1500|         null|\n",
      "|  4|  maria|    2020-01-01|   5000|         null|\n",
      "|  3|   alex|    2019-03-01|   1700|         null|\n",
      "|  1|   alex|    2019-01-01|   1000|         null|\n",
      "+---+-------+--------------+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "read_delta=spark.read.format(\"delta\").load(f\"s3://{bucket}/delta\")\n",
    "read_delta.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af13efe",
   "metadata": {},
   "source": [
    "#### Schema Enforcement\n",
    "Aufgabe: spiele mit der Optiob mergeSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "102f2390",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Failed to merge fields 'id' and 'id'. Failed to merge incompatible data types LongType and StringType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[178], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m write_delta\u001b[38;5;241m=\u001b[39m(\u001b[43mdf2\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m           \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maccount\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpeter\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m           \u001b[49m\u001b[38;5;66;43;03m# Spaltentyp ändern\u001b[39;49;00m\n\u001b[1;32m      4\u001b[0m \u001b[43m           \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstring\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m           \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m           \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdelta\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m           \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmergeSchema\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfalse\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m           \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# append\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m           \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ms3://\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mbucket\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/delta\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m           )\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/readwriter.py:968\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave()\n\u001b[1;32m    967\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 968\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Failed to merge fields 'id' and 'id'. Failed to merge incompatible data types LongType and StringType"
     ]
    }
   ],
   "source": [
    "write_delta=(df2\n",
    "           .where(f.col(\"account\")==\"peter\")\n",
    "           # Spaltentyp ändern\n",
    "           .withColumn(\"id\", f.col(\"id\").cast(\"string\"))\n",
    "           .write\n",
    "           .format(\"delta\")\n",
    "           # Bei Delta kann bein Schreiben gesetzt werden ob die Tabelle erweitert werden kann oder nicht, Default ist false\n",
    "           .option(\"mergeSchema\", \"false\")\n",
    "           .mode(\"overwrite\") # append\n",
    "           .save(f\"s3://{bucket}/delta\")\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3fc7c4",
   "metadata": {},
   "source": [
    "#### Historie und Audit Track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "7cb3480c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- version: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      " |-- userName: string (nullable = true)\n",
      " |-- operation: string (nullable = true)\n",
      " |-- operationParameters: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      " |-- job: struct (nullable = true)\n",
      " |    |-- jobId: string (nullable = true)\n",
      " |    |-- jobName: string (nullable = true)\n",
      " |    |-- runId: string (nullable = true)\n",
      " |    |-- jobOwnerId: string (nullable = true)\n",
      " |    |-- triggerType: string (nullable = true)\n",
      " |-- notebook: struct (nullable = true)\n",
      " |    |-- notebookId: string (nullable = true)\n",
      " |-- clusterId: string (nullable = true)\n",
      " |-- readVersion: long (nullable = true)\n",
      " |-- isolationLevel: string (nullable = true)\n",
      " |-- isBlindAppend: boolean (nullable = true)\n",
      " |-- operationMetrics: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      " |-- userMetadata: string (nullable = true)\n",
      " |-- engineInfo: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deltaTable = DeltaTable.forPath(spark, f\"s3://{bucket}/delta\")\n",
    "# --> Vermutlich falsche Delta Version zu Spark\n",
    "fullHistoryDF = deltaTable.history()    # get the full history of the table\n",
    "\n",
    "fullHistoryDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "ac186df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-------------------+------+---------+--------------------+--------------------+------------+\n",
      "|version|readVersion|          timestamp|userId|operation| operationParameters|    operationMetrics|userMetadata|\n",
      "+-------+-----------+-------------------+------+---------+--------------------+--------------------+------------+\n",
      "|      2|          1|2023-06-28 23:36:05|  null|    WRITE|{mode -> Overwrit...|{numFiles -> 2, n...|        null|\n",
      "|      1|          0|2023-06-28 23:34:35|  null|    WRITE|{mode -> Append, ...|{numFiles -> 2, n...|        null|\n",
      "|      0|       null|2023-06-28 23:34:24|  null|    WRITE|{mode -> Overwrit...|{numFiles -> 3, n...|        null|\n",
      "+-------+-----------+-------------------+------+---------+--------------------+--------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Zeige wichtige Felder aus der Historie an\n",
    "# Bonus: löse die genested Spalten in eigene Spalten auf\n",
    "fullHistoryDF.select(\"version\",\"readVersion\",\"timestamp\",\"userId\",\"operation\",\"operationParameters\",\"operationMetrics\",\"userMetadata\").show(truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "49f89cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------+-------+-------------+\n",
      "| id|account|dt_transaction|balance|          new|\n",
      "+---+-------+--------------+-------+-------------+\n",
      "|  1|   otto|    2019-10-01|   4444|neue Spalte 1|\n",
      "+---+-------+--------------+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"delta\").load(f\"s3://{bucket}/delta\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc61efc",
   "metadata": {},
   "source": [
    "## Delta: Time travel\n",
    "Aufgabe: lese verschiedenen Versionen nach VersionAsOf und Datum ein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "6617afc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 350:================================================>      (44 + 2) / 50]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------+-------+-------------+\n",
      "| id|account|dt_transaction|balance|          new|\n",
      "+---+-------+--------------+-------+-------------+\n",
      "|  1|   otto|    2019-10-01|   4444|neue Spalte 1|\n",
      "+---+-------+--------------+-------+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"delta\").option(\"versionAsOf\", \"1\").load(f\"s3://{bucket}/delta\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55d249d",
   "metadata": {},
   "source": [
    "## Delta: Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "2aea3fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------+-------+----+\n",
      "| id|account|dt_transaction|balance| new|\n",
      "+---+-------+--------------+-------+----+\n",
      "|  1|   alex|    2019-03-01|   3300|test|\n",
      "|  2|  peter|    2021-01-01|    100|test|\n",
      "+---+-------+--------------+-------+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------+-------+-------------+\n",
      "| id|account|dt_transaction|balance|          new|\n",
      "+---+-------+--------------+-------+-------------+\n",
      "|  1|   otto|    2019-10-01|   4444|neue Spalte 1|\n",
      "|  2|   alex|    2019-02-01|   1500|         null|\n",
      "|  4|  maria|    2020-01-01|   5000|         null|\n",
      "|  3|   alex|    2019-03-01|   1700|         null|\n",
      "|  1|   alex|    2019-01-01|   1000|         null|\n",
      "+---+-------+--------------+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deltaTable2 = DeltaTable.forPath(spark, f\"s3://{bucket}/delta\")\n",
    "\n",
    "\n",
    "df2a=df2.withColumn(\"new\",f.lit(\"test\"))\n",
    "df2a.show()\n",
    "deltaTable2.toDF().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "e1688108",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------+-------+-------------+\n",
      "| id|account|dt_transaction|balance|          new|\n",
      "+---+-------+--------------+-------+-------------+\n",
      "|  1|   otto|    2019-10-01|   4444|neue Spalte 1|\n",
      "|  1|   alex|    2019-03-01|   3300|         test|\n",
      "|  2|  peter|    2021-01-01|    100|         test|\n",
      "|  2|   alex|    2019-02-01|   1500|         null|\n",
      "|  4|  maria|    2020-01-01|   5000|         null|\n",
      "|  1|   alex|    2019-01-01|   1000|         null|\n",
      "+---+-------+--------------+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt3=(deltaTable2.alias(\"oldData\")\n",
    "      .merge(df2a.alias(\"newData\"),\n",
    "            \"oldData.account = newData.account AND oldData.dt_transaction = newData.dt_transaction\")\n",
    "            .whenMatchedUpdateAll()\n",
    "            .whenNotMatchedInsertAll()\n",
    "      .execute()\n",
    "    )\n",
    "\n",
    "deltaTable2.toDF().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe344799",
   "metadata": {},
   "source": [
    "#### Erkenntnisse Delta\n",
    "* Wie funktioniert das Metadatenmanagement und was steht im Delta Log?\n",
    "* Schema Evolution: Kann das Schema erweitert werden, also eine neue Spalte angefügt werden?\n",
    "* Schema Enforcement on write: Kann eine Spalte mit falschem Datetyp einfach beim schreiben hinzugefügt werden? \n",
    "* Schema Enforcement on read: Kann ein Verzeichnis mit mehreren Parquet Dateien bei der eine Spalte ein anderes Schema hat gelesen werden? (um die Ecke Denk Frage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b85012",
   "metadata": {},
   "source": [
    "## Iceberg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fafcace",
   "metadata": {},
   "source": [
    "- a **table format**\n",
    "- supports schema evolution and provides a portable table metadata format\n",
    "- best suited for analytical workloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2cb24f4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Catalog: default\n",
      "List Catalogs: [Database(name='default', description='Default Hive database', locationUri='file:/home/hive/warehouse')]\n",
      "List Tables in current Catalog: []\n"
     ]
    }
   ],
   "source": [
    "print(\"Current Catalog:\",spark.catalog.currentDatabase())\n",
    "print(\"List Catalogs:\",spark.catalog.listDatabases())\n",
    "print(\"List Tables in current Catalog:\",spark.catalog.listTables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7377b8ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a Database(name=<db_name>, locationUri='s3a://<bucket>/')\n",
    "spark.sql(f\"CREATE DATABASE iceberg_db LOCATION 's3a://{bucket}/'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "59b8232c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "| namespace|\n",
      "+----------+\n",
      "|   default|\n",
      "|iceberg_db|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### show databases and tables in iceberg catalog (only sees iceberg formated tables)\n",
    "# all databases from hive are shown\n",
    "spark.sql(\"SHOW databases from ice\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5010f061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables from iceberg_db\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cba4b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Delete Iceberg tables: first drop the table \n",
    "#spark.sql(\"drop table iceberg_db.iceberg_table\")\n",
    "#delete_objects(\"aleks-test\", \"iceberg_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "eafa7b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "write_iceberg=(df1\n",
    "                  .write\n",
    "                  .format(\"iceberg\")\n",
    "                  .mode(\"overwrite\")\n",
    "                  .saveAsTable(\"iceberg_db.iceberg\")\n",
    "               )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "494fb421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iceberg/data/00000-545-3b1bf31a-0ecb-4245-8a30-5d6b4de18962-00001.parquet\n",
      "iceberg/data/00000-550-7fa8b7a8-5984-487f-b524-9753864c2808-00001.parquet\n",
      "iceberg/data/00001-546-96c2733b-2dc9-4e04-b06d-1c2cbf6fe159-00001.parquet\n",
      "iceberg/data/00001-551-6cfea253-4caa-4fb6-bc7e-e3d9f956388b-00001.parquet\n",
      "iceberg/data/00002-547-3a8580fe-e207-4cf4-8015-2e6f91d99deb-00001.parquet\n",
      "iceberg/metadata/00000-2346cea3-3db3-46a4-bb55-419ae993156b.metadata.json\n",
      "iceberg/metadata/00001-70cc0e6d-feed-4202-bf2a-587f8dd81bbe.metadata.json\n",
      "iceberg/metadata/5dc0a4c4-3c72-4d16-a200-c299b635415c-m0.avro\n",
      "iceberg/metadata/73a18816-64a1-4119-8453-bb671b8abfb3-m0.avro\n",
      "iceberg/metadata/snap-2282180466624073266-1-73a18816-64a1-4119-8453-bb671b8abfb3.avro\n",
      "iceberg/metadata/snap-4263160168885610306-1-5dc0a4c4-3c72-4d16-a200-c299b635415c.avro\n"
     ]
    }
   ],
   "source": [
    "ls(bucket,\"iceberg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "92013c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "write_iceberg=(df2\n",
    "                   .write\n",
    "                   .format(\"iceberg\")\n",
    "                   .mode(\"append\") # append\n",
    "                   .saveAsTable(\"iceberg_db.iceberg\")\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fe763c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iceberg/data/00000-545-3b1bf31a-0ecb-4245-8a30-5d6b4de18962-00001.parquet\n",
      "iceberg/data/00000-550-7fa8b7a8-5984-487f-b524-9753864c2808-00001.parquet\n",
      "iceberg/data/00001-546-96c2733b-2dc9-4e04-b06d-1c2cbf6fe159-00001.parquet\n",
      "iceberg/data/00001-551-6cfea253-4caa-4fb6-bc7e-e3d9f956388b-00001.parquet\n",
      "iceberg/data/00002-547-3a8580fe-e207-4cf4-8015-2e6f91d99deb-00001.parquet\n",
      "iceberg/metadata/00000-2346cea3-3db3-46a4-bb55-419ae993156b.metadata.json\n",
      "iceberg/metadata/00001-70cc0e6d-feed-4202-bf2a-587f8dd81bbe.metadata.json\n",
      "iceberg/metadata/5dc0a4c4-3c72-4d16-a200-c299b635415c-m0.avro\n",
      "iceberg/metadata/73a18816-64a1-4119-8453-bb671b8abfb3-m0.avro\n",
      "iceberg/metadata/snap-2282180466624073266-1-73a18816-64a1-4119-8453-bb671b8abfb3.avro\n",
      "iceberg/metadata/snap-4263160168885610306-1-5dc0a4c4-3c72-4d16-a200-c299b635415c.avro\n"
     ]
    }
   ],
   "source": [
    "ls(bucket,\"iceberg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "964c7ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: iceberg/metadata/00000-2346cea3-3db3-46a4-bb55-419ae993156b.metadata.json\n",
      "----------------------\n",
      "{\n",
      "  \"format-version\" : 1,\n",
      "  \"table-uuid\" : \"c1a84965-5daa-434b-8f3a-3f27cd91ac55\",\n",
      "  \"location\" : \"s3a://fileformats//iceberg\",\n",
      "  \"last-updated-ms\" : 1684152769085,\n",
      "  \"last-column-id\" : 4,\n",
      "  \"schema\" : {\n",
      "    \"type\" : \"struct\",\n",
      "    \"schema-id\" : 0,\n",
      "    \"fields\" : [ {\n",
      "      \"id\" : 1,\n",
      "      \"name\" : \"id\",\n",
      "      \"required\" : false,\n",
      "      \"type\" : \"long\"\n",
      "    }, {\n",
      "      \"id\" : 2,\n",
      "      \"name\" : \"account\",\n",
      "      \"required\" : false,\n",
      "      \"type\" : \"string\"\n",
      "    }, {\n",
      "      \"id\" : 3,\n",
      "      \"name\" : \"dt_transaction\",\n",
      "      \"required\" : false,\n",
      "      \"type\" : \"date\"\n",
      "    }, {\n",
      "      \"id\" : 4,\n",
      "      \"name\" : \"balance\",\n",
      "      \"required\" : false,\n",
      "      \"type\" : \"long\"\n",
      "    } ]\n",
      "  },\n",
      "  \"current-schema-id\" : 0,\n",
      "  \"schemas\" : [ {\n",
      "    \"type\" : \"struct\",\n",
      "    \"schema-id\" : 0,\n",
      "    \"fields\" : [ {\n",
      "      \"id\" : 1,\n",
      "      \"name\" : \"id\",\n",
      "      \"required\" : false,\n",
      "      \"type\" : \"long\"\n",
      "    }, {\n",
      "      \"id\" : 2,\n",
      "      \"name\" : \"account\",\n",
      "      \"required\" : false,\n",
      "      \"type\" : \"string\"\n",
      "    }, {\n",
      "      \"id\" : 3,\n",
      "      \"name\" : \"dt_transaction\",\n",
      "      \"required\" : false,\n",
      "      \"type\" : \"date\"\n",
      "    }, {\n",
      "      \"id\" : 4,\n",
      "      \"name\" : \"balance\",\n",
      "      \"required\" : false,\n",
      "      \"type\" : \"long\"\n",
      "    } ]\n",
      "  } ],\n",
      "  \"partition-spec\" : [ ],\n",
      "  \"default-spec-id\" : 0,\n",
      "  \"partition-specs\" : [ {\n",
      "    \"spec-id\" : 0,\n",
      "    \"fields\" : [ ]\n",
      "  } ],\n",
      "  \"last-partition-id\" : 999,\n",
      "  \"default-sort-order-id\" : 0,\n",
      "  \"sort-orders\" : [ {\n",
      "    \"order-id\" : 0,\n",
      "    \"fields\" : [ ]\n",
      "  } ],\n",
      "  \"properties\" : {\n",
      "    \"owner\" : \"root\"\n",
      "  },\n",
      "  \"current-snapshot-id\" : 2282180466624073266,\n",
      "  \"refs\" : {\n",
      "    \"main\" : {\n",
      "      \"snapshot-id\" : 2282180466624073266,\n",
      "      \"type\" : \"branch\"\n",
      "    }\n",
      "  },\n",
      "  \"snapshots\" : [ {\n",
      "    \"snapshot-id\" : 2282180466624073266,\n",
      "    \"timestamp-ms\" : 1684152769085,\n",
      "    \"summary\" : {\n",
      "      \"operation\" : \"append\",\n",
      "      \"spark.app.id\" : \"spark-671357a054d742b2babacac61c4c8506\",\n",
      "      \"added-data-files\" : \"3\",\n",
      "      \"added-records\" : \"4\",\n",
      "      \"added-files-size\" : \"3689\",\n",
      "      \"changed-partition-count\" : \"1\",\n",
      "      \"total-records\" : \"4\",\n",
      "      \"total-files-size\" : \"3689\",\n",
      "      \"total-data-files\" : \"3\",\n",
      "      \"total-delete-files\" : \"0\",\n",
      "      \"total-position-deletes\" : \"0\",\n",
      "      \"total-equality-deletes\" : \"0\"\n",
      "    },\n",
      "    \"manifest-list\" : \"s3a://fileformats/iceberg/metadata/snap-2282180466624073266-1-73a18816-64a1-4119-8453-bb671b8abfb3.avro\",\n",
      "    \"schema-id\" : 0\n",
      "  } ],\n",
      "  \"statistics\" : [ ],\n",
      "  \"snapshot-log\" : [ {\n",
      "    \"timestamp-ms\" : 1684152769085,\n",
      "    \"snapshot-id\" : 2282180466624073266\n",
      "  } ],\n",
      "  \"metadata-log\" : [ ]\n",
      "}\n",
      "######################\n"
     ]
    }
   ],
   "source": [
    "cat(bucket,\"iceberg/metadata/00000-2346cea3-3db3-46a4-bb55-419ae993156b.metadata.json\",False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a10efcf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ALTER TABLE myTable ADD COLUMNS (address VARCHAR) - the number of columns in the df3 does not match the schema of the table, so we modify the schema of the existing table\n",
    "spark.sql(\"ALTER TABLE iceberg_db.iceberg ADD COLUMNS (new VARCHAR(50))\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a31c068c",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_iceberg=(df3\n",
    "                  .write\n",
    "                  .format(\"iceberg\")\n",
    "                  .mode(\"append\") # append\n",
    "                  .option(\"schema\", schema3)\n",
    "                  .saveAsTable(\"iceberg_db.iceberg\")\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2ff8d8a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- account: string (nullable = true)\n",
      " |-- dt_transaction: date (nullable = true)\n",
      " |-- balance: long (nullable = true)\n",
      " |-- new: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Read Iceberg table:\n",
    "\n",
    "iceberg_df = spark.read.table(\"iceberg_db.iceberg\")\n",
    "iceberg_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "53b286f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "|     made_current_at|        snapshot_id|          parent_id|is_current_ancestor|\n",
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "|2023-05-15 14:12:...|2282180466624073266|               null|               true|\n",
      "|2023-05-15 14:13:...|4263160168885610306|2282180466624073266|               true|\n",
      "|2023-05-15 14:16:...|3806458280092384569|4263160168885610306|               true|\n",
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-----------+-------+------------+------------------+--------------------+--------------------+--------------------+----------------+--------------------+--------------------+------------+-------------+------------+-------------+\n",
      "|content|           file_path|file_format|spec_id|record_count|file_size_in_bytes|        column_sizes|        value_counts|   null_value_counts|nan_value_counts|        lower_bounds|        upper_bounds|key_metadata|split_offsets|equality_ids|sort_order_id|\n",
      "+-------+--------------------+-----------+-------+------------+------------------+--------------------+--------------------+--------------------+----------------+--------------------+--------------------+------------+-------------+------------+-------------+\n",
      "|      0|s3a://fileformats...|    PARQUET|      0|           1|              1534|{1 -> 52, 2 -> 55...|{1 -> 1, 2 -> 1, ...|{1 -> 0, 2 -> 0, ...|              {}|{1 -> \u0001\u0000\u0000\u0000\u0000\u0000\u0000\u0000, 2...|{1 -> \u0001\u0000\u0000\u0000\u0000\u0000\u0000\u0000, 2...|        null|          [4]|        null|            0|\n",
      "|      0|s3a://fileformats...|    PARQUET|      0|           1|              1226|{1 -> 52, 2 -> 55...|{1 -> 1, 2 -> 1, ...|{1 -> 0, 2 -> 0, ...|              {}|{1 -> \u0001\u0000\u0000\u0000\u0000\u0000\u0000\u0000, 2...|{1 -> \u0001\u0000\u0000\u0000\u0000\u0000\u0000\u0000, 2...|        null|          [4]|        null|            0|\n",
      "|      0|s3a://fileformats...|    PARQUET|      0|           1|              1232|{1 -> 51, 2 -> 56...|{1 -> 1, 2 -> 1, ...|{1 -> 0, 2 -> 0, ...|              {}|{1 -> \u0002\u0000\u0000\u0000\u0000\u0000\u0000\u0000, 2...|{1 -> \u0002\u0000\u0000\u0000\u0000\u0000\u0000\u0000, 2...|        null|          [4]|        null|            0|\n",
      "|      0|s3a://fileformats...|    PARQUET|      0|           1|              1227|{1 -> 52, 2 -> 55...|{1 -> 1, 2 -> 1, ...|{1 -> 0, 2 -> 0, ...|              {}|{1 -> \u0001\u0000\u0000\u0000\u0000\u0000\u0000\u0000, 2...|{1 -> \u0001\u0000\u0000\u0000\u0000\u0000\u0000\u0000, 2...|        null|          [4]|        null|            0|\n",
      "|      0|s3a://fileformats...|    PARQUET|      0|           2|              1236|{1 -> 55, 2 -> 63...|{1 -> 2, 2 -> 2, ...|{1 -> 0, 2 -> 0, ...|              {}|{1 -> \u0002\u0000\u0000\u0000\u0000\u0000\u0000\u0000, 2...|{1 -> \u0004\u0000\u0000\u0000\u0000\u0000\u0000\u0000, 2...|        null|          [4]|        null|            0|\n",
      "|      0|s3a://fileformats...|    PARQUET|      0|           1|              1226|{1 -> 52, 2 -> 55...|{1 -> 1, 2 -> 1, ...|{1 -> 0, 2 -> 0, ...|              {}|{1 -> \u0003\u0000\u0000\u0000\u0000\u0000\u0000\u0000, 2...|{1 -> \u0003\u0000\u0000\u0000\u0000\u0000\u0000\u0000, 2...|        null|          [4]|        null|            0|\n",
      "+-------+--------------------+-----------+-------+------------+------------------+--------------------+--------------------+--------------------+----------------+--------------------+--------------------+------------+-------------+------------+-------------+\n",
      "\n",
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "|        committed_at|        snapshot_id|          parent_id|operation|       manifest_list|             summary|\n",
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "|2023-05-15 14:12:...|2282180466624073266|               null|   append|s3a://fileformats...|{spark.app.id -> ...|\n",
      "|2023-05-15 14:13:...|4263160168885610306|2282180466624073266|   append|s3a://fileformats...|{spark.app.id -> ...|\n",
      "|2023-05-15 14:16:...|3806458280092384569|4263160168885610306|   append|s3a://fileformats...|{spark.app.id -> ...|\n",
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark.sql(\"SELECT * FROM iceberg_db.iceberg.history;\").show()\n",
    "spark.sql(\"SELECT * FROM iceberg_db.iceberg.files;\").show()\n",
    "spark.sql(\"SELECT * FROM iceberg_db.iceberg.snapshots;\").show()\n",
    "\n",
    "## alternative syntax example:\n",
    "# spark.read.format(\"iceberg\").load(\"iceberg_db.iceberg_table.files\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6db241",
   "metadata": {},
   "source": [
    "### Iceberg: Time Travel\n",
    "- ```snapshot-id``` selects a specific table snapshot\n",
    "- ```as-of-timestamp``` selects the current snapshot at a timestamp, in milliseconds\n",
    "- ```branch``` selects the head snapshot of the specified branch. Note that currently branch cannot be combined with as-of-timestamp.\n",
    "- ```tag``` selects the snapshot associated with the specified tag. Tags cannot be combined with as-of-timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6c2d137e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 107:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------+-------+\n",
      "| id|account|dt_transaction|balance|\n",
      "+---+-------+--------------+-------+\n",
      "|  1|   alex|    2019-01-01|   1000|\n",
      "|  2|   alex|    2019-02-01|   1500|\n",
      "|  4|  maria|    2020-01-01|   5000|\n",
      "|  3|   alex|    2019-03-01|   1700|\n",
      "+---+-------+--------------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# from the results of iceberg_table.snapshots get the snapshots IDs\n",
    "snapshot1 = spark.read \\\n",
    "                 .option(\"snapshot-id\", \"2282180466624073266\") \\\n",
    "                 .format(\"iceberg\") \\\n",
    "                 .load(\"iceberg_db.iceberg\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "da46097d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------+-------+\n",
      "| id|account|dt_transaction|balance|\n",
      "+---+-------+--------------+-------+\n",
      "|  1|   alex|    2019-01-01|   1000|\n",
      "|  2|   alex|    2019-02-01|   1500|\n",
      "|  4|  maria|    2020-01-01|   5000|\n",
      "|  3|   alex|    2019-03-01|   1700|\n",
      "|  1|   alex|    2019-03-01|   3300|\n",
      "|  2|  peter|    2021-01-01|    100|\n",
      "+---+-------+--------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "snapshot2 = spark.read \\\n",
    "                 .option(\"snapshot-id\", \"4263160168885610306\") \\\n",
    "                 .format(\"iceberg\") \\\n",
    "                 .load(\"iceberg_db.iceberg\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "abaf72d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column<'(current_timestamp() - INTERVAL '0 00:10:00' DAY TO SECOND)'>\n"
     ]
    }
   ],
   "source": [
    "tsToExpire = f.current_timestamp() - timedelta(minutes=10)\n",
    "print(tsToExpire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "60455a0f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'table' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[82], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## need iceberg.table\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtable\u001b[49m\u001b[38;5;241m.\u001b[39mexpireSnapshots()\u001b[38;5;241m.\u001b[39mexpireOlderThan(tsToExpire)\u001b[38;5;241m.\u001b[39mcommit();\n",
      "\u001b[0;31mNameError\u001b[0m: name 'table' is not defined"
     ]
    }
   ],
   "source": [
    "## need iceberg.table\n",
    "## geth nicht verstehe ich. noch nicht??\n",
    "table.expireSnapshots().expireOlderThan(tsToExpire).commit();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94da9d7c",
   "metadata": {},
   "source": [
    "# Hudi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e558c54",
   "metadata": {},
   "source": [
    "- a **storage abstraction layer** \n",
    "- enables data ingestion and query capability on large-scale, evolving datasets\n",
    "- well-suited for real-time streaming workloads and batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "99ae9de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update partition path, i.e. \"id/dt_transaction\"\n",
    "record_key = \"id\"\n",
    "partition_path = \"id\"\n",
    "\n",
    "hudi_options = {\n",
    "    \"hoodie.table.name\": df1,\n",
    "    \"hoodie.datasource.write.recordkey.field\": record_key,\n",
    "    \"hoodie.datasource.write.partitionpath.field\": partition_path,\n",
    "    \"hoodie.datasource.write.table.name\": df1,\n",
    "    \"hoodie.datasource.write.operation\": \"upsert\",\n",
    "    \"hoodie.datasource.write.precombine.field\": \"ts\",  # This field is used by Hoodie to resolve conflicts between records with the same key (in this case, id) \n",
    "    \"hoodie.upsert.shuffle.parallelism\": 2,\n",
    "    \"hoodie.insert.shuffle.parallelism\": 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1ad416e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "write_hudi=(df1.withColumn(\"ts\", f.current_timestamp()).write.format(\"hudi\") # \"ts\" field is a mandatory field in Hoodie that specifies the timestamp of the record, so we add a new column and use simple current_timestamp() function\n",
    "               .options(**hudi_options)\n",
    "               .mode(\"overwrite\")\n",
    "               .save(f\"s3://{bucket}/hudi\")\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e2136c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hudi/.hoodie/.aux/.bootstrap/.fileids/\n",
      "hudi/.hoodie/.aux/.bootstrap/.partitions/\n",
      "hudi/.hoodie/.schema/\n",
      "hudi/.hoodie/.temp/\n",
      "hudi/.hoodie/20230515122157032.commit\n",
      "hudi/.hoodie/20230515122157032.commit.requested\n",
      "hudi/.hoodie/20230515122157032.inflight\n",
      "hudi/.hoodie/archived/\n",
      "hudi/.hoodie/hoodie.properties\n",
      "hudi/.hoodie/metadata/.hoodie/.aux/.bootstrap/.fileids/\n",
      "hudi/.hoodie/metadata/.hoodie/.aux/.bootstrap/.partitions/\n",
      "hudi/.hoodie/metadata/.hoodie/.schema/\n",
      "hudi/.hoodie/metadata/.hoodie/.temp/\n",
      "hudi/.hoodie/metadata/.hoodie/00000000000000.deltacommit\n",
      "hudi/.hoodie/metadata/.hoodie/00000000000000.deltacommit.inflight\n",
      "hudi/.hoodie/metadata/.hoodie/00000000000000.deltacommit.requested\n",
      "hudi/.hoodie/metadata/.hoodie/20230515122157032.deltacommit\n",
      "hudi/.hoodie/metadata/.hoodie/20230515122157032.deltacommit.inflight\n",
      "hudi/.hoodie/metadata/.hoodie/20230515122157032.deltacommit.requested\n",
      "hudi/.hoodie/metadata/.hoodie/archived/\n",
      "hudi/.hoodie/metadata/.hoodie/hoodie.properties\n",
      "hudi/.hoodie/metadata/files/.files-0000_00000000000000.log.1_0-0-0\n",
      "hudi/.hoodie/metadata/files/.files-0000_00000000000000.log.2_0-138-601\n",
      "hudi/.hoodie/metadata/files/.hoodie_partition_metadata\n",
      "hudi/1/.hoodie_partition_metadata\n",
      "hudi/1/cf0f8517-23b9-4415-ac61-57bdbfe69965-0_0-127-589_20230515122157032.parquet\n",
      "hudi/2/.hoodie_partition_metadata\n",
      "hudi/2/0cd93c57-fb35-4ff7-83ac-566835d662a0-0_1-127-590_20230515122157032.parquet\n",
      "hudi/3/.hoodie_partition_metadata\n",
      "hudi/3/d3728799-997a-412c-ae8d-bfcb35c89ea6-0_2-127-591_20230515122157032.parquet\n",
      "hudi/4/.hoodie_partition_metadata\n",
      "hudi/4/ee5e9af9-3f2d-471c-943d-26cc7607b950-0_3-127-592_20230515122157032.parquet\n"
     ]
    }
   ],
   "source": [
    "ls(bucket,\"hudi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "52aa8b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# WARNING: Unable to get Instrumentation. Dynamic Attach failed. You may add this JAR as -javaagent manually, or supply -Djdk.attach.allowAttachSelf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 151:>                                                        (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# WARNING: Unable to attach Serviceability Agent. Unable to attach even with module exceptions: [org.apache.hudi.org.openjdk.jol.vm.sa.SASupportException: Sense failed., org.apache.hudi.org.openjdk.jol.vm.sa.SASupportException: Sense failed., org.apache.hudi.org.openjdk.jol.vm.sa.SASupportException: Sense failed.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "write_hudi=(df2.withColumn(\"ts\", f.current_timestamp()).write.format(\"hudi\") # \"ts\" field is a mandatory field in Hoodie that specifies the timestamp of the record, so we add a new column and use simple current_timestamp() function\n",
    "               .options(**hudi_options)\n",
    "               .mode(\"append\")\n",
    "               .save(f\"s3://{bucket}/hudi\")\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ed1b5e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "write_hudi=(df3.withColumn(\"ts\", f.current_timestamp()).write.format(\"hudi\") # \"ts\" field is a mandatory field in Hoodie that specifies the timestamp of the record, so we add a new column and use simple current_timestamp() function\n",
    "               .options(**hudi_options)\n",
    "               .mode(\"append\")\n",
    "               .save(f\"s3://{bucket}/hudi\")\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6b2066ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+------------------+----------------------+--------------------+-------+--------------+-------+-------------+--------------------+---+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|account|dt_transaction|balance|          new|                  ts| id|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-------+--------------+-------+-------------+--------------------+---+\n",
      "|  20230515122339203|20230515122339203...|                 1|                     1|cf0f8517-23b9-441...|   otto|    2019-10-01|   4444|neue Spalte 1|2023-05-15 14:23:...|  1|\n",
      "|  20230515122157032|20230515122157032...|                 4|                     4|ee5e9af9-3f2d-471...|  maria|    2020-01-01|   5000|         null|2023-05-15 14:21:...|  4|\n",
      "|  20230515122321620|20230515122321620...|                 2|                     2|0cd93c57-fb35-4ff...|  peter|    2021-01-01|    100|         null|2023-05-15 14:23:...|  2|\n",
      "|  20230515122157032|20230515122157032...|                 3|                     3|d3728799-997a-412...|   alex|    2019-03-01|   1700|         null|2023-05-15 14:21:...|  3|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-------+--------------+-------+-------------+--------------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_hudi = spark.read.format(\"hudi\").load(f\"s3://{bucket}/hudi\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "814d698d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hudi/.hoodie/.aux/.bootstrap/.fileids/\n",
      "hudi/.hoodie/.aux/.bootstrap/.partitions/\n",
      "hudi/.hoodie/.schema/\n",
      "hudi/.hoodie/.temp/\n",
      "hudi/.hoodie/20230515122157032.commit\n",
      "hudi/.hoodie/20230515122157032.commit.requested\n",
      "hudi/.hoodie/20230515122157032.inflight\n",
      "hudi/.hoodie/20230515122321620.commit\n",
      "hudi/.hoodie/20230515122321620.commit.requested\n",
      "hudi/.hoodie/20230515122321620.inflight\n",
      "hudi/.hoodie/20230515122339203.commit\n",
      "hudi/.hoodie/20230515122339203.commit.requested\n",
      "hudi/.hoodie/20230515122339203.inflight\n",
      "hudi/.hoodie/archived/\n",
      "hudi/.hoodie/hoodie.properties\n",
      "hudi/.hoodie/metadata/.hoodie/.aux/.bootstrap/.fileids/\n",
      "hudi/.hoodie/metadata/.hoodie/.aux/.bootstrap/.partitions/\n",
      "hudi/.hoodie/metadata/.hoodie/.schema/\n",
      "hudi/.hoodie/metadata/.hoodie/.temp/\n",
      "hudi/.hoodie/metadata/.hoodie/00000000000000.deltacommit\n",
      "hudi/.hoodie/metadata/.hoodie/00000000000000.deltacommit.inflight\n",
      "hudi/.hoodie/metadata/.hoodie/00000000000000.deltacommit.requested\n",
      "hudi/.hoodie/metadata/.hoodie/20230515122157032.deltacommit\n",
      "hudi/.hoodie/metadata/.hoodie/20230515122157032.deltacommit.inflight\n",
      "hudi/.hoodie/metadata/.hoodie/20230515122157032.deltacommit.requested\n",
      "hudi/.hoodie/metadata/.hoodie/20230515122321620.deltacommit\n",
      "hudi/.hoodie/metadata/.hoodie/20230515122321620.deltacommit.inflight\n",
      "hudi/.hoodie/metadata/.hoodie/20230515122321620.deltacommit.requested\n",
      "hudi/.hoodie/metadata/.hoodie/20230515122339203.deltacommit\n",
      "hudi/.hoodie/metadata/.hoodie/20230515122339203.deltacommit.inflight\n",
      "hudi/.hoodie/metadata/.hoodie/20230515122339203.deltacommit.requested\n",
      "hudi/.hoodie/metadata/.hoodie/archived/\n",
      "hudi/.hoodie/metadata/.hoodie/hoodie.properties\n",
      "hudi/.hoodie/metadata/files/.files-0000_00000000000000.log.1_0-0-0\n",
      "hudi/.hoodie/metadata/files/.files-0000_00000000000000.log.2_0-138-601\n",
      "hudi/.hoodie/metadata/files/.files-0000_00000000000000.log.3_0-175-641\n",
      "hudi/.hoodie/metadata/files/.files-0000_00000000000000.log.4_0-212-674\n",
      "hudi/.hoodie/metadata/files/.hoodie_partition_metadata\n",
      "hudi/1/.hoodie_partition_metadata\n",
      "hudi/1/cf0f8517-23b9-4415-ac61-57bdbfe69965-0_0-127-589_20230515122157032.parquet\n",
      "hudi/1/cf0f8517-23b9-4415-ac61-57bdbfe69965-0_0-164-633_20230515122321620.parquet\n",
      "hudi/1/cf0f8517-23b9-4415-ac61-57bdbfe69965-0_0-201-668_20230515122339203.parquet\n",
      "hudi/2/.hoodie_partition_metadata\n",
      "hudi/2/0cd93c57-fb35-4ff7-83ac-566835d662a0-0_1-127-590_20230515122157032.parquet\n",
      "hudi/2/0cd93c57-fb35-4ff7-83ac-566835d662a0-0_1-164-634_20230515122321620.parquet\n",
      "hudi/3/.hoodie_partition_metadata\n",
      "hudi/3/d3728799-997a-412c-ae8d-bfcb35c89ea6-0_2-127-591_20230515122157032.parquet\n",
      "hudi/4/.hoodie_partition_metadata\n",
      "hudi/4/ee5e9af9-3f2d-471c-943d-26cc7607b950-0_3-127-592_20230515122157032.parquet\n"
     ]
    }
   ],
   "source": [
    "ls(bucket,\"hudi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2c394a",
   "metadata": {},
   "source": [
    "#### Hudi: Time Travel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1a428acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+------------------+----------------------+--------------------+-------+--------------+-------+-------------+--------------------+---+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|account|dt_transaction|balance|          new|                  ts| id|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-------+--------------+-------+-------------+--------------------+---+\n",
      "|  20230515122339203|20230515122339203...|                 1|                     1|cf0f8517-23b9-441...|   otto|    2019-10-01|   4444|neue Spalte 1|2023-05-15 14:23:...|  1|\n",
      "|  20230515122157032|20230515122157032...|                 4|                     4|ee5e9af9-3f2d-471...|  maria|    2020-01-01|   5000|         null|2023-05-15 14:21:...|  4|\n",
      "|  20230515122321620|20230515122321620...|                 2|                     2|0cd93c57-fb35-4ff...|  peter|    2021-01-01|    100|         null|2023-05-15 14:23:...|  2|\n",
      "|  20230515122157032|20230515122157032...|                 3|                     3|d3728799-997a-412...|   alex|    2019-03-01|   1700|         null|2023-05-15 14:21:...|  3|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-------+--------------+-------+-------------+--------------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Get the commit time from the Hudi table\n",
    "\n",
    "spark.read.format(\"hudi\")\\\n",
    "     .option(\"as.of.instant\", \"20230515122339203\")\\\n",
    "     .load(f\"s3://{bucket}/hudi\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "46611d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "account_data4 = [\n",
    "    (5,\"anna\",\"2020-11-01\",2000,\"neue Spalte 1\")\n",
    "]\n",
    "df4 = spark.createDataFrame(data=account_data4, schema = schema3).withColumn(\"dt_transaction\",col(\"dt_transaction\").cast(\"date\")).repartition(3)\n",
    "\n",
    "write_hudi=(df4.withColumn(\"ts\", f.current_timestamp()).write.format(\"hudi\")\n",
    "               .options(**hudi_options)\n",
    "               .mode(\"append\")\n",
    "               .save(f\"s3://{bucket}/hudi\")\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ea78e5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Incremental query:\n",
    "\n",
    "spark.read.format(\"hudi\"). \\\n",
    "  load(f\"s3://{bucket}/hudi\"). \\\n",
    "  createOrReplaceTempView(\"hudi_snapshots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "fc0c8bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 276:============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20230515122157032', '20230515122321620', '20230515122339203', '20230515123027657']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "commits = list(map(lambda row: row[0], spark.sql(\"select distinct(_hoodie_commit_time) as commitTime from hudi_snapshots order by commitTime\").limit(10).collect()))\n",
    "print(commits)\n",
    "\n",
    "beginTime = commits[len(commits) - 4] # commit time we are interested in\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9c791ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+-------+--------------+--------------------------+\n",
      "|_hoodie_commit_time|account|balance|dt_transaction|ts                        |\n",
      "+-------------------+-------+-------+--------------+--------------------------+\n",
      "|20230515122339203  |otto   |4444   |2019-10-01    |2023-05-15 14:23:39.428869|\n",
      "|20230515123027657  |anna   |2000   |2020-11-01    |2023-05-15 14:30:27.805069|\n",
      "|20230515122321620  |peter  |100    |2021-01-01    |2023-05-15 14:23:21.809413|\n",
      "+-------------------+-------+-------+--------------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# incrementally query data\n",
    "incremental_read_options = {\n",
    "  'hoodie.datasource.query.type': 'incremental',\n",
    "  'hoodie.datasource.read.begin.instanttime': beginTime,\n",
    "}\n",
    "\n",
    "hudiIncrementalDF = spark.read.format(\"hudi\"). \\\n",
    "  options(**incremental_read_options). \\\n",
    "  load(f\"s3://{bucket}/hudi\")\n",
    "hudiIncrementalDF .createOrReplaceTempView(\"hudi_incremental\")\n",
    "\n",
    "spark.sql(\"select `_hoodie_commit_time`, account, balance, dt_transaction, ts from hudi_incremental\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ac5492",
   "metadata": {},
   "source": [
    "### Hudi: Table maintenance\n",
    "Hudi can run async or inline table services while running Strucrured Streaming query and takes care of cleaning, compaction and clustering. There's no operational overhead for the user.\n",
    "For CoW tables, table services work in inline mode by default.\n",
    "For MoR tables, some async services are enabled by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fcb4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
