apiVersion: "sparkoperator.k8s.io/v1beta2"
kind: SparkApplication
metadata:
  name: performance-test
  namespace: spark
spec:
  type: Python
  pythonVersion: "3"
  mode: cluster
  image: "thinkportgmbh/workshops:spark-3.3.1"
  imagePullPolicy: IfNotPresent # or Always
  volumes:
    - name: data
      persistentVolumeClaim:
        claimName: spark-history-server
  # Python Script/Java Jar that should be submitted
  mainApplicationFile: s3a://scripts/performance_test.py
  sparkVersion: "3.3.1"
  # extra Spark configurations
  sparkConf:
    "spark.serializer": "org.apache.spark.serializer.KryoSerializer"
    "spark.driver.extraJavaOptions": "-Divy.cache.dir=/tmp -Divy.home=/tmp"
    "spark.eventLog.enabled": "true"
    "spark.eventLog.dir": "file:/tmp/spark-events"
  # s3 connection configuration
  hadoopConf:
    "fs.s3a.endpoint": "minio.minio.svc.cluster.local:9000"
    "fs.s3a.access.key": "trainadm"
    "fs.s3a.secret.key": "train@thinkport"
    "fs.s3a.path.style.access": "true"
    "fs.s3.impl": "org.apache.hadoop.fs.s3a.S3AFileSystem"
    "fs.s3a.aws.credentials.provider": "org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider"
    "fs.s3a.connection.ssl.enabled": "false"
  # driver pod configuration
  driver:
    cores: 1
    coreLimit: "1200m"
    memory: "512m"
    labels:
      version: 3.3.1
    serviceAccount: spark
    env:
      - name: app_name
        value: "auto_broadcast"
    volumeMounts:
      - name: data
        # default path the spark history server looks for logs
        mountPath: /tmp/spark-events
  # executor pod configuration
  executor:
    cores: 1
    instances: 1
    memory: "512m"
    labels:
      version: 3.3.1
    volumeMounts:
      - name: data
        # default path the spark history server looks for logs
        mountPath: /tmp/spark-events
  monitoring:
    exposeDriverMetrics: true
    exposeExecutorMetrics: true
