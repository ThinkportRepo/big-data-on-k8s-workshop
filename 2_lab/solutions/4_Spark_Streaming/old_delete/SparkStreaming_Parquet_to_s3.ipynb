{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d998ae2c",
   "metadata": {},
   "source": [
    "# Spark Streaming - Write to Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b53a21eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_537818/1492466114.py:15: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container {width:100% !important; }<style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import Row\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "\n",
    "# use 95% of the screen for jupyter cell\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container {width:100% !important; }<style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c547643b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.kubernetes.namespace = frontend\n",
      "spark.app.name = jupyter-stream-to-s3\n",
      "spark.master = k8s://https://kubernetes.default.svc.cluster.local:443\n",
      "spark.executor.memory = 1G\n",
      "spark.executor.cores = 2\n",
      "spark.driver.host = jupyter-spark-driver.frontend.svc.cluster.local\n"
     ]
    }
   ],
   "source": [
    "appName=\"jupyter-stream-to-s3\"\n",
    "\n",
    "conf = SparkConf()\n",
    "\n",
    "# CLUSTER MANAGER\n",
    "################################################################################\n",
    "# set Kubernetes Master as Cluster Manager(“k8s://https://” is NOT a typo, this is how Spark knows the “provider” type).\n",
    "conf.setMaster(\"k8s://https://kubernetes.default.svc.cluster.local:443\")\n",
    "\n",
    "# CONFIGURE KUBERNETES\n",
    "################################################################################\n",
    "# set the namespace that will be used for running the driver and executor pods.\n",
    "conf.set(\"spark.kubernetes.namespace\",\"frontend\")\n",
    "# set the docker image from which the Worker pods are created\n",
    "conf.set(\"spark.kubernetes.container.image\", \"thinkportgmbh/workshops:spark-3.3.1\")\n",
    "conf.set(\"spark.kubernetes.container.image.pullPolicy\", \"Always\")\n",
    "\n",
    "# set service account to be used\n",
    "conf.set(\"spark.kubernetes.authenticate.driver.serviceAccountName\", \"spark\")\n",
    "# authentication for service account(required to create worker pods):\n",
    "conf.set(\"spark.kubernetes.authenticate.caCertFile\", \"/var/run/secrets/kubernetes.io/serviceaccount/ca.crt\")\n",
    "conf.set(\"spark.kubernetes.authenticate.oauthTokenFile\", \"/var/run/secrets/kubernetes.io/serviceaccount/token\")\n",
    "\n",
    "\n",
    "# CONFIGURE SPARK\n",
    "################################################################################\n",
    "conf.set(\"spark.sql.session.timeZone\", \"Europe/Berlin\")\n",
    "# set driver host. In this case the ingres service for the spark driver\n",
    "# find name of the driver service with 'kubectl get services' or in the helm chart configuration\n",
    "conf.set(\"spark.driver.host\", \"jupyter-spark-driver.frontend.svc.cluster.local\")\n",
    "# set the port, If this port is busy, spark-shell tries to bind to another port.\n",
    "conf.set(\"spark.driver.port\", \"29413\")\n",
    "# add the postgres driver jars into session\n",
    "conf.set(\"spark.jars\", \"/opt/spark/jars/spark-sql-kafka-0-10_2.12-3.3.1.jar, /opt/spark/jars/kafka-clients-3.3.1.jar\")\n",
    "conf.set(\"spark.driver.extraClassPath\",\"/opt/spark/jars/spark-sql-kafka-0-10_2.12-3.3.1.jar, /opt/spark/jars/kafka-clients-3.3.1.jar\")\n",
    "conf.set(\"spark.executor.extraClassPath\",\"/opt/spark/jars/spark-sql-kafka-0-10_2.12-3.3.1.jar, /opt/spark/jars/kafka-clients-3.3.1.jar\")\n",
    "#conf.set(\"spark.executor.extraLibrary\",\"/opt/spark/jars/spark-sql-kafka-0-10_2.12-3.3.1.jar, /opt/spark/jars/kafka-clients-3.3.1.jar\")\n",
    "\n",
    "# CONFIGURE S3 CONNECTOR\n",
    "conf.set(\"spark.hadoop.fs.s3a.endpoint\", \"minio.minio.svc.cluster.local:9000\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.access.key\", \"trainadm\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.secret.key\", \"train@thinkport\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "conf.set(\"spark.hadoop.fs.s3.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "\n",
    "# conf.set(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.1\")\n",
    "\n",
    "# CONFIGURE WORKER (Customize based on workload)\n",
    "################################################################################\n",
    "# set number of worker pods\n",
    "conf.set(\"spark.executor.instances\", \"1\")\n",
    "# set memory of each worker pod\n",
    "conf.set(\"spark.executor.memory\", \"1G\")\n",
    "# set cpu of each worker pod\n",
    "conf.set(\"spark.executor.cores\", \"2\")\n",
    "\n",
    "# SPARK SESSION\n",
    "################################################################################\n",
    "# and last, create the spark session and pass it the config object\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .config(conf=conf) \\\n",
    "    .config('spark.sql.session.timeZone', 'Europe/Berlin') \\\n",
    "    .appName(appName)\\\n",
    "    .getOrCreate()\n",
    "\n",
    "# also get the spark context\n",
    "sc=spark.sparkContext\n",
    "# change the log level to warning, to see less output\n",
    "sc.setLogLevel('WARN')\n",
    "\n",
    "# get the configuration object to check all the configurations the session was startet with\n",
    "for entry in sc.getConf().getAll():\n",
    "        if entry[0] in [\"spark.app.name\",\"spark.kubernetes.namespace\",\"spark.executor.memory\",\"spark.executor.cores\",\"spark.driver.host\",\"spark.master\"]:\n",
    "            print(entry[0],\"=\",entry[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c47cd2cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_step_1 = (spark\n",
    "      .readStream\n",
    "      .format(\"kafka\")\n",
    "      .option(\"kafka.bootstrap.servers\", \"kafka-cp-kafka.kafka.svc.cluster.local:9092\")\n",
    "      .option(\"subscribe\", \"twitter-json\")\n",
    "      .option(\"startingOffsets\", \"earliest\")\n",
    "      .load()\n",
    "     )\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb622cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "   df_step_2 = (df_step_1\n",
    "       # take only the value and cast as string\n",
    "       .selectExpr(\"CAST(value AS STRING)\")\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba1302bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_step_3 = (df_step_2\n",
    "       .select(\n",
    "           f.get_json_object(f.col(\"value\"),\"$.payload.CreatedAt\").alias(\"tweet_created\"),\n",
    "           f.get_json_object(f.col(\"value\"),\"$.payload.Id\").alias(\"tweet_id\"),\n",
    "           f.get_json_object(f.col(\"value\"),\"$.payload.Lang\").alias(\"tweet_language\"),\n",
    "           f.get_json_object(f.col(\"value\"),\"$.payload.HashtagEntities[*].Text\").alias(\"hashtag\"),\n",
    "           f.get_json_object(f.col(\"value\"),\"$.payload.Place.Country\").alias(\"country\"),\n",
    "           f.get_json_object(f.col(\"value\"),\"$.payload.Place.CountryCode\").alias(\"country_code\"),\n",
    "           f.get_json_object(f.col(\"value\"),\"$.payload.User.ScreenName\").alias(\"user\"),\n",
    "           f.get_json_object(f.col(\"value\"),\"$.payload.User.Lang\").alias(\"user_language\"),   \n",
    "           f.get_json_object(f.col(\"value\"),\"$.payload.User.Location\").alias(\"user_location\"),\n",
    "           f.get_json_object(f.col(\"value\"),\"$.payload.User.StatusesCount\").alias(\"statuses_count\"),\n",
    "           f.get_json_object(f.col(\"value\"),\"$.payload.RetweetCount\").alias(\"retweet_count\"),\n",
    "           f.get_json_object(f.col(\"value\"),\"$.payload.Text\").alias(\"tweet_text\")    \n",
    "       ) \n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d180e326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UDF to parse array stored as string using JSON\n",
    "def parse_array_from_string(x):\n",
    "    \n",
    "    if x is not None:\n",
    "        res = json.loads(x)\n",
    "    else:\n",
    "        res =[None]\n",
    "    return res\n",
    "\n",
    "hashtag_array = f.udf(parse_array_from_string, ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7631bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    " df_step_4 = (df_step_3\n",
    "        # cast to correct data types \n",
    "        # convert to timestamp (cast string to long --> convert from number to timestamp)\n",
    "        .withColumn(\"tweet_created\",f.from_unixtime((f.col(\"tweet_created\").cast(\"long\"))/1000))\n",
    "        .withColumn(\"tweet_id\",f.col(\"tweet_id\").cast(\"long\"))\n",
    "        # using udf function to convert string to json array\n",
    "        .withColumn(\"hashtag\",hashtag_array(f.col(\"hashtag\")))\n",
    "        .withColumn(\"statuses_count\",f.col(\"statuses_count\").cast(\"int\"))\n",
    "        .withColumn(\"retweet_count\",f.col(\"retweet_count\").cast(\"int\"))\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "589d178b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/29 16:55:18 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "stream_query=(df_step_4\n",
    "        .writeStream\n",
    "        .format(\"parquet\")\n",
    "        .option(\"path\", \"s3a://twitter/twitter_clean\")\n",
    "        .trigger(processingTime='2 seconds')\n",
    "        .option(\"checkpointLocation\", \"/opt/spark/work-dir\")\n",
    "        .start()\n",
    "        #.awaitTermination()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "26c937c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "stream_query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "378c365b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.streams.active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "caf40f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/29 16:50:04 WARN ExecutorPodsWatchSnapshotSource: Kubernetes client has been closed.\n"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d85984",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
