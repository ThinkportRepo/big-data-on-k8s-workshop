{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbff7e9a",
   "metadata": {},
   "source": [
    "# Spark Streaming Transform Data\n",
    "read data from Kafka topic, filter and reduce and write back to other Kafka Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150a23b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba0572c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34833/1808055798.py:21: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container {width:100% !important; }<style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.avro.functions import to_avro, from_avro\n",
    "\n",
    "# DELETE\n",
    "from schema_registry.client import SchemaRegistryClient\n",
    "\n",
    "\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "\n",
    "# use 95% of the screen for jupyter cell\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container {width:100% !important; }<style>\"))\n",
    "\n",
    "\n",
    "schemaRegistryAddr=\"http://kafka-cp-schema-registry.kafka.svc.cluster.local:8081\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c618dd27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.kubernetes.namespace = frontend\n",
      "spark.master = k8s://https://kubernetes.default.svc.cluster.local:443\n",
      "spark.app.name = jupyter-stream\n",
      "spark.executor.memory = 1G\n",
      "spark.executor.cores = 2\n",
      "spark.driver.host = jupyter-spark-driver.frontend.svc.cluster.local\n"
     ]
    }
   ],
   "source": [
    "appName=\"jupyter-stream\"\n",
    "\n",
    "conf = SparkConf()\n",
    "\n",
    "# CLUSTER MANAGER\n",
    "################################################################################\n",
    "# set Kubernetes Master as Cluster Manager(“k8s://https://” is NOT a typo, this is how Spark knows the “provider” type).\n",
    "conf.setMaster(\"k8s://https://kubernetes.default.svc.cluster.local:443\")\n",
    "\n",
    "# CONFIGURE KUBERNETES\n",
    "################################################################################\n",
    "# set the namespace that will be used for running the driver and executor pods.\n",
    "conf.set(\"spark.kubernetes.namespace\",\"frontend\")\n",
    "# set the docker image from which the Worker pods are created\n",
    "conf.set(\"spark.kubernetes.container.image\", \"thinkportgmbh/workshops:spark-3.3.1\")\n",
    "conf.set(\"spark.kubernetes.container.image.pullPolicy\", \"Always\")\n",
    "\n",
    "# set service account to be used\n",
    "conf.set(\"spark.kubernetes.authenticate.driver.serviceAccountName\", \"spark\")\n",
    "# authentication for service account(required to create worker pods):\n",
    "conf.set(\"spark.kubernetes.authenticate.caCertFile\", \"/var/run/secrets/kubernetes.io/serviceaccount/ca.crt\")\n",
    "conf.set(\"spark.kubernetes.authenticate.oauthTokenFile\", \"/var/run/secrets/kubernetes.io/serviceaccount/token\")\n",
    "\n",
    "\n",
    "# CONFIGURE SPARK\n",
    "################################################################################\n",
    "conf.set(\"spark.sql.adaptive.enabled\", \"False\")\n",
    "# set driver host. In this case the ingres service for the spark driver\n",
    "# find name of the driver service with 'kubectl get services' or in the helm chart configuration\n",
    "conf.set(\"spark.driver.host\", \"jupyter-spark-driver.frontend.svc.cluster.local\")\n",
    "# set the port, If this port is busy, spark-shell tries to bind to another port.\n",
    "conf.set(\"spark.driver.port\", \"29413\")\n",
    "# add the postgres driver jars into session\n",
    "conf.set(\"spark.jars\", \"/opt/spark/jars/spark-sql-kafka-0-10_2.12-3.3.1.jar, /opt/spark/jars/kafka-clients-3.3.1.jar, /opt/spark/jars/spark-avro_2.12-3.3.1.jar\")\n",
    "conf.set(\"spark.driver.extraClassPath\",\"/opt/spark/jars/spark-sql-kafka-0-10_2.12-3.3.1.jar, /opt/spark/jars/kafka-clients-3.3.1.jar, /opt/spark/jars/spark-avro_2.12-3.3.1.jar\")\n",
    "conf.set(\"spark.executor.extraClassPath\",\"/opt/spark/jars/spark-sql-kafka-0-10_2.12-3.3.1.jar, /opt/spark/jars/kafka-clients-3.3.1.jar, /opt/spark/jars/spark-avro_2.12-3.3.1.jar\")\n",
    "#conf.set(\"spark.executor.extraLibrary\",\"/opt/spark/jars/spark-sql-kafka-0-10_2.12-3.3.1.jar, /opt/spark/jars/kafka-clients-3.3.1.jar\")\n",
    "\n",
    "\n",
    "\n",
    "# CONFIGURE S3 CONNECTOR\n",
    "conf.set(\"spark.hadoop.fs.s3a.endpoint\", \"minio.minio.svc.cluster.local:9000\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.access.key\", \"trainadm\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.secret.key\", \"train@thinkport\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "conf.set(\"spark.hadoop.fs.s3.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "\n",
    "\n",
    "\n",
    "# conf.set(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.1\")\n",
    "\n",
    "# CONFIGURE WORKER (Customize based on workload)\n",
    "################################################################################\n",
    "# set number of worker pods\n",
    "conf.set(\"spark.executor.instances\", \"1\")\n",
    "# set memory of each worker pod\n",
    "conf.set(\"spark.executor.memory\", \"1G\")\n",
    "# set cpu of each worker pod\n",
    "conf.set(\"spark.executor.cores\", \"2\")\n",
    "\n",
    "# SPARK SESSION\n",
    "################################################################################\n",
    "# and last, create the spark session and pass it the config object\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .config(conf=conf) \\\n",
    "    .config('spark.sql.session.timeZone', 'Europe/Berlin') \\\n",
    "    .appName(appName)\\\n",
    "    .getOrCreate()\n",
    "\n",
    "# also get the spark context\n",
    "sc=spark.sparkContext\n",
    "ssc = StreamingContext(sc, 2)\n",
    "\n",
    "# change the log level to warning, to see less output\n",
    "sc.setLogLevel('WARN')\n",
    "\n",
    "# get the configuration object to check all the configurations the session was startet with\n",
    "for entry in sc.getConf().getAll():\n",
    "        if entry[0] in [\"spark.app.name\",\"spark.kubernetes.namespace\",\"spark.executor.memory\",\"spark.executor.cores\",\"spark.driver.host\",\"spark.master\"]:\n",
    "            print(entry[0],\"=\",entry[1])\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad755079",
   "metadata": {},
   "source": [
    "#### Read Schema from Kafka registry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b436a47f",
   "metadata": {},
   "source": [
    "#### Read Stream from Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "31316ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_step_1 = (spark\n",
    "      .readStream\n",
    "      .format(\"kafka\")\n",
    "      .option(\"kafka.bootstrap.servers\", \"kafka-cp-kafka.kafka.svc.cluster.local:9092\")\n",
    "      .option(\"subscribe\", \"twitter-json\")\n",
    "      .option(\"startingOffsets\", \"earliest\")\n",
    "      .load()\n",
    "     )\n",
    "\n",
    "df_step_1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "13563232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema einmal aus einer Rohnachricht bestimmen\n",
    "# PS am besten noch in eine Datei rausspeichern\n",
    "jsonSchema=StructType([StructField('payload', StructType([StructField('Contributors', ArrayType(StringType(), True), True), StructField('CreatedAt', LongType(), True), StructField('CurrentUserRetweetId', LongType(), True), StructField('FavoriteCount', LongType(), True), StructField('Favorited', BooleanType(), True), StructField('GeoLocation', StringType(), True), StructField('HashtagEntities', ArrayType(StructType([StructField('End', LongType(), True), StructField('Start', LongType(), True), StructField('Text', StringType(), True)]), True), True), StructField('Id', LongType(), True), StructField('InReplyToScreenName', StringType(), True), StructField('InReplyToStatusId', LongType(), True), StructField('InReplyToUserId', LongType(), True), StructField('Lang', StringType(), True), StructField('MediaEntities', ArrayType(StructType([StructField('DisplayURL', StringType(), True), StructField('End', LongType(), True), StructField('ExpandedURL', StringType(), True), StructField('ExtAltText', StringType(), True), StructField('Id', LongType(), True), StructField('MediaURL', StringType(), True), StructField('MediaURLHttps', StringType(), True), StructField('Sizes', ArrayType(ArrayType(StringType(), True), True), True), StructField('Start', LongType(), True), StructField('Text', StringType(), True), StructField('Type', StringType(), True), StructField('URL', StringType(), True), StructField('VideoAspectRatioHeight', LongType(), True), StructField('VideoAspectRatioWidth', LongType(), True), StructField('VideoDurationMillis', LongType(), True), StructField('VideoVariants', ArrayType(StructType([StructField('Bitrate', LongType(), True), StructField('ContentType', StringType(), True), StructField('Url', StringType(), True)]), True), True)]), True), True), StructField('Place', StructType([StructField('Country', StringType(), True), StructField('CountryCode', StringType(), True), StructField('FullName', StringType(), True), StructField('Id', StringType(), True), StructField('Name', StringType(), True), StructField('PlaceType', StringType(), True), StructField('StreetAddress', StringType(), True), StructField('URL', StringType(), True)]), True), StructField('PossiblySensitive', BooleanType(), True), StructField('Retweet', BooleanType(), True), StructField('RetweetCount', LongType(), True), StructField('Retweeted', BooleanType(), True), StructField('RetweetedByMe', BooleanType(), True), StructField('Source', StringType(), True), StructField('SymbolEntities', ArrayType(StructType([StructField('End', LongType(), True), StructField('Start', LongType(), True), StructField('Text', StringType(), True)]), True), True), StructField('Text', StringType(), True), StructField('Truncated', BooleanType(), True), StructField('URLEntities', ArrayType(StructType([StructField('DisplayURL', StringType(), True), StructField('End', LongType(), True), StructField('ExpandedURL', StringType(), True), StructField('Start', LongType(), True), StructField('Text', StringType(), True), StructField('URL', StringType(), True)]), True), True), StructField('User', StructType([StructField('BiggerProfileImageURL', StringType(), True), StructField('BiggerProfileImageURLHttps', StringType(), True), StructField('ContributorsEnabled', BooleanType(), True), StructField('CreatedAt', LongType(), True), StructField('DefaultProfile', BooleanType(), True), StructField('DefaultProfileImage', BooleanType(), True), StructField('Description', StringType(), True), StructField('FavouritesCount', LongType(), True), StructField('FollowRequestSent', BooleanType(), True), StructField('FollowersCount', LongType(), True), StructField('FriendsCount', LongType(), True), StructField('GeoEnabled', BooleanType(), True), StructField('Id', LongType(), True), StructField('Lang', StringType(), True), StructField('ListedCount', LongType(), True), StructField('Location', StringType(), True), StructField('MiniProfileImageURL', StringType(), True), StructField('MiniProfileImageURLHttps', StringType(), True), StructField('Name', StringType(), True), StructField('OriginalProfileImageURL', StringType(), True), StructField('OriginalProfileImageURLHttps', StringType(), True), StructField('ProfileBackgroundColor', StringType(), True), StructField('ProfileBackgroundImageURL', StringType(), True), StructField('ProfileBackgroundImageUrlHttps', StringType(), True), StructField('ProfileBackgroundTiled', BooleanType(), True), StructField('ProfileBannerIPadRetinaURL', StringType(), True), StructField('ProfileBannerIPadURL', StringType(), True), StructField('ProfileBannerMobileRetinaURL', StringType(), True), StructField('ProfileBannerMobileURL', StringType(), True), StructField('ProfileBannerRetinaURL', StringType(), True), StructField('ProfileBannerURL', StringType(), True), StructField('ProfileImageURL', StringType(), True), StructField('ProfileImageURLHttps', StringType(), True), StructField('ProfileLinkColor', StringType(), True), StructField('ProfileSidebarBorderColor', StringType(), True), StructField('ProfileSidebarFillColor', StringType(), True), StructField('ProfileTextColor', StringType(), True), StructField('ProfileUseBackgroundImage', BooleanType(), True), StructField('Protected', BooleanType(), True), StructField('ScreenName', StringType(), True), StructField('ShowAllInlineMedia', BooleanType(), True), StructField('StatusesCount', LongType(), True), StructField('TimeZone', StringType(), True), StructField('Translator', BooleanType(), True), StructField('URL', StringType(), True), StructField('UtcOffset', LongType(), True), StructField('Verified', BooleanType(), True), StructField('WithheldInCountries', ArrayType(StringType(), True), True)]), True), StructField('UserMentionEntities', ArrayType(StructType([StructField('End', LongType(), True), StructField('Id', LongType(), True), StructField('Name', StringType(), True), StructField('ScreenName', StringType(), True), StructField('Start', LongType(), True), StructField('Text', StringType(), True)]), True), True), StructField('WithheldInCountries', ArrayType(StringType(), True), True)]), True), StructField('schema', StructType([StructField('doc', StringType(), True), StructField('fields', ArrayType(StructType([StructField('doc', StringType(), True), StructField('field', StringType(), True), StructField('fields', ArrayType(StructType([StructField('doc', StringType(), True), StructField('field', StringType(), True), StructField('items', StructType([StructField('optional', BooleanType(), True), StructField('type', StringType(), True)]), True), StructField('name', StringType(), True), StructField('optional', BooleanType(), True), StructField('type', StringType(), True), StructField('version', LongType(), True)]), True), True), StructField('items', StructType([StructField('doc', StringType(), True), StructField('fields', ArrayType(StructType([StructField('doc', StringType(), True), StructField('field', StringType(), True), StructField('items', StructType([StructField('doc', StringType(), True), StructField('fields', ArrayType(StructType([StructField('doc', StringType(), True), StructField('field', StringType(), True), StructField('optional', BooleanType(), True), StructField('type', StringType(), True)]), True), True), StructField('name', StringType(), True), StructField('optional', BooleanType(), True), StructField('type', StringType(), True)]), True), StructField('keys', StructType([StructField('optional', BooleanType(), True), StructField('type', StringType(), True)]), True), StructField('optional', BooleanType(), True), StructField('type', StringType(), True), StructField('values', StructType([StructField('doc', StringType(), True), StructField('fields', ArrayType(StructType([StructField('doc', StringType(), True), StructField('field', StringType(), True), StructField('optional', BooleanType(), True), StructField('type', StringType(), True)]), True), True), StructField('name', StringType(), True), StructField('optional', BooleanType(), True), StructField('type', StringType(), True)]), True)]), True), True), StructField('name', StringType(), True), StructField('optional', BooleanType(), True), StructField('type', StringType(), True)]), True), StructField('name', StringType(), True), StructField('optional', BooleanType(), True), StructField('type', StringType(), True), StructField('version', LongType(), True)]), True), True), StructField('name', StringType(), True), StructField('optional', BooleanType(), True), StructField('type', StringType(), True)]), True), StructField('partition', IntegerType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4323a03a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- p: struct (nullable = true)\n",
      " |    |-- Contributors: array (nullable = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |    |-- CreatedAt: long (nullable = true)\n",
      " |    |-- CurrentUserRetweetId: long (nullable = true)\n",
      " |    |-- FavoriteCount: long (nullable = true)\n",
      " |    |-- Favorited: boolean (nullable = true)\n",
      " |    |-- GeoLocation: string (nullable = true)\n",
      " |    |-- HashtagEntities: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- End: long (nullable = true)\n",
      " |    |    |    |-- Start: long (nullable = true)\n",
      " |    |    |    |-- Text: string (nullable = true)\n",
      " |    |-- Id: long (nullable = true)\n",
      " |    |-- InReplyToScreenName: string (nullable = true)\n",
      " |    |-- InReplyToStatusId: long (nullable = true)\n",
      " |    |-- InReplyToUserId: long (nullable = true)\n",
      " |    |-- Lang: string (nullable = true)\n",
      " |    |-- MediaEntities: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- DisplayURL: string (nullable = true)\n",
      " |    |    |    |-- End: long (nullable = true)\n",
      " |    |    |    |-- ExpandedURL: string (nullable = true)\n",
      " |    |    |    |-- ExtAltText: string (nullable = true)\n",
      " |    |    |    |-- Id: long (nullable = true)\n",
      " |    |    |    |-- MediaURL: string (nullable = true)\n",
      " |    |    |    |-- MediaURLHttps: string (nullable = true)\n",
      " |    |    |    |-- Sizes: array (nullable = true)\n",
      " |    |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |    |-- Start: long (nullable = true)\n",
      " |    |    |    |-- Text: string (nullable = true)\n",
      " |    |    |    |-- Type: string (nullable = true)\n",
      " |    |    |    |-- URL: string (nullable = true)\n",
      " |    |    |    |-- VideoAspectRatioHeight: long (nullable = true)\n",
      " |    |    |    |-- VideoAspectRatioWidth: long (nullable = true)\n",
      " |    |    |    |-- VideoDurationMillis: long (nullable = true)\n",
      " |    |    |    |-- VideoVariants: array (nullable = true)\n",
      " |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |-- Bitrate: long (nullable = true)\n",
      " |    |    |    |    |    |-- ContentType: string (nullable = true)\n",
      " |    |    |    |    |    |-- Url: string (nullable = true)\n",
      " |    |-- Place: struct (nullable = true)\n",
      " |    |    |-- Country: string (nullable = true)\n",
      " |    |    |-- CountryCode: string (nullable = true)\n",
      " |    |    |-- FullName: string (nullable = true)\n",
      " |    |    |-- Id: string (nullable = true)\n",
      " |    |    |-- Name: string (nullable = true)\n",
      " |    |    |-- PlaceType: string (nullable = true)\n",
      " |    |    |-- StreetAddress: string (nullable = true)\n",
      " |    |    |-- URL: string (nullable = true)\n",
      " |    |-- PossiblySensitive: boolean (nullable = true)\n",
      " |    |-- Retweet: boolean (nullable = true)\n",
      " |    |-- RetweetCount: long (nullable = true)\n",
      " |    |-- Retweeted: boolean (nullable = true)\n",
      " |    |-- RetweetedByMe: boolean (nullable = true)\n",
      " |    |-- Source: string (nullable = true)\n",
      " |    |-- SymbolEntities: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- End: long (nullable = true)\n",
      " |    |    |    |-- Start: long (nullable = true)\n",
      " |    |    |    |-- Text: string (nullable = true)\n",
      " |    |-- Text: string (nullable = true)\n",
      " |    |-- Truncated: boolean (nullable = true)\n",
      " |    |-- URLEntities: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- DisplayURL: string (nullable = true)\n",
      " |    |    |    |-- End: long (nullable = true)\n",
      " |    |    |    |-- ExpandedURL: string (nullable = true)\n",
      " |    |    |    |-- Start: long (nullable = true)\n",
      " |    |    |    |-- Text: string (nullable = true)\n",
      " |    |    |    |-- URL: string (nullable = true)\n",
      " |    |-- User: struct (nullable = true)\n",
      " |    |    |-- BiggerProfileImageURL: string (nullable = true)\n",
      " |    |    |-- BiggerProfileImageURLHttps: string (nullable = true)\n",
      " |    |    |-- ContributorsEnabled: boolean (nullable = true)\n",
      " |    |    |-- CreatedAt: long (nullable = true)\n",
      " |    |    |-- DefaultProfile: boolean (nullable = true)\n",
      " |    |    |-- DefaultProfileImage: boolean (nullable = true)\n",
      " |    |    |-- Description: string (nullable = true)\n",
      " |    |    |-- FavouritesCount: long (nullable = true)\n",
      " |    |    |-- FollowRequestSent: boolean (nullable = true)\n",
      " |    |    |-- FollowersCount: long (nullable = true)\n",
      " |    |    |-- FriendsCount: long (nullable = true)\n",
      " |    |    |-- GeoEnabled: boolean (nullable = true)\n",
      " |    |    |-- Id: long (nullable = true)\n",
      " |    |    |-- Lang: string (nullable = true)\n",
      " |    |    |-- ListedCount: long (nullable = true)\n",
      " |    |    |-- Location: string (nullable = true)\n",
      " |    |    |-- MiniProfileImageURL: string (nullable = true)\n",
      " |    |    |-- MiniProfileImageURLHttps: string (nullable = true)\n",
      " |    |    |-- Name: string (nullable = true)\n",
      " |    |    |-- OriginalProfileImageURL: string (nullable = true)\n",
      " |    |    |-- OriginalProfileImageURLHttps: string (nullable = true)\n",
      " |    |    |-- ProfileBackgroundColor: string (nullable = true)\n",
      " |    |    |-- ProfileBackgroundImageURL: string (nullable = true)\n",
      " |    |    |-- ProfileBackgroundImageUrlHttps: string (nullable = true)\n",
      " |    |    |-- ProfileBackgroundTiled: boolean (nullable = true)\n",
      " |    |    |-- ProfileBannerIPadRetinaURL: string (nullable = true)\n",
      " |    |    |-- ProfileBannerIPadURL: string (nullable = true)\n",
      " |    |    |-- ProfileBannerMobileRetinaURL: string (nullable = true)\n",
      " |    |    |-- ProfileBannerMobileURL: string (nullable = true)\n",
      " |    |    |-- ProfileBannerRetinaURL: string (nullable = true)\n",
      " |    |    |-- ProfileBannerURL: string (nullable = true)\n",
      " |    |    |-- ProfileImageURL: string (nullable = true)\n",
      " |    |    |-- ProfileImageURLHttps: string (nullable = true)\n",
      " |    |    |-- ProfileLinkColor: string (nullable = true)\n",
      " |    |    |-- ProfileSidebarBorderColor: string (nullable = true)\n",
      " |    |    |-- ProfileSidebarFillColor: string (nullable = true)\n",
      " |    |    |-- ProfileTextColor: string (nullable = true)\n",
      " |    |    |-- ProfileUseBackgroundImage: boolean (nullable = true)\n",
      " |    |    |-- Protected: boolean (nullable = true)\n",
      " |    |    |-- ScreenName: string (nullable = true)\n",
      " |    |    |-- ShowAllInlineMedia: boolean (nullable = true)\n",
      " |    |    |-- StatusesCount: long (nullable = true)\n",
      " |    |    |-- TimeZone: string (nullable = true)\n",
      " |    |    |-- Translator: boolean (nullable = true)\n",
      " |    |    |-- URL: string (nullable = true)\n",
      " |    |    |-- UtcOffset: long (nullable = true)\n",
      " |    |    |-- Verified: boolean (nullable = true)\n",
      " |    |    |-- WithheldInCountries: array (nullable = true)\n",
      " |    |    |    |-- element: string (containsNull = true)\n",
      " |    |-- UserMentionEntities: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- End: long (nullable = true)\n",
      " |    |    |    |-- Id: long (nullable = true)\n",
      " |    |    |    |-- Name: string (nullable = true)\n",
      " |    |    |    |-- ScreenName: string (nullable = true)\n",
      " |    |    |    |-- Start: long (nullable = true)\n",
      " |    |    |    |-- Text: string (nullable = true)\n",
      " |    |-- WithheldInCountries: array (nullable = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_step_2= (df_step_1\n",
    "            # cast binary to string and string with json schema to json object\n",
    "            .select(f.from_json(f.col(\"value\").cast(\"string\"),jsonSchema).alias(\"data\"))\n",
    "            # from json take only the payload\n",
    "            .select(f.col(\"data.payload\").alias(\"p\"))\n",
    "           )\n",
    "\n",
    "df_step_2.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8403dd3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: long (nullable = true)\n",
      " |-- CreatedAt: long (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- CountryCode: string (nullable = true)\n",
      " |-- UserID: long (nullable = true)\n",
      " |-- ScreenName: string (nullable = true)\n",
      " |-- RetweetCount: long (nullable = true)\n",
      " |-- FriendsCount: long (nullable = true)\n",
      " |-- FollowersCount: long (nullable = true)\n",
      " |-- Lang: string (nullable = true)\n",
      " |-- UserLang: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Hashtags: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- Text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_step_3=(df_step_2\n",
    "           .select(\n",
    "               \"p.Id\",\n",
    "               \"p.CreatedAt\",\n",
    "               \"p.Place.Country\",\n",
    "               \"p.Place.CountryCode\",\n",
    "               f.col(\"p.User.id\").alias(\"UserID\"),\n",
    "               \"p.User.ScreenName\",\n",
    "               \"p.RetweetCount\",\n",
    "               \"p.User.FriendsCount\",\n",
    "               \"p.User.FollowersCount\",\n",
    "               \"p.Lang\",\n",
    "                f.col(\"p.User.Lang\").alias(\"UserLang\"),\n",
    "               \"p.User.Location\",\n",
    "               f.col(\"p.HashtagEntities.Text\").alias(\"Hashtags\"),\n",
    "               \"p.Text\"\n",
    "               \n",
    "           )\n",
    "          )\n",
    "\n",
    "df_step_3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8c9ab96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/06 13:58:23 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-851c178f-9dcc-4a2d-900d-6d2cb36727f6. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-------------------+-------------+-------+-----------+-------------------+---------------+------------+------------+--------------+----+--------+----------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Id                 |CreatedAt    |Country|CountryCode|UserID             |ScreenName     |RetweetCount|FriendsCount|FollowersCount|Lang|UserLang|Location              |Hashtags                                                                                                                                                                                                                                                             |Text                                                                                                                                                                                                                                                                                                                                                                                                                           |\n",
      "+-------------------+-------------+-------+-----------+-------------------+---------------+------------+------------+--------------+----+--------+----------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|1600086630160662530|1670325355000|null   |null       |737142202481016832 |chidambara09   |0           |13          |11086         |en  |null    |Mysore & 𝗕𝗘𝗥𝗟𝗶𝗡*|[laptops, Holland, bigdata, Twitter, SOciaLmediA]                                                                                                                                                                                                                    |@sonu_monika @Khulood_Almani @danfiehn @tobiaskintzel @RLDI_Lamy @BetaMoroney @JagersbergKnut @bimedotcom @PDH_Metaverse @TheAdityaPatro @asokan_telecom @itsmuzza2004 @CurieuxExplorer @Eli_Krumova @enilev @ArchimedesInte2 @EstelaMandela Cycling & r\\nRecharging #laptops at \\nthe same time, a library in Utrecht,#Holland! \\nWould you do that?\\n\\nThAnk  U \\nDr @sonu_monika \\n\\n#bigdata \\n#Twitter \\n#SOciaLmediA \\n🍁|\n",
      "|1600086661869686786|1670325363000|null   |null       |1200330399102197761|DataAugmented  |0           |11824       |11168         |en  |null    |interwebs             |[hacked, infographic, Linux, Programming, Coding, BigData, IoT, PyTorch]                                                                                                                                                                                             |RT @Stanleyhacks2: DM me for any hacking services\\nInbox me now\\n\\n#hacked\\n#infographic\\n#Linux #Programming #Coding #BigData #IoT #PyTorch #Ja…                                                                                                                                                                                                                                                                              |\n",
      "|1600086663484645379|1670325363000|null   |null       |1305786657828937728|RpatoolsC      |0           |31          |879           |en  |null    |null                  |[hacked, instagram, snapchat, whatsapp, facebook, btc]                                                                                                                                                                                                               |RT @Stanleyhacks2: Inbox me for any hacking services\\nDM me now\\nAvailable 24/7\\n\\nDm\\n#hacked #instagram #snapchat #whatsapp #facebook #btc   …                                                                                                                                                                                                                                                                               |\n",
      "|1600086673961852928|1670325365000|null   |null       |216542215          |RockStar_Grim  |0           |1528        |559           |en  |null    |Planet Earth          |[MachineLearning, DataScience, SQL, Cybersecurity, BigData, Analytics, AI, IIoT, Python, RStats, TensorFlow]                                                                                                                                                         |RT @Sheraj99: Types of VPN #MachineLearning #DataScience #SQL #Cybersecurity #BigData #Analytics #AI #IIoT #Python #RStats #TensorFlow #Jav…                                                                                                                                                                                                                                                                                   |\n",
      "|1600086709361786880|1670325374000|null   |null       |2715099702         |maxilolapaz    |0           |1750        |1010          |es  |null    |null                  |[Biomedicina, InteligenciaArtificial, MaxiloLaPaz, biotecnologia, epigenetica, bioetica, genomica, biologiamolecular, bigdata]                                                                                                                                       |🔹Francisco Torres, Pr. de #Biomedicina de la UIC: Gracias a la #InteligenciaArtificial se puede aumentar muchísimo la precisión de cómo tratar a cada paciente\\n\\n🔸https://t.co/TLUyb0C2BM\\n\\n#MaxiloLaPaz #biotecnologia #epigenetica #bioetica #genomica #biologiamolecular #bigdata https://t.co/Hwvlm1tJfy                                                                                                               |\n",
      "|1600086836915113984|1670325404000|null   |null       |1528753700415475713|Tsocolates     |0           |1287        |362           |en  |null    |null                  |[defi, cryptocurrency, DAO, blockchain, crypto, Bitcoin, Ethereum]                                                                                                                                                                                                   |RT @DomainNameGear: https://t.co/Jjp2uUSI27 sold for $5127 Afternic\\n\\n#defi #cryptocurrency #DAO #blockchain #crypto #Bitcoin  \\n#Ethereum #i…                                                                                                                                                                                                                                                                                |\n",
      "|1600086838756057088|1670325405000|null   |null       |1571881920883052545|Stanleyhacks2  |0           |536         |251           |en  |null    |United States         |[hacked, BigData, Analytics, DataScience, AI, MachineLearning, Python, TensorFlow, Java, JavaScript, ReactJS, React, Serverless, DataScientist, Programming, Coding, 100DaysofCode, DevOps, SQL, Blockchain, CyberSecurity, Flutter, Github]                         |Inbox me now for any hacking services\\nDm me\\n\\n#hacked\\n#BigData #Analytics #DataScience #AI #MachineLearning #Python #TensorFlow #Java #JavaScript #ReactJS #React #Serverless #DataScientist #Programming #Coding #100DaysofCode #DevOps #SQL #Blockchain #CyberSecurity #Flutter #Github                                                                                                                                   |\n",
      "|1600087006075408384|1670325445000|null   |null       |4839215822         |periscopeislit |0           |0           |619           |en  |null    |null                  |[DataScience, 100DaysOfCode, 5G, AI, ArtificialIntelligence, BigData]                                                                                                                                                                                                |RT @Stanleyhacks2: Inbox me for any of hacking services\\nDm me now\\n#DataScience \\n#100DaysOfCode #5G #AI #ArtificialIntelligence #BigData #Cl…                                                                                                                                                                                                                                                                                |\n",
      "|1600087247574933506|1670325502000|null   |null       |1371551211351121920|PythonRoboto   |0           |2           |3015          |en  |null    |null                  |[hacked, instagram, snapchat, whatsapp, facebook, btc]                                                                                                                                                                                                               |RT @Stanleyhacks2: Inbox me for any hacking services\\nDM me now\\nAvailable 24/7\\n\\nDm\\n#hacked #instagram #snapchat #whatsapp #facebook #btc   …                                                                                                                                                                                                                                                                               |\n",
      "|1600087353984679937|1670325528000|null   |null       |710123736175783938 |sectest9       |0           |1           |43726         |en  |null    |Hyderabad, India      |[hacked, infographic, Linux, Programming, Coding, BigData, IoT, PyTorch]                                                                                                                                                                                             |RT @Stanleyhacks2: DM me for any hacking services\\nInbox me now\\n\\n#hacked\\n#infographic\\n#Linux #Programming #Coding #BigData #IoT #PyTorch #Ja…                                                                                                                                                                                                                                                                              |\n",
      "|1600087415968104448|1670325542000|null   |null       |1142424032794406912|CyberSecurityN8|0           |2           |34756         |en  |null    |null                  |[hacked, infographic, Linux, Programming, Coding, BigData, IoT, PyTorch]                                                                                                                                                                                             |RT @Stanleyhacks2: DM me for any hacking services\\nInbox me now\\n\\n#hacked\\n#infographic\\n#Linux #Programming #Coding #BigData #IoT #PyTorch #Ja…                                                                                                                                                                                                                                                                              |\n",
      "|1600087434322071553|1670325547000|null   |null       |78680177           |Seneves        |0           |476         |305           |en  |null    |null                  |[BigData, Analytics, DataScience, IIoT, Python, Javascript, TensorFlow, CloudComputing]                                                                                                                                                                              |RT @BDAnalyticsnews: Big tecg, Big fines..😳\\n\\n#BigData #Analytics #DataScience #IIoT #Python #Javascript #TensorFlow #CloudComputing #DevCom…                                                                                                                                                                                                                                                                                |\n",
      "|1600087569269522432|1670325579000|null   |null       |22516489           |adamsconsulting|0           |45934       |140736        |en  |null    |Atlanta               |[]                                                                                                                                                                                                                                                                   |RT @grattonboy: We are often overwhelmed with new technology buzzwords, but this one has been with us for some time and is regularly misund…                                                                                                                                                                                                                                                                                   |\n",
      "|1600087656897052674|1670325600000|null   |null       |547337124          |ValueCoders    |0           |2861        |5308          |en  |null    |Globe                 |[MachineLearning, DataScience, SQL, Cybersecurity, BigData, Analytics, AI, IIoT, Python, RStats, TensorFlow, JavaScript, ReactJS, CloudComputing, Serverless, DataScientist, Linux, Programming, Coding, 100DaysofCode, NodeJS, golang, GitHub, IoT, Blockchain, AWS]|Happy learning!\\n\\n#MachineLearning #DataScience #SQL #Cybersecurity #BigData #Analytics #AI #IIoT #Python #RStats #TensorFlow #JavaScript #ReactJS #CloudComputing #Serverless #DataScientist #Linux #Programming #Coding #100DaysofCode #NodeJS #golang #GitHub #IoT #Blockchain #AWS https://t.co/8H1vRu5JxV                                                                                                                |\n",
      "|1600087657157169155|1670325600000|null   |null       |49427323           |RichardEudes   |0           |1943        |18390         |en  |null    |Paris, France         |[analytics, datascience, bigdata, datascience, datascience, ds, machinelearning]                                                                                                                                                                                     |Using Causal ML Instead of A/B Testing https://t.co/zQLtaihH52 #analytics #datascience, #bigdata, #datascience, #datascience #ds, #machinelearning                                                                                                                                                                                                                                                                             |\n",
      "|1600087730406715392|1670325617000|null   |null       |1712050710         |abhikch91      |0           |4691        |974           |en  |null    |India, Kolkata        |[AI, MachineLearning, IoT, IIoT, CX, SelfDrivingCars, SmartCity, AutonomousVehicles, DataScience]                                                                                                                                                                    |RT @WestArt_Factory: The world of Data #AI #MachineLearning #IoT #IIoT #CX #SelfDrivingCars #SmartCity #AutonomousVehicles #DataScience #Bi…                                                                                                                                                                                                                                                                                   |\n",
      "|1600087758894243841|1670325624000|null   |null       |1533209625176248326|FRCretweets    |0           |7           |3962          |en  |null    |null                  |[AI, MachineLearning, IoT, IIoT, CX, SelfDrivingCars, SmartCity, AutonomousVehicles, DataScience]                                                                                                                                                                    |RT @WestArt_Factory: The world of Data #AI #MachineLearning #IoT #IIoT #CX #SelfDrivingCars #SmartCity #AutonomousVehicles #DataScience #Bi…                                                                                                                                                                                                                                                                                   |\n",
      "|1600087857477058561|1670325648000|null   |null       |1243990473934622720|realrubberduck1|0           |46          |1178          |en  |null    |null                  |[hacked, infographic, Linux, Programming, Coding, BigData, IoT, PyTorch]                                                                                                                                                                                             |RT @Stanleyhacks2: DM me for any hacking services\\nInbox me now\\n\\n#hacked\\n#infographic\\n#Linux #Programming #Coding #BigData #IoT #PyTorch #Ja…                                                                                                                                                                                                                                                                              |\n",
      "|1600087939710701568|1670325667000|null   |null       |833999192523685889 |masentogroup   |0           |939         |121           |en  |null    |London, England       |[BigData]                                                                                                                                                                                                                                                            |Organizations can meet their agility needs with cloud automation: Here are its benefits https://t.co/peFH09Wh1N #BigData                                                                                                                                                                                                                                                                                                       |\n",
      "|1600087979002777600|1670325677000|null   |null       |1524754252316426242|and_Economist  |0           |73          |3             |es  |null    |null                  |[]                                                                                                                                                                                                                                                                   |RT @RosanaFerrero: 👀Una de las mejores publicaciones sobre análisis de datos\\n📚The Art of Data Analysis\\n👤 @rdpeng \\n🔗 https://t.co/gg4oYYuuNj…                                                                                                                                                                                                                                                                            |\n",
      "+-------------------+-------------+-------+-----------+-------------------+---------------+------------+------------+--------------+----+--------+----------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-------------------+-------------+-------+-----------+------------------+---------------+------------+------------+--------------+----+--------+------------+----------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Id                 |CreatedAt    |Country|CountryCode|UserID            |ScreenName     |RetweetCount|FriendsCount|FollowersCount|Lang|UserLang|Location    |Hashtags                                                                                                  |Text                                                                                                                                                                                                                                                                                    |\n",
      "+-------------------+-------------+-------+-----------+------------------+---------------+------------+------------+--------------+----+--------+------------+----------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|1600127517272293383|1670335103000|null   |null       |1309582843        |cuongcz        |0           |234         |462           |en  |null    |New York, NY|[ai, ml, artificialintelligence, machinelearning, datascience, bigdata, analytics, blockchain, tech, data]|#ai #ml #artificialintelligence #machinelearning #datascience #bigdata #analytics #blockchain #tech #data @Nicochan33 @TrippBraden @Paula_Piccard @haroldsinnott @sallyeaves\\n16 Tech Leaders’ Tips For Managing The Day-To-Day Work Grind https://t.co/fsgJRkzgtw                      |\n",
      "|1600127522544443393|1670335104000|null   |null       |1309582843        |cuongcz        |0           |234         |462           |en  |null    |New York, NY|[ai, ml, artificialintelligence, machinelearning, datascience, bigdata, analytics, blockchain, tech, data]|#ai #ml #artificialintelligence #machinelearning #datascience #bigdata #analytics #blockchain #tech #data @Nicochan33 @TrippBraden @Paula_Piccard @haroldsinnott @sallyeaves\\nEasy And Cost-Effective Ways To Make Your Tech Business Carbon-Neutral https://t.co/932yO1vPdE            |\n",
      "|1600127528173293569|1670335106000|null   |null       |1309582843        |cuongcz        |0           |234         |462           |en  |null    |New York, NY|[ai, ml, artificialintelligence, machinelearning, datascience, bigdata, analytics, blockchain, tech, data]|#ai #ml #artificialintelligence #machinelearning #datascience #bigdata #analytics #blockchain #tech #data @Nicochan33 @TrippBraden @Paula_Piccard @haroldsinnott @sallyeaves\\nTeaming Up Two Biotech Winners to Fight Cancer: CRISPR and CAR T https://t.co/fFf2FL3XH5                  |\n",
      "|1600127536415006720|1670335108000|null   |null       |1309582843        |cuongcz        |0           |234         |462           |en  |null    |New York, NY|[ai, ml, artificialintelligence, machinelearning, datascience, bigdata, analytics, blockchain, tech, data]|#ai #ml #artificialintelligence #machinelearning #datascience #bigdata #analytics #blockchain #tech #data @Nicochan33 @TrippBraden @Paula_Piccard @haroldsinnott @sallyeaves\\nThe Best ‘Marvel Snap’ Silver Surfer Deck For The New Season https://t.co/8eUbml58HY                      |\n",
      "|1600127537266368512|1670335108000|null   |null       |952148228673343488|SupplyChain2030|0           |7197        |6552          |en  |null    |null        |[DataScience, BigData, Analytics, innovation, digital]                                                    |RT @SupplyChain2030: Brief History of #DataScience \\n\\nv/ @BBNTimes_en | @Khulood_Almani \\n \\n#BigData #Analytics #innovation #digital #technol…                                                                                                                                        |\n",
      "|1600127514776686594|1670335103000|null   |null       |1309582843        |cuongcz        |0           |234         |462           |en  |null    |New York, NY|[ai, ml, artificialintelligence, machinelearning, datascience, bigdata, analytics, blockchain, tech, data]|#ai #ml #artificialintelligence #machinelearning #datascience #bigdata #analytics #blockchain #tech #data @Nicochan33 @TrippBraden @Paula_Piccard @haroldsinnott @sallyeaves\\nAndroid And IOS Hacks: How To Detect Them And Protect Your Smartphones https://t.co/fAiK1B2bhn            |\n",
      "|1600127520057303040|1670335104000|null   |null       |1309582843        |cuongcz        |0           |234         |462           |en  |null    |New York, NY|[ai, ml, artificialintelligence, machinelearning, datascience, bigdata, analytics, blockchain, tech, data]|#ai #ml #artificialintelligence #machinelearning #datascience #bigdata #analytics #blockchain #tech #data @Nicochan33 @TrippBraden @Paula_Piccard @haroldsinnott @sallyeaves\\nCarputty Wins Investor Millions To Dull Auto Financing Pain Points https://t.co/HU3M7Sv1xI                |\n",
      "|1600127526239883265|1670335105000|null   |null       |1309582843        |cuongcz        |0           |234         |462           |en  |null    |New York, NY|[ai, ml, artificialintelligence, machinelearning, datascience, bigdata, analytics, blockchain, tech, data]|#ai #ml #artificialintelligence #machinelearning #datascience #bigdata #analytics #blockchain #tech #data @Nicochan33 @TrippBraden @Paula_Piccard @haroldsinnott @sallyeaves\\nItaly’s Far-Right Transport Minister Urges EU To Halt 2035 Ban On Fossil Fuel Car… https://t.co/DHlPMcwsUL|\n",
      "|1600127532187140097|1670335107000|null   |null       |1309582843        |cuongcz        |0           |234         |462           |en  |null    |New York, NY|[ai, ml, artificialintelligence, machinelearning, datascience, bigdata, analytics, blockchain, tech, data]|#ai #ml #artificialintelligence #machinelearning #datascience #bigdata #analytics #blockchain #tech #data @Nicochan33 @TrippBraden @Paula_Piccard @haroldsinnott @sallyeaves\\nAI Offers Fintech Much More Than Curb Appeal https://t.co/nWxZWf0vYE                                      |\n",
      "+-------------------+-------------+-------+-----------+------------------+---------------+------------+------------+--------------+----+--------+------------+----------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+-------------------+-------------+-------+-----------+------------------+---------------+------------+------------+--------------+----+--------+--------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Id                 |CreatedAt    |Country|CountryCode|UserID            |ScreenName     |RetweetCount|FriendsCount|FollowersCount|Lang|UserLang|Location|Hashtags                                                                                     |Text                                                                                                                                            |\n",
      "+-------------------+-------------+-------+-----------+------------------+---------------+------------+------------+--------------+----+--------+--------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|1600127588235501568|1670335120000|null   |null       |952148228673343488|SupplyChain2030|0           |7197        |6552          |en  |null    |null    |[DataScience, BigData, Analytics, AI, MachineLearning, IoT, Python, RStats, TensorFlow, Java]|RT @SupplyChain2030: Graphic on #DataScience Landscape \\n\\n#BigData #Analytics #AI #MachineLearning #IoT #Python #RStats #TensorFlow #Java #J…  |\n",
      "|1600127580115533825|1670335118000|null   |null       |952148228673343488|SupplyChain2030|0           |7195        |6552          |en  |null    |null    |[DataScience, BigData, Analytics, innovation, digital]                                       |RT @digitalhealthxx: Brief History of #DataScience \\n\\nv/ @BBNTimes_en | @Khulood_Almani \\n \\n#BigData #Analytics #innovation #digital #technol…|\n",
      "+-------------------+-------------+-------+-----------+------------------+---------------+------------+------------+--------------+----+--------+--------+---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+-------------------+-------------+-------+-----------+------------------+---------------+------------+------------+--------------+----+--------+--------+--------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Id                 |CreatedAt    |Country|CountryCode|UserID            |ScreenName     |RetweetCount|FriendsCount|FollowersCount|Lang|UserLang|Location|Hashtags                                    |Text                                                                                                                                          |\n",
      "+-------------------+-------------+-------+-----------+------------------+---------------+------------+------------+--------------+----+--------+--------+--------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|1600127628223971328|1670335130000|null   |null       |952148228673343488|SupplyChain2030|0           |7197        |6552          |en  |null    |null    |[Procurement, SupplyChain, innovation, tech]|RT @mvollmer1: Hype Cycle for Procurement and Sourcing Solutions 2022 by @Gartner_inc \\n\\n#Procurement #SupplyChain #innovation #tech #Digita…|\n",
      "+-------------------+-------------+-------+-----------+------------------+---------------+------------+------------+--------------+----+--------+--------+--------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+-------------------+-------------+-------+-----------+----------+----------+------------+------------+--------------+----+--------+---------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Id                 |CreatedAt    |Country|CountryCode|UserID    |ScreenName|RetweetCount|FriendsCount|FollowersCount|Lang|UserLang|Location       |Hashtags                                                                                                                                                                                                                                                                                 |Text                                                                                                                                                                                                                                                                                                            |\n",
      "+-------------------+-------------+-------+-----------+----------+----------+------------+------------+--------------+----+--------+---------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|1600127721421406211|1670335152000|null   |null       |2984376936|BeingOvee |0           |2043        |1734          |qme |null    |London, England|[designthinking, creativity, HumanResources, BigData, Analytics, DataScience, IoT, Python, Javascript, TensorFlow, CloudComputing, DevCommunity, digitalhealth, Serverless, DataScientist, Coding, DigitalTransformation, Cloud, MachineLearning, AI, ArtificialIntelligence, technology]|#designthinking #creativity #HumanResources #BigData #Analytics #DataScience #IoT #Python #Javascript #TensorFlow #CloudComputing #DevCommunity #digitalhealth #Serverless #DataScientist #Coding #DigitalTransformation #Cloud #MachineLearning #AI #ArtificialIntelligence #technology https://t.co/YcoMAIS5cT|\n",
      "+-------------------+-------------+-------+-----------+----------+----------+------------+------------+--------------+----+--------+---------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 5\n",
      "-------------------------------------------\n",
      "+-------------------+-------------+-------+-----------+------------------+---------------+------------+------------+--------------+----+--------+--------+----------------+-------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Id                 |CreatedAt    |Country|CountryCode|UserID            |ScreenName     |RetweetCount|FriendsCount|FollowersCount|Lang|UserLang|Location|Hashtags        |Text                                                                                                                                             |\n",
      "+-------------------+-------------+-------+-----------+------------------+---------------+------------+------------+--------------+----+--------+--------+----------------+-------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|1600127784365395968|1670335167000|null   |null       |952148228673343488|SupplyChain2030|0           |7197        |6552          |en  |null    |null    |[exchangestrong]|RT @michaelbathurst: #exchangestrong via NodeXL https://t.co/wPkuG8RhwD\\n@michaelbathurst\\n@melanieluckey\\n@this0499154500\\n@chidambara09\\n@exch…|\n",
      "+-------------------+-------------+-------+-----------+------------------+---------------+------------+------------+--------------+----+--------+--------+----------------+-------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 6\n",
      "-------------------------------------------\n",
      "+-------------------+-------------+-------+-----------+--------+-------------+------------+------------+--------------+----+--------+----------------+----------------+-----------------------------------------------------------------------------------------+\n",
      "|Id                 |CreatedAt    |Country|CountryCode|UserID  |ScreenName   |RetweetCount|FriendsCount|FollowersCount|Lang|UserLang|Location        |Hashtags        |Text                                                                                     |\n",
      "+-------------------+-------------+-------+-----------+--------+-------------+------------+------------+--------------+----+--------+----------------+----------------+-----------------------------------------------------------------------------------------+\n",
      "|1600127789545521154|1670335168000|null   |null       |29717423|lukaspfeiffer|0           |771         |1066          |de  |null    |May Contain Bots|[cloud, bigdata]|The @lukaspfeiffer Daily wurde soeben publiziert! https://t.co/6THTP9xamr #cloud #bigdata|\n",
      "+-------------------+-------------+-------+-----------+--------+-------------+------------+------------+--------------+----+--------+----------------+----------------+-----------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/lib/python3.9/socket.py\", line 704, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m stream_query_debug\u001b[38;5;241m=\u001b[39m(\u001b[43mdf_step_3\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriteStream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconsole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtruncate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfalse\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutputMode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mappend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m                    )\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/streaming.py:107\u001b[0m, in \u001b[0;36mStreamingQuery.awaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsq\u001b[38;5;241m.\u001b[39mawaitTermination(\u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m))\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/py4j/java_gateway.py:1320\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1313\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.9/socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 704\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    706\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "stream_query_debug=(df_step_3\n",
    "                    .writeStream.format(\"console\")\n",
    "                    .option(\"truncate\", \"false\")\n",
    "                    .outputMode(\"append\")\n",
    "                    .start()\n",
    "                    .awaitTermination()\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef219e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.streams.active[0].stop()\n",
    "#stream_query_debug.stop()\n",
    "\n",
    "df_step_4=df_step_3.select(f.to_json(f.struct(df_step_3.schema.names)).alias('value')) \n",
    "\n",
    "df_step_4.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3cbdd7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: binary (nullable = false)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_step_5=df_step_3.select(to_avro(f.struct(df_step_3.schema.names)).alias('value')) \n",
    "\n",
    "df_step_5.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f1acf7d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.                             \n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/lib/python3.9/socket.py\", line 704, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m kafka_write\u001b[38;5;241m=\u001b[39m(\u001b[43mdf_step_4\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m             \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriteStream\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m             \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkafka\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m             \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutputMode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mappend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m             \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkafka.bootstrap.servers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkafka-cp-kafka.kafka.svc.cluster.local:9092\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m             \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtopic\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark-target-schema-avro\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m             \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcheckpointLocation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/opt/spark/work-dir/12\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m             \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m             \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m             )\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# topic\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# spark-target-schema2 --> json mit struct f.to_json(f.struct(df_step_4.schema.names)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# spark-target-schema3 --> read from twitter-json mit orginal Schema und struct f.to_json(f.struct(df_step_4.schema.names)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# spark-target-schema-avro --> read from twitter-json mit orginal Schema und struct f.to_avro(f.struct(df_step_4.schema.names)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/streaming.py:107\u001b[0m, in \u001b[0;36mStreamingQuery.awaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsq\u001b[38;5;241m.\u001b[39mawaitTermination(\u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m))\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/py4j/java_gateway.py:1320\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1313\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.9/socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 704\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    706\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "kafka_write=(df_step_4\n",
    "             .writeStream\n",
    "             .format(\"kafka\")\n",
    "             .outputMode(\"append\")\n",
    "             .option(\"kafka.bootstrap.servers\", \"kafka-cp-kafka.kafka.svc.cluster.local:9092\")\n",
    "             .option(\"topic\", \"spark-target-schema-avro\")\n",
    "             .option(\"checkpointLocation\", \"/opt/spark/work-dir/12\")\n",
    "             .start()\n",
    "             .awaitTermination()\n",
    "            )\n",
    "\n",
    "# topic\n",
    "# spark-target-schema2 --> json mit struct f.to_json(f.struct(df_step_4.schema.names)\n",
    "# spark-target-schema3 --> read from twitter-json mit orginal Schema und struct f.to_json(f.struct(df_step_4.schema.names)\n",
    "# spark-target-schema-avro --> read from twitter-json mit orginal Schema und struct f.to_avro(f.struct(df_step_4.schema.names)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2995a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_write_json=(df_step_4\n",
    "          .writeStream\n",
    "          .format(\"json\")\n",
    "          .outputMode(\"append\")\n",
    "          .option(\"path\", \"s3a://spark-stream-to-s3/json\")\n",
    "          .option(\"checkpointLocation\", \"/opt/spark/work-dir/5\")\n",
    "          .trigger(processingTime='10 seconds')\n",
    "          .start()\n",
    "          .awaitTermination()\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119c7409",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_write_avro=(df_step_4\n",
    "          .writeStream\n",
    "          .format(\"avro\")\n",
    "          .outputMode(\"append\")\n",
    "          .option(\"path\", \"s3a://spark-stream-to-s3/avro\")\n",
    "          .option(\"checkpointLocation\", \"/opt/spark/work-dir/6\")\n",
    "          .start()\n",
    "          .awaitTermination()\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f03dcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_write_parquet=(df_step_4\n",
    "          .writeStream\n",
    "          .format(\"parquet\")\n",
    "          .outputMode(\"append\")\n",
    "          .option(\"path\", \"s3a://spark-stream-to-s3/parquet\")\n",
    "          .option(\"checkpointLocation\", \"/opt/spark/work-dir/7\")\n",
    "          .start()\n",
    "          .awaitTermination()\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120b7f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pyspark --version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "76213f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/01 23:58:57 WARN ExecutorPodsWatchSnapshotSource: Kubernetes client has been closed.\n",
      "22/12/01 23:59:50 ERROR MicroBatchExecution: Query [id = 49bbe66e-4bcf-41ea-b086-f931e24ac560, runId = b2491bba-b795-44f4-9c83-eb6f118c595a] terminated with error\n",
      "org.apache.spark.SparkException: The Spark SQL phase planning failed with an internal error. Please, fill a bug report in, and provide the full stack trace.\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.toInternalError(QueryExecution.scala:500)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:512)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:145)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:138)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executedPlan$1(QueryExecution.scala:158)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:158)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:151)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$15(MicroBatchExecution.scala:657)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:647)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:219)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:213)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)\n",
      "Caused by: java.lang.NullPointerException\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderConsumer.getSortedExecutorList(KafkaOffsetReaderConsumer.scala:484)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderConsumer.getOffsetRangesFromResolvedOffsets(KafkaOffsetReaderConsumer.scala:539)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.planInputPartitions(KafkaMicroBatchStream.scala:200)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.MicroBatchScanExec.inputPartitions$lzycompute(MicroBatchScanExec.scala:45)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.MicroBatchScanExec.inputPartitions(MicroBatchScanExec.scala:45)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanExecBase.supportsColumnar(DataSourceV2ScanExecBase.scala:142)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanExecBase.supportsColumnar$(DataSourceV2ScanExecBase.scala:141)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.MicroBatchScanExec.supportsColumnar(MicroBatchScanExec.scala:29)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2Strategy.apply(DataSourceV2Strategy.scala:153)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$1(QueryPlanner.scala:63)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n",
      "\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:69)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n",
      "\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:69)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n",
      "\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:69)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.createSparkPlan(QueryExecution.scala:459)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$sparkPlan$1(QueryExecution.scala:145)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n",
      "\t... 32 more\n",
      "22/12/01 23:59:50 ERROR MicroBatchExecution: Query [id = 648128cd-8500-41d9-a2dc-3c43ef2e77e5, runId = e8a4cd1e-5028-401c-972b-de0f85d71d41] terminated with error\n",
      "org.apache.spark.SparkException: The Spark SQL phase planning failed with an internal error. Please, fill a bug report in, and provide the full stack trace.\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.toInternalError(QueryExecution.scala:500)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:512)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:145)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:138)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executedPlan$1(QueryExecution.scala:158)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:158)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:151)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$15(MicroBatchExecution.scala:657)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:647)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:219)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:213)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)\n",
      "Caused by: java.lang.NullPointerException\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderConsumer.getSortedExecutorList(KafkaOffsetReaderConsumer.scala:484)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderConsumer.getOffsetRangesFromResolvedOffsets(KafkaOffsetReaderConsumer.scala:539)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.planInputPartitions(KafkaMicroBatchStream.scala:200)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.MicroBatchScanExec.inputPartitions$lzycompute(MicroBatchScanExec.scala:45)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.MicroBatchScanExec.inputPartitions(MicroBatchScanExec.scala:45)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanExecBase.supportsColumnar(DataSourceV2ScanExecBase.scala:142)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanExecBase.supportsColumnar$(DataSourceV2ScanExecBase.scala:141)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.MicroBatchScanExec.supportsColumnar(MicroBatchScanExec.scala:29)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2Strategy.apply(DataSourceV2Strategy.scala:153)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$1(QueryPlanner.scala:63)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n",
      "\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:69)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n",
      "\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:69)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n",
      "\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:69)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.createSparkPlan(QueryExecution.scala:459)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$sparkPlan$1(QueryExecution.scala:145)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n",
      "\t... 32 more\n",
      "22/12/01 23:59:50 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@7a037ffa[Not completed, task = java.util.concurrent.Executors$RunnableAdapter@65617a0e[Wrapped task = org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1@198d23a7]] rejected from java.util.concurrent.ScheduledThreadPoolExecutor@4aa45bab[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]\n",
      "22/12/01 23:59:50 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@5a4f3a13[Not completed, task = java.util.concurrent.Executors$RunnableAdapter@71c9c68b[Wrapped task = org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1@3e05f71]] rejected from java.util.concurrent.ScheduledThreadPoolExecutor@4aa45bab[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread \"stream execution thread for [id = 49bbe66e-4bcf-41ea-b086-f931e24ac560, runId = b2491bba-b795-44f4-9c83-eb6f118c595a]\" org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef.deactivateInstances(StateStoreCoordinator.scala:119)\n",
      "\tat org.apache.spark.sql.streaming.StreamingQueryManager.notifyQueryTermination(StreamingQueryManager.scala:406)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$3(StreamExecution.scala:357)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:338)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)\n",
      "Caused by: org.apache.spark.rpc.RpcEnvStoppedException: RpcEnv already stopped.\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:176)\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:144)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.askAbortable(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.askAbortable(NettyRpcEnv.scala:554)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:558)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:102)\n",
      "\t... 8 more\n",
      "Exception in thread \"stream execution thread for [id = 648128cd-8500-41d9-a2dc-3c43ef2e77e5, runId = e8a4cd1e-5028-401c-972b-de0f85d71d41]\" org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef.deactivateInstances(StateStoreCoordinator.scala:119)\n",
      "\tat org.apache.spark.sql.streaming.StreamingQueryManager.notifyQueryTermination(StreamingQueryManager.scala:406)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$3(StreamExecution.scala:357)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:338)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)\n",
      "Caused by: org.apache.spark.rpc.RpcEnvStoppedException: RpcEnv already stopped.\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:176)\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:144)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.askAbortable(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.askAbortable(NettyRpcEnv.scala:554)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:558)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:102)\n",
      "\t... 8 more\n"
     ]
    }
   ],
   "source": [
    "# Terminate Spark Session\n",
    "# shut down executor pods\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b4590d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
