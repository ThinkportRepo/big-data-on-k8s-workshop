{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5765a06e",
   "metadata": {},
   "source": [
    "# Spark Aufgaben\n",
    "\n",
    "1. Read Twitter Streams from Avro\n",
    "2. Do analysis Tasks\n",
    "3. Write Result to Delta several times\n",
    "4. Du some History analysis on it\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "eebe3ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25314/3232487455.py:18: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container {width:100% !important; }<style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import Row\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "from delta import *\n",
    "\n",
    "\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "\n",
    "# use 95% of the screen for jupyter cell\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container {width:100% !important; }<style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a6d15c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.kubernetes.namespace = frontend\n",
      "spark.master = k8s://https://kubernetes.default.svc.cluster.local:443\n",
      "spark.app.name = jupyter-spark\n",
      "spark.executor.memory = 1G\n",
      "spark.executor.cores = 2\n",
      "spark.driver.host = jupyter-spark-driver.frontend.svc.cluster.local\n"
     ]
    }
   ],
   "source": [
    "appName=\"jupyter-spark\"\n",
    "\n",
    "conf = SparkConf()\n",
    "\n",
    "# CLUSTER MANAGER\n",
    "################################################################################\n",
    "# set Kubernetes Master as Cluster Manager(“k8s://https://” is NOT a typo, this is how Spark knows the “provider” type).\n",
    "conf.setMaster(\"k8s://https://kubernetes.default.svc.cluster.local:443\")\n",
    "\n",
    "# CONFIGURE KUBERNETES\n",
    "################################################################################\n",
    "# set the namespace that will be used for running the driver and executor pods.\n",
    "conf.set(\"spark.kubernetes.namespace\",\"frontend\")\n",
    "# set the docker image from which the Worker pods are created\n",
    "conf.set(\"spark.kubernetes.container.image\", \"thinkportgmbh/workshops:spark-3.3.1\")\n",
    "conf.set(\"spark.kubernetes.container.image.pullPolicy\", \"Always\")\n",
    "\n",
    "# set service account to be used\n",
    "conf.set(\"spark.kubernetes.authenticate.driver.serviceAccountName\", \"spark\")\n",
    "# authentication for service account(required to create worker pods):\n",
    "conf.set(\"spark.kubernetes.authenticate.caCertFile\", \"/var/run/secrets/kubernetes.io/serviceaccount/ca.crt\")\n",
    "conf.set(\"spark.kubernetes.authenticate.oauthTokenFile\", \"/var/run/secrets/kubernetes.io/serviceaccount/token\")\n",
    "\n",
    "\n",
    "# CONFIGURE SPARK\n",
    "################################################################################\n",
    "conf.set(\"spark.sql.session.timeZone\", \"Europe/Berlin\")\n",
    "# set driver host. In this case the ingres service for the spark driver\n",
    "# find name of the driver service with 'kubectl get services' or in the helm chart configuration\n",
    "conf.set(\"spark.driver.host\", \"jupyter-spark-driver.frontend.svc.cluster.local\")\n",
    "# set the port, If this port is busy, spark-shell tries to bind to another port.\n",
    "conf.set(\"spark.driver.port\", \"29413\")\n",
    "# add the postgres driver jars into session\n",
    "conf.set(\"spark.jars\", \"/opt/spark/jars/spark-sql-kafka-0-10_2.12-3.3.1.jar, /opt/spark/jars/kafka-clients-3.3.1.jar, /opt/spark/jars/spark-avro_2.12-3.3.1.jar\")\n",
    "#conf.set(\"spark.driver.extraClassPath\",\"/opt/spark/jars/spark-sql-kafka-0-10_2.12-3.3.1.jar, /opt/spark/jars/kafka-clients-3.3.1.jar, /opt/spark/jars/spark-avro_2.12-3.3.1.jar\")\n",
    "conf.set(\"spark.executor.extraClassPath\",\"/opt/spark/jars/spark-sql-kafka-0-10_2.12-3.3.1.jar, /opt/spark/jars/kafka-clients-3.3.1.jar, /opt/spark/jars/spark-avro_2.12-3.3.1.jar\")\n",
    "#conf.set(\"spark.executor.extraLibrary\",\"/opt/spark/jars/spark-sql-kafka-0-10_2.12-3.3.1.jar, /opt/spark/jars/kafka-clients-3.3.1.jar\")\n",
    "\n",
    "# CONFIGURE S3 CONNECTOR\n",
    "conf.set(\"spark.hadoop.fs.s3a.endpoint\", \"minio.minio.svc.cluster.local:9000\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.access.key\", \"trainadm\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.secret.key\", \"train@thinkport\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "conf.set(\"spark.hadoop.fs.s3.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "\n",
    "# conf.set(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.1\")\n",
    "\n",
    "# CONFIGURE WORKER (Customize based on workload)\n",
    "################################################################################\n",
    "# set number of worker pods\n",
    "conf.set(\"spark.executor.instances\", \"1\")\n",
    "# set memory of each worker pod\n",
    "conf.set(\"spark.executor.memory\", \"1G\")\n",
    "# set cpu of each worker pod\n",
    "conf.set(\"spark.executor.cores\", \"2\")\n",
    "# Number of possible tasks = cores * executores\n",
    "\n",
    "# SPARK SESSION\n",
    "################################################################################\n",
    "# and last, create the spark session and pass it the config object\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .config(conf=conf) \\\n",
    "    .config('spark.sql.session.timeZone', 'Europe/Berlin') \\\n",
    "    .appName(appName)\\\n",
    "    .getOrCreate()\n",
    "\n",
    "# also get the spark context\n",
    "sc=spark.sparkContext\n",
    "# change the log level to warning, to see less output\n",
    "sc.setLogLevel('WARN')\n",
    "\n",
    "# get the configuration object to check all the configurations the session was startet with\n",
    "for entry in sc.getConf().getAll():\n",
    "        if entry[0] in [\"spark.app.name\",\"spark.kubernetes.namespace\",\"spark.executor.memory\",\"spark.executor.cores\",\"spark.driver.host\",\"spark.master\"]:\n",
    "            print(entry[0],\"=\",entry[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "90b65543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CreatedAt: long (nullable = true)\n",
      " |-- FollowersCount: long (nullable = true)\n",
      " |-- FriendsCount: long (nullable = true)\n",
      " |-- Hashtags: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- Id: long (nullable = true)\n",
      " |-- Lang: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- RetweetCount: long (nullable = true)\n",
      " |-- ScreenName: string (nullable = true)\n",
      " |-- Text: string (nullable = true)\n",
      " |-- UserID: long (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      "\n",
      "70\n"
     ]
    }
   ],
   "source": [
    "df=spark.read.format(\"json\").load(\"s3a://spark-target-schema3/topics/spark-target-schema3/\")\n",
    "df.printSchema()\n",
    "print(df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "de995980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.show of DataFrame[CreatedAt: string, Hashtags: array<string>]>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.withColumn(\"CreatedAt\",f.from_unixtime(f.col(\"CreatedAt\")/1000)).select(\"CreatedAt\",\"Hashtags\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "990cbeff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+--------------+\n",
      "|      tweet_created|           tweet_id|tweet_language|\n",
      "+-------------------+-------------------+--------------+\n",
      "|2022-11-29 12:15:55|1597549913642090496|            en|\n",
      "|2022-11-29 12:15:55|1597549916074790913|            en|\n",
      "|2022-11-29 12:15:56|1597549917639245824|            en|\n",
      "|2022-11-29 12:15:56|1597549918113402880|            en|\n",
      "|2022-11-29 12:15:56|1597549920029900802|            en|\n",
      "|2022-11-29 12:15:58|1597549927374127105|            en|\n",
      "|2022-11-29 12:15:58|1597549928733097984|            en|\n",
      "|2022-11-29 12:15:59|1597549930490601472|            en|\n",
      "|2022-11-29 12:15:59|1597549931857940482|            en|\n",
      "|2022-11-29 12:16:22|1597550030130388992|            en|\n",
      "|2022-11-29 12:16:24|1597550037373943809|            en|\n",
      "|2022-11-29 12:16:34|1597550080382599169|            en|\n",
      "|2022-11-29 12:16:53|1597550157830455296|            en|\n",
      "|2022-11-29 12:18:35|1597550584345030657|            es|\n",
      "|2022-11-29 12:19:37|1597550846039973888|            en|\n",
      "|2022-11-29 12:19:47|1597550886683037696|            en|\n",
      "|2022-11-29 12:20:02|1597550952030011392|            en|\n",
      "|2022-11-29 12:20:36|1597551092862418947|            en|\n",
      "|2022-11-29 12:20:37|1597551098835136512|            en|\n",
      "|2022-11-29 12:20:58|1597551187393495041|            en|\n",
      "+-------------------+-------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"avro\").load(\"s3a://spark-stream-to-s3/avro\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "78ccd5b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+--------------+--------------------+-------+------------+---------------+-------------+--------------------+--------------+-------------+--------------------+\n",
      "|      tweet_created|           tweet_id|tweet_language|             hashtag|country|country_code|           user|user_language|       user_location|statuses_count|retweet_count|          tweet_text|\n",
      "+-------------------+-------------------+--------------+--------------------+-------+------------+---------------+-------------+--------------------+--------------+-------------+--------------------+\n",
      "|2022-11-29 12:15:56|1597549921485332480|            en|[ai, ml, artifici...|   null|        null|        cuongcz|         null|        New York, NY|         37313|            0|#ai #ml #artifici...|\n",
      "|2022-11-29 12:15:57|1597549922915590144|            en|[ai, ml, artifici...|   null|        null|        cuongcz|         null|        New York, NY|         37314|            0|#ai #ml #artifici...|\n",
      "|2022-11-29 12:15:57|1597549924597612545|            en|[ai, ml, artifici...|   null|        null|        cuongcz|         null|        New York, NY|         37315|            0|#ai #ml #artifici...|\n",
      "|2022-11-29 12:15:58|1597549926044639232|            en|[ai, ml, artifici...|   null|        null|        cuongcz|         null|        New York, NY|         37316|            0|#ai #ml #artifici...|\n",
      "|2022-11-29 12:15:59|1597549933174968321|            en|[ai, ml, artifici...|   null|        null|        cuongcz|         null|        New York, NY|         37321|            0|#ai #ml #artifici...|\n",
      "|2022-11-29 12:18:40|1597550607686336513|            en|    [SymbolicAI, AI]|   null|        null|   Tabularasaqu|         null|                null|          1877|            0|RT @KirkDBorne: #...|\n",
      "|2022-11-29 12:18:49|1597550643190849538|            en|[AIart, deeplearn...|   null|        null|realrubberduck1|         null|                null|         88610|            0|RT @Mlearning_ai:...|\n",
      "|2022-11-29 12:19:24|1597550790045929472|            en|[AIart, deeplearn...|   null|        null|     vegasazure|         null|       Las Vegas, NV|         59418|            0|RT @Mlearning_ai:...|\n",
      "|2022-11-29 12:19:25|1597550797163667456|            en|[analytics, datas...|   null|        null|        Pvalsfr|         null|Nord-Pas-de-Calai...|        557992|            0|RT @RichardEudes:...|\n",
      "|2022-11-29 12:20:00|1597550943599484933|            en|[analytics, datas...|   null|        null|   RichardEudes|         null|       Paris, France|         55278|            0|N-BEATS: Time-Ser...|\n",
      "|2022-11-29 12:20:45|1597551131265503233|            en|[BigData, Enterpr...|   null|        null| Khulood_Almani|         null|Kingdom of Saudi ...|         70728|            0|RT @CorporateNimb...|\n",
      "|2022-11-29 12:20:58|1597551187330490368|            es|              [null]|   null|        null|o_fabriciosouza|         null|      Rio de janeiro|       1106438|            0|RT @campusbigdata...|\n",
      "|2022-11-29 12:20:58|1597551187364122624|            en|[BigData, Enterpr...|   null|        null|o_fabriciosouza|         null|      Rio de janeiro|       1106438|            0|RT @Khulood_Alman...|\n",
      "|2022-11-29 12:20:58|1597551187359842309|            en|[AIart, deeplearn...|   null|        null|o_fabriciosouza|         null|      Rio de janeiro|       1106438|            0|RT @Mlearning_ai:...|\n",
      "|2022-11-29 12:20:58|1597551187406082051|            en|[analytics, datas...|   null|        null|o_fabriciosouza|         null|      Rio de janeiro|       1106438|            0|RT @RichardEudes:...|\n",
      "|2022-11-29 12:20:58|1597551187406069763|            en|[BigData, Analyti...|   null|        null|o_fabriciosouza|         null|      Rio de janeiro|       1106438|            0|RT @ginacostag_: ...|\n",
      "|2022-11-29 12:20:58|1597551187410255872|            en|    [SymbolicAI, AI]|   null|        null|o_fabriciosouza|         null|      Rio de janeiro|       1106438|            0|RT @KirkDBorne: #...|\n",
      "|2022-11-29 12:20:58|1597551187351535622|            en|[BigData, Enterpr...|   null|        null|o_fabriciosouza|         null|      Rio de janeiro|       1106438|            0|RT @CorporateNimb...|\n",
      "|2022-11-29 12:21:05|1597551216879407109|            en|[AI, ML, DL, data...|   null|        null|  mylesgilsenan|         null|     Short Hills, NJ|          5286|            0|Breaking the scal...|\n",
      "|2022-11-29 12:21:16|1597551261649444864|            en|[Metaverse, Block...|   null|        null|  PDH_Metaverse|         null|                 UK |        147454|            0|Breaking #Metaver...|\n",
      "+-------------------+-------------------+--------------+--------------------+-------+------------+---------------+-------------+--------------------+--------------+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"parquet\").load(\"s3a://spark-stream-to-s3/parquet\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "414b9e0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndf = (spark\\n      .read\\n      .json(\"s3a://json-500/topics/twitter-table\")\\n      .withColumnRenamed(\"hashtag\",\"hashtags\")\\n     ).cache()\\n'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "df = (spark\n",
    "      .read\n",
    "      .json(\"s3a://json-500/topics/twitter-table\")\n",
    "      .withColumnRenamed(\"hashtag\",\"hashtags\")\n",
    "     ).cache()\n",
    "\"\"\"      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "7b714a0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndf.select(\"hashtags\").show(2,truncate=False)\\nprint(df.columns)\\ndf.count()\\n'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "df.select(\"hashtags\").show(2,truncate=False)\n",
    "print(df.columns)\n",
    "df.count()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "475fc25f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndf_avro = df = spark.read.format(\"avro\").load(\"s3a://arvo-100/topics/twitter-avroConverter\").cache()\\n'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "df_avro = df = spark.read.format(\"avro\").load(\"s3a://arvo-100/topics/twitter-avroConverter\").cache()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "b5f80d2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndf_avro.select(\"CreatedAt\",\"Id\").show(2,truncate=False)\\ndf_avro.columns\\n'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "df_avro.select(\"CreatedAt\",\"Id\").show(2,truncate=False)\n",
    "df_avro.columns\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518be64b",
   "metadata": {},
   "source": [
    "## Aufgaben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "8588d58a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CreatedAt: long (nullable = true)\n",
      " |-- FollowersCount: long (nullable = true)\n",
      " |-- FriendsCount: long (nullable = true)\n",
      " |-- Hashtags: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- Id: long (nullable = true)\n",
      " |-- Lang: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- RetweetCount: long (nullable = true)\n",
      " |-- ScreenName: string (nullable = true)\n",
      " |-- Text: string (nullable = true)\n",
      " |-- UserID: long (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "3af88657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(CreatedAt=1669938163000, FollowersCount=4813, FriendsCount=84, Hashtags=['MachineLearning', 'BigData', 'Analytics', 'DataScience', 'AI', 'IoT', 'IIoT', 'Python', 'RStats', 'TensorFlow', 'JavaScript', 'ReactJS', 'CloudComputing', 'Serverless', 'Linux', 'Programming', 'Coding', '100DaysofCode', 'blockchain', 'AWS', 'SQL', 'NLP', 'CodeNewbies', 'DL'], Id=1598462628237807616, Lang='en', Location='Toronto, Ontario', RetweetCount=0, ScreenName='Sheraj99', Text='Scikit-learn for #MachineLearning Cheatsheet #BigData #Analytics #DataScience #AI #IoT #IIoT #Python #RStats #TensorFlow #JavaScript #ReactJS #CloudComputing #Serverless #Linux #Programming #Coding #100DaysofCode #blockchain #AWS #SQL #NLP #CodeNewbies #DL https://t.co/9K8SQRyVK4 https://t.co/SArtolUrYr', UserID=268619848, partition=0)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "29262b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|          CreatedAt|\n",
      "+-------------------+\n",
      "|2022-12-02 00:42:43|\n",
      "|2022-12-02 00:43:00|\n",
      "|2022-12-02 00:43:33|\n",
      "|2022-12-02 00:44:03|\n",
      "|2022-12-02 00:44:12|\n",
      "|2022-12-02 00:44:24|\n",
      "|2022-12-02 00:44:25|\n",
      "|2022-12-02 00:44:26|\n",
      "|2022-12-02 00:44:35|\n",
      "|2022-12-02 00:45:00|\n",
      "|2022-12-02 00:38:58|\n",
      "|2022-12-02 00:39:07|\n",
      "|2022-12-02 00:40:01|\n",
      "|2022-12-02 00:40:14|\n",
      "|2022-12-02 00:40:41|\n",
      "|2022-12-02 00:40:47|\n",
      "|2022-12-02 00:41:37|\n",
      "|2022-12-02 00:42:30|\n",
      "|2022-12-02 00:42:32|\n",
      "|2022-12-02 00:42:36|\n",
      "+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df2=df.withColumn(\"CreatedAt\",f.from_unixtime(f.col(\"CreatedAt\")/1000))\n",
    "df2.select(\"CreatedAt\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc238b9",
   "metadata": {},
   "source": [
    "### 1. Zählen der Tweets pro Stunde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "1185327b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 63:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|hour|total|\n",
      "+----+-----+\n",
      "|   0|   70|\n",
      "+----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_hourly=(df2  \n",
    "        # if stream did run less than an hour take minutes\n",
    "        .withColumn(\"hour\", f.hour(f.col(\"CreatedAt\")))\n",
    "        .groupBy(\"hour\")\n",
    "        .count()\n",
    "        .withColumnRenamed(\"count\",\"total\")\n",
    "        .sort(\"hour\")\n",
    "    )\n",
    "\n",
    "df_hourly.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0d5a8f",
   "metadata": {},
   "source": [
    "### 1. Top 10 User nach Tweet-Anzahl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "c2a6ca1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|    ScreenName|total|\n",
      "+--------------+-----+\n",
      "|       sdogdev|    6|\n",
      "| jayeshmthakur|    4|\n",
      "|periscopeislit|    4|\n",
      "| flutterbyamey|    3|\n",
      "| MedicaliPhone|    2|\n",
      "|     TimedoArt|    2|\n",
      "|     aisear_ch|    2|\n",
      "|  magnifintech|    2|\n",
      "|  amit_rai_333|    2|\n",
      "|  hernangraffe|    2|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_top_user=(df2\n",
    "                .groupBy(\"ScreenName\")\n",
    "                .agg(\n",
    "                    f.count(\"ScreenName\").alias(\"total\")\n",
    "                    )\n",
    "                .orderBy(f.col(\"total\").desc())\n",
    "                .limit(10)\n",
    "                )\n",
    "\n",
    "df_top_user.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58765db1",
   "metadata": {},
   "source": [
    "### Top 5 Hashtags der Top 10 User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "f0bd4d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 84:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|      hashtag|count|\n",
      "+-------------+-----+\n",
      "|           AI|   18|\n",
      "|       Python|   16|\n",
      "|       RStats|   15|\n",
      "|   TensorFlow|   14|\n",
      "|DataAnalytics|   11|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Top 5 Hashtags der Top 10 User\n",
    "# a) reduziere Gesamtdaten auf die Menge der 10 Top User via Join\n",
    "df_top3_per_user=(df_top_user\n",
    "            # filter via join\n",
    "            .join(df,[df_top_user.ScreenName==df.ScreenName],how=\"left\")\n",
    "            # hashtags array in Zeilen Einträge exploden\n",
    "            .withColumn(\"hashtag\",f.explode(f.col(\"Hashtags\")))\n",
    "            # hashtag array Spalte entfernen\n",
    "            .drop(\"Hashtags\")\n",
    "            # groupieren und counten by hashtag\n",
    "            .groupBy(\"hashtag\").count()\n",
    "            # rückwärts sortieren\n",
    "            .sort(f.col(\"count\").desc())\n",
    "            # top 5 selectieren\n",
    "            .limit(5)\n",
    "            \n",
    "    )\n",
    "    \n",
    "df_top3_per_user.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb68e6c",
   "metadata": {},
   "source": [
    " ### Top 10 Influencer (User deren #BigData-tweets mit den meisten Followern) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b5113b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK WARUM IST RETWEET COUNT IMMER 0??\n",
    "df_withRetweets=(df_top_user\n",
    "            # filter via join auf die Top 10 Influencer\n",
    "            .join(df,[df_top_user.user==df.user],how=\"left\")\n",
    "            .where(f.col(\"retweet_count\")>0)\n",
    "                )   \n",
    "  \n",
    "df_withRetweets.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47a7c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 Influencer (User deren #BigData-tweets am meisten retweeted wurden) (des letzten Monats)\n",
    "df_withRetweets=(df_top_user\n",
    "            # filter via join auf die Top 10 Influencer\n",
    "            .join(df,[df_top_user.user==df.user],how=\"left\")\n",
    "            # filter auf wurde retweeded hat eine Wert\n",
    "            .where(f.col(\"tweet_id\").isNotNull())\n",
    "            # filter auf die etzten zwei Wochen\n",
    "            #.filter(f.add_months(f.current_date(), -1) < f.col(\"created_date\"))\n",
    "            .filter(f.array_contains(f.col(\"hashtags\"),\"BigData\"))\n",
    "            # groupieren bei retweed_id und name\n",
    "            .groupBy(\"tweet_id\",df.user)\n",
    "            .agg(f.max(\"retweet_count\"))\n",
    "            .groupBy(df.user)\n",
    "            .agg(f.sum(\"max(retweet_count)\"))\n",
    "            .withColumnRenamed(\"sum(max(retweet_count))\",\"total\")\n",
    "            .orderBy(f.col(\"total\").desc())\n",
    "            .limit(10)\n",
    "            \n",
    "    )\n",
    "\n",
    "df_withRetweets.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13676250",
   "metadata": {},
   "source": [
    "### Aufgabe der Titel noch fehlt( Filter nach Location der TOP 10 User, Top  Hashtag pro Location)  IN SQL oder Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1d4233",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql=\"\"\"\n",
    "SELECT user_location, word, total FROM (\n",
    "    SELECT user_location, word, total, rank() over(partition by a.user_location order by a.total desc) as rank FROM\n",
    "    (\n",
    "        SELECT user_location,word, count(*) as total\n",
    "        FROM tweets\n",
    "        LATERAL VIEW explode(tweets.hashtags) tweets as word\n",
    "        WHERE word not in (\"BigData\",\"bigdata\",\"\")\n",
    "        AND tweets.user_location IN\n",
    "        (SELECT t.user_location\n",
    "                FROM (\n",
    "                         SELECT user_location, count(*) as total from tweets\n",
    "                         WHERE user_location not in (\"\",\"null\",\"REMOTE\",\"Earth\")\n",
    "                         GROUP BY user_location\n",
    "                         ORDER BY total DESC LIMIT 10\n",
    "                     ) as t\n",
    "        )\n",
    "        GROUP BY user_location, word\n",
    "        ) as a\n",
    "    ) as b\n",
    "WHERE rank=1\n",
    "ORDER BY total DESC LIMIT 10;\n",
    "\"\"\"\n",
    "\n",
    "df.registerTempTable(\"tweets\")\n",
    "\n",
    "df_result = spark.sql(sql)\n",
    "df_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f14e0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3=(df\n",
    "    .select(\"user_location\")\n",
    "    .where(~f.col(\"user_location\").isin(\"\",\"null\",\"REMOTE\",\"Earth\"))\n",
    "    .groupBy(\"user_location\")\n",
    "    .count()\n",
    "    .withColumnRenamed(\"count\",\"location_total\")\n",
    "    .orderBy(f.col(\"location_total\").desc())\n",
    "    .limit(10)\n",
    "    )\n",
    "\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb05cf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4=(df\n",
    "    .select(\"user_location\",\"hashtag\")\n",
    "    .withColumn(\"singletag\",f.explode(f.col(\"hashtag\")))\n",
    "    .groupBy(\"user_location\",\"singletag\")\n",
    "    .count()\n",
    "    .withColumnRenamed(\"count\",\"tags_total\")\n",
    "    )\n",
    "\n",
    "df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132149f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5=(df3.alias(\"a\")\n",
    "    .join(f.broadcast(df4.alias(\"b\")),[df3.user_location==df4.user_location],how=\"left\")\n",
    "    .select(\"a.user_location\",\"a.location_total\",\"b.singletag\",\"b.tags_total\")\n",
    "    .withColumn(\"rank\",f.row_number().over(Window.partitionBy(\"a.user_location\").orderBy(f.col(\"b.tags_total\").desc())))\n",
    "    .filter(f.col(\"rank\")==1)\n",
    "    .sort(f.col(\"location_total\").desc())\n",
    "    .limit(100)\n",
    "    )\n",
    "df5.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17094ce",
   "metadata": {},
   "source": [
    "## Schreiben der Daten nach Parquet und Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f44c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer_parquet=(df\n",
    "                .write.partitionBy(\"tweet_language\")\n",
    "                .mode(\"overwrite\")\n",
    "                .format(\"parquet\")\n",
    "                .save(\"s3a://solution/twitter_parquet\")\n",
    "            )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e819843c",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer_delta=(df\n",
    "                #.where(f.col(\"tweet_language\").isin(\"en\",\"de\"))\n",
    "                .write.partitionBy(\"tweet_language\")\n",
    "                .mode(\"overwrite\")\n",
    "                .format(\"delta\")\n",
    "                .save(\"s3a://solution/twitter_delta\")\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d75cb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a081f82e",
   "metadata": {},
   "source": [
    "## Delta History and Time Travel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f34013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Delta file in s3 into Delta Table Object\n",
    "dt = DeltaTable.forPath(spark, \"s3a://solution/twitter_delta\")\n",
    "dt.toDF().show(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7862d91",
   "metadata": {},
   "source": [
    "#### Time Travel Aufgabe\n",
    "1. Excecute write to delta several times and check how the history adds new entries  \n",
    "2. Load one of the versions and check all the `tweet_language` (via distinct().show())\n",
    "3. write again delta with activated filter to write only out the `tweet_language` `en` and `de` \n",
    "4. confirm in the history log that less files were written out\n",
    "5. Load the latest version and check that only two entries are available\n",
    "6. Load older versions and confirm that there are still all data available\n",
    "7. Revert old version to latest again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5127f540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the metadata for the full history of the table\n",
    "fullHistoryDF = dt.history()    \n",
    "\n",
    "# get the metadata for the last operation\n",
    "lastOperationDF = dt.history(1) \n",
    "\n",
    "fullHistoryDF.select(\"version\",\"readVersion\",\"timestamp\",\"userId\",\"operation\",\"operationParameters\",\"operationMetrics\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92161ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load latest delta version\n",
    "df_timetravel = spark.read.format(\"delta\").load(\"s3a://solution/twitter_delta\")\n",
    "\n",
    "df_timetravel.select(\"tweet_language\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e31aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load specific historic version\n",
    "df_timetravel = spark.read.format(\"delta\").option(\"versionAsOf\", 2).load(\"s3a://solution/twitter_delta\")\n",
    "\n",
    "df_timetravel.select(\"tweet_language\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e561fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write old version back as latest\n",
    "f_timetravel = (spark\n",
    "                .read.format(\"delta\").option(\"versionAsOf\", 0).load(\"s3a://solution/twitter_delta\")\n",
    "                .write.partitionBy(\"tweet_language\").mode(\"overwrite\").format(\"delta\").save(\"s3a://solution/twitter_delta\")\n",
    "               )\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8be4272d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/01 23:00:29 WARN ExecutorPodsWatchSnapshotSource: Kubernetes client has been closed.\n"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdb31e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
