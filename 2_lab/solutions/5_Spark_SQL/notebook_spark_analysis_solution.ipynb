{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5765a06e",
      "metadata": {},
      "source": [
        "# Spark Aufgaben\n",
        "1. Importe laden\n",
        "2. Jupyter Spark starten und Twitter-Streams von Avro lesen\n",
        "3. ETL Strecke: Avro Daten einlesen und als Delta Datei wieder raus schreiben\n",
        "4. Analyse-Aufgaben erledigen \n",
        "5. Verlaufsanalyse durchführen\n",
        "6. **Ausschalten der Spark-App**\n",
        "\n",
        "## Wichtige Hinweise\n",
        "1. Führe alle Anweisungen in der vorgegebenen Reihenfolge aus. Die einzelnen Programmierzellen bauen aufeinander auf.\n",
        "2. **Beende unbedingt am Ende die Spark-Anwendung mit dem untersten Befehl \"spark.stop()\" , wenn du aufhörst an den Daten zu arbeiten.**\n",
        "3. Du kannst jederzeit das Notebook wieder hochfahren, wenn du Schritt 1 & 2 (Laden der Imports & Jupyter Spark und seine Konfigurationen hochfahren) ausführen.\n",
        "4. Mit **\"Strg\" + \"Enter\"** führst du einzelne Zellen direkt aus.\n",
        "5. In der oberen Leiste kannst du über **\"Insert\"** weitere Zellen hinzufügen, um weitere Test-Funktionen zu schreiben. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03c9d1f3",
      "metadata": {},
      "source": [
        "## 1. Laden der Imports\n",
        "Hier werden alle benötigten Libraries für dieses Lab heruntergeladen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "eebe3ff2",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_303/1413759690.py:20: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
            "  from IPython.core.display import display, HTML\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>.container {width:100% !important; }<style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import SQLContext\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql import Row\n",
        "from pyspark.sql.functions import explode\n",
        "from pyspark.sql.functions import lower, col\n",
        "import pyspark.sql.functions as f\n",
        "\n",
        "from delta import *\n",
        "\n",
        "\n",
        "import datetime\n",
        "from datetime import datetime\n",
        "import json\n",
        "\n",
        "\n",
        "# use 95% of the screen for jupyter cell\n",
        "from IPython.core.display import display, HTML\n",
        "display(HTML(\"<style>.container {width:100% !important; }<style>\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2579b387",
      "metadata": {},
      "source": [
        "## 2. Jupyter Spark & Konfigurationen hochfahren\n",
        "Hier wird die App jupyter-spark konfiguriert und hochgefahren, welche unsere weiteren Schritte ausführt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "a6d15c94",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "22/12/08 22:38:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "spark.kubernetes.namespace = frontend\n",
            "spark.master = k8s://https://kubernetes.default.svc.cluster.local:443\n",
            "spark.app.name = jupyter-spark\n",
            "spark.executor.memory = 1G\n",
            "spark.executor.cores = 2\n",
            "spark.driver.host = jupyter-spark-driver.frontend.svc.cluster.local\n"
          ]
        }
      ],
      "source": [
        "appName=\"jupyter-spark\"\n",
        "\n",
        "conf = SparkConf()\n",
        "\n",
        "# CLUSTER MANAGER\n",
        "################################################################################\n",
        "# set Kubernetes Master as Cluster Manager(“k8s://https://” is NOT a typo, this is how Spark knows the “provider” type).\n",
        "conf.setMaster(\"k8s://https://kubernetes.default.svc.cluster.local:443\")\n",
        "\n",
        "# CONFIGURE KUBERNETES\n",
        "################################################################################\n",
        "# set the namespace that will be used for running the driver and executor pods.\n",
        "conf.set(\"spark.kubernetes.namespace\",\"frontend\")\n",
        "# set the docker image from which the Worker pods are created\n",
        "conf.set(\"spark.kubernetes.container.image\", \"thinkportgmbh/workshops:spark-3.3.2\")\n",
        "conf.set(\"spark.kubernetes.container.image.pullPolicy\", \"Always\")\n",
        "\n",
        "# set service account to be used\n",
        "conf.set(\"spark.kubernetes.authenticate.driver.serviceAccountName\", \"spark\")\n",
        "# authentication for service account(required to create worker pods):\n",
        "conf.set(\"spark.kubernetes.authenticate.caCertFile\", \"/var/run/secrets/kubernetes.io/serviceaccount/ca.crt\")\n",
        "conf.set(\"spark.kubernetes.authenticate.oauthTokenFile\", \"/var/run/secrets/kubernetes.io/serviceaccount/token\")\n",
        "\n",
        "\n",
        "# CONFIGURE SPARK\n",
        "################################################################################\n",
        "conf.set(\"spark.sql.session.timeZone\", \"Europe/Berlin\")\n",
        "# set driver host. In this case the ingres service for the spark driver\n",
        "# find name of the driver service with 'kubectl get services' or in the helm chart configuration\n",
        "conf.set(\"spark.driver.host\", \"jupyter-spark-driver.frontend.svc.cluster.local\")\n",
        "# set the port, If this port is busy, spark-shell tries to bind to another port.\n",
        "conf.set(\"spark.driver.port\", \"29413\")\n",
        "# add the postgres driver jars into session\n",
        "conf.set(\"spark.jars\", \"/opt/spark/jars/spark-avro_2.12-3.3.2.jar\")\n",
        "conf.set(\"spark.executor.extraClassPath\",\"/opt/spark/jars/spark-avro_2.12-3.3.2.jar\")\n",
        "#conf.set(\"spark.executor.extraLibrary\",\"/opt/spark/jars/spark-sql-kafka-0-10_2.12-3.3.1.jar, /opt/spark/jars/kafka-clients-3.3.1.jar\")\n",
        "#conf.set(\"spark.driver.extraClassPath\",\"/opt/spark/jars/spark-sql-kafka-0-10_2.12-3.3.1.jar, /opt/spark/jars/kafka-clients-3.3.1.jar, /opt/spark/jars/spark-avro_2.12-3.3.1.jar\")\n",
        "\n",
        "# CONFIGURE S3 CONNECTOR\n",
        "conf.set(\"spark.hadoop.fs.s3a.endpoint\", \"minio.minio.svc.cluster.local:9000\")\n",
        "conf.set(\"spark.hadoop.fs.s3a.access.key\", \"trainadm\")\n",
        "conf.set(\"spark.hadoop.fs.s3a.secret.key\", \"train@thinkport\")\n",
        "conf.set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
        "conf.set(\"spark.hadoop.fs.s3.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
        "conf.set(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
        "conf.set(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
        "\n",
        "#conf.set(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension, org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions, org.apache.spark.sql.hudi.HoodieSparkSessionExtension\")\n",
        "conf.set(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
        "conf.set(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
        "\n",
        "# CONFIGURE WORKER (Customize based on workload)\n",
        "################################################################################\n",
        "# set number of worker pods\n",
        "conf.set(\"spark.executor.instances\", \"1\")\n",
        "# set memory of each worker pod\n",
        "conf.set(\"spark.executor.memory\", \"1G\")\n",
        "# set cpu of each worker pod\n",
        "conf.set(\"spark.executor.cores\", \"2\")\n",
        "# Number of possible tasks = cores * executores\n",
        "\n",
        "## Deltalake\n",
        "# conf.set(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
        "\n",
        "# SPARK SESSION\n",
        "################################################################################\n",
        "# and last, create the spark session and pass it the config object\n",
        "\n",
        "spark = SparkSession\\\n",
        "    .builder\\\n",
        "    .config(conf=conf) \\\n",
        "    .config('spark.sql.session.timeZone', 'Europe/Berlin') \\\n",
        "    .appName(appName)\\\n",
        "    .getOrCreate()\n",
        "\n",
        "# also get the spark context\n",
        "sc=spark.sparkContext\n",
        "# change the log level to warning, to see less output\n",
        "sc.setLogLevel('ERROR')\n",
        "\n",
        "# get the configuration object to check all the configurations the session was startet with\n",
        "for entry in sc.getConf().getAll():\n",
        "        if entry[0] in [\"spark.app.name\",\"spark.kubernetes.namespace\",\"spark.executor.memory\",\"spark.executor.cores\",\"spark.driver.host\",\"spark.master\"]:\n",
        "            print(entry[0],\"=\",entry[1])\n",
        "            \n",
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f42ed16",
      "metadata": {},
      "source": [
        "## 3. Einlesen und Schreiben von Daten"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93cbac52",
      "metadata": {},
      "source": [
        "### 3.1 Einlesen der Daten aus unserem S3 Speicher-Bucket \n",
        "Laden der Daten aus unserem Bucket in \"s3a://twitter/avro\" in einen DataFrame, um auf den Daten zu arbeiten. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "90b65543",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "22/12/08 22:38:22 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
          ]
        }
      ],
      "source": [
        "df_avro=(spark\n",
        "    .read.format(\"avro\")\n",
        "    # Pfad zu Bucket\n",
        "    .load(\"s3a://twitter/avro\")\n",
        "    # repartition auf 20 um optimierter mit den wenigen cpu zu arbeiten\n",
        "    .repartition(20)\n",
        "   ).cache()\n",
        "\n",
        "\n",
        " # nur Tweets mit dem Hashtag BigData weiter verwenden\n",
        "df = df_avro.filter(f.array_contains(f.col(\"hashtags\"),\"BigData\")==True)\n",
        "\n",
        "print(\"Anzahl aller Tweets: \",df_avro.count())\n",
        "print(\"Anzahl Tweets mit BigData: \",df.count())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "0f31f3ee",
      "metadata": {},
      "source": [
        "Kurz anschauen was da drin ist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "5dec3929",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------+-------------------+--------------------+-------------+---------------+-------------------+------------------+-------------+--------+--------------------+\n",
            "|           tweet_id|         created_at|       tweet_message|    user_name|  user_location|user_follower_count|user_friends_count|retweet_count|language|            hashtags|\n",
            "+-------------------+-------------------+--------------------+-------------+---------------+-------------------+------------------+-------------+--------+--------------------+\n",
            "|1600981511217459200|2022-12-08 23:31:51|Lead Data Enginee...|    vinay_145|           null|              19009|              3419|            0|      en|[LeadDataEngineer...|\n",
            "|1600982807937155072|2022-12-08 23:37:00|RT @gp_pulipaka: ...| phillip4real|  Cleveland, OH|                830|               696|            0|      en|[Maths, BigData, ...|\n",
            "|1600982603145924643|2022-12-08 23:36:12|RT @HacBrain247: ...|DevHighlights|           null|              11094|             12203|            0|      en|[MachineLearning,...|\n",
            "|1600983047465472000|2022-12-08 23:37:57|I am trusted and ...|  HacBrain247|London, Ontario|                467|                41|            0|      en|[ArtificialIntell...|\n",
            "|1600982702219415552|2022-12-08 23:36:35|Do you want to le...|  HacBrain247|London, Ontario|                467|                41|            0|      en|[DataScience, Cyb...|\n",
            "|1600981377591431168|2022-12-08 23:31:19|Inbox me for any ...|Stanleyhacks2|  United States|                257|               544|            0|      en|[BigData, Analyti...|\n",
            "|1600980382610083845|2022-12-08 23:27:22|RT @AI_Advice: TO...| PythonRoboto|           null|               3043|                 2|            0|      en|[AI, 100DaysofCod...|\n",
            "|1600981933282168832|2022-12-08 23:33:32|SQL Cheat Sheet ?...|  HacBrain247|London, Ontario|                466|                41|            0|      en|[BigData, Analyti...|\n",
            "|1600982949071310848|2022-12-08 23:37:34|Text me for any h...|Stanleyhacks2|  United States|                258|               544|            0|      en|[Instagram, BigDa...|\n",
            "|1600980170705362945|2022-12-08 23:26:32|RT @HacBrain247: ...| greentechdon|       Virginia|               5182|              1244|            0|      en|[MachineLearning,...|\n",
            "|1600982339454001152|2022-12-08 23:35:09|Any hacking servi...|  HacBrain247|London, Ontario|                466|                41|            0|      en|[MachineLearning,...|\n",
            "|1600980851491483648|2022-12-08 23:29:14|Hacking service\\n...|Stanleyhacks2|  United States|                257|               544|            0|      en|[BigData, Analyti...|\n",
            "|1600980568203620354|2022-12-08 23:28:06|I am trusted and ...|     Richack_|    Chicago, IL|               1464|               122|            0|      en|[ArtificialIntell...|\n",
            "|1600981153301004289|2022-12-08 23:30:26|RT @gp_pulipaka: ...| Anasalmana55|           null|                305|               151|            0|      en|[BigData, Analyti...|\n",
            "+-------------------+-------------------+--------------------+-------------+---------------+-------------------+------------------+-------------+--------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.show()print(\"Anzahl aller Tweets: \",df_avro.count())\n",
        "print(\"Anzahl Tweets mit BigData: \",df.count())\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7da71da4",
      "metadata": {},
      "source": [
        "### 3.2 Schreiben der Daten ins Delta-Format\n",
        "Hier werden die Daten direkt im Delta-Format umgewandelt und in den S3-Bucket \"s3a://twitter/delta\" geschrieben. Dieser Schritt ist wichtig, um die Daten passend für Trino zu abzulegen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "d6249e07",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "writer_delta=(df\n",
        "                .write.partitionBy(\"language\")\n",
        "                .mode(\"overwrite\")\n",
        "                .format(\"delta\")\n",
        "                .option(\"overwriteSchema\", \"true\")\n",
        "                .option(\"userMetadata\", \"Initial Ladung\")\n",
        "                .save(\"s3a://twitter/delta\")\n",
        "             )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "518be64b",
      "metadata": {},
      "source": [
        "## 4. Analyse-Aufgaben\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9b79d70",
      "metadata": {},
      "source": [
        "### 4.1 Tweets anschauen und den Aufbau des Dataframes\n",
        "Schau dir den Datensatz einmal genau an. Welche Spalten gibt es? Welche Datentypen sind vorhanden?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "406525a9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------+-------------------+--------------------+-------------+---------------+-------------------+------------------+-------------+--------+--------------------+\n",
            "|           tweet_id|         created_at|       tweet_message|    user_name|  user_location|user_follower_count|user_friends_count|retweet_count|language|            hashtags|\n",
            "+-------------------+-------------------+--------------------+-------------+---------------+-------------------+------------------+-------------+--------+--------------------+\n",
            "|1600981511217459200|2022-12-08 23:31:51|Lead Data Enginee...|    vinay_145|           null|              19009|              3419|            0|      en|[LeadDataEngineer...|\n",
            "|1600982807937155072|2022-12-08 23:37:00|RT @gp_pulipaka: ...| phillip4real|  Cleveland, OH|                830|               696|            0|      en|[Maths, BigData, ...|\n",
            "|1600982603145924643|2022-12-08 23:36:12|RT @HacBrain247: ...|DevHighlights|           null|              11094|             12203|            0|      en|[MachineLearning,...|\n",
            "|1600983047465472000|2022-12-08 23:37:57|I am trusted and ...|  HacBrain247|London, Ontario|                467|                41|            0|      en|[ArtificialIntell...|\n",
            "|1600982702219415552|2022-12-08 23:36:35|Do you want to le...|  HacBrain247|London, Ontario|                467|                41|            0|      en|[DataScience, Cyb...|\n",
            "|1600981377591431168|2022-12-08 23:31:19|Inbox me for any ...|Stanleyhacks2|  United States|                257|               544|            0|      en|[BigData, Analyti...|\n",
            "|1600980382610083845|2022-12-08 23:27:22|RT @AI_Advice: TO...| PythonRoboto|           null|               3043|                 2|            0|      en|[AI, 100DaysofCod...|\n",
            "|1600981933282168832|2022-12-08 23:33:32|SQL Cheat Sheet ?...|  HacBrain247|London, Ontario|                466|                41|            0|      en|[BigData, Analyti...|\n",
            "|1600982949071310848|2022-12-08 23:37:34|Text me for any h...|Stanleyhacks2|  United States|                258|               544|            0|      en|[Instagram, BigDa...|\n",
            "|1600980170705362945|2022-12-08 23:26:32|RT @HacBrain247: ...| greentechdon|       Virginia|               5182|              1244|            0|      en|[MachineLearning,...|\n",
            "|1600982339454001152|2022-12-08 23:35:09|Any hacking servi...|  HacBrain247|London, Ontario|                466|                41|            0|      en|[MachineLearning,...|\n",
            "|1600980851491483648|2022-12-08 23:29:14|Hacking service\\n...|Stanleyhacks2|  United States|                257|               544|            0|      en|[BigData, Analyti...|\n",
            "|1600980568203620354|2022-12-08 23:28:06|I am trusted and ...|     Richack_|    Chicago, IL|               1464|               122|            0|      en|[ArtificialIntell...|\n",
            "|1600981153301004289|2022-12-08 23:30:26|RT @gp_pulipaka: ...| Anasalmana55|           null|                305|               151|            0|      en|[BigData, Analyti...|\n",
            "+-------------------+-------------------+--------------------+-------------+---------------+-------------------+------------------+-------------+--------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b4223d1",
      "metadata": {},
      "source": [
        "### 4.2  Das Schema des Datensatzes anzeigen \n",
        "<br>\n",
        "<code> df.printSchema()</code> gibt das Schema des Datensatzes aus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "196c1696",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- tweet_id: string (nullable = true)\n",
            " |-- created_at: timestamp (nullable = true)\n",
            " |-- tweet_message: string (nullable = true)\n",
            " |-- user_name: string (nullable = true)\n",
            " |-- user_location: string (nullable = true)\n",
            " |-- user_follower_count: integer (nullable = true)\n",
            " |-- user_friends_count: integer (nullable = true)\n",
            " |-- retweet_count: integer (nullable = true)\n",
            " |-- language: string (nullable = true)\n",
            " |-- hashtags: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edc238b9",
      "metadata": {},
      "source": [
        "### 4.3 Zählen der Tweets pro Stunde\n",
        "Schreibe eine Abfrage, die **die Anzahl an Tweets pro Stunde** zählt.\n",
        "<br>\n",
        "<br>\n",
        "<details>\n",
        "<summary> &#8964 Lösung </summary>\n",
        "<p>\n",
        "<code>df_hourly=(df\n",
        "            .withColumn(\"hour\", f.hour(f.col(\"created_at\")))\n",
        "            .groupBy(\"hour\")\n",
        "            .count()\n",
        "            .withColumnRenamed(\"count\",\"total\")\n",
        "            .sort(\"hour\")\n",
        "          )\n",
        "df_hourly.show(20)</code>\n",
        "</details>\n",
        "</p>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "1185327b",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 21:============================================>           (16 + 2) / 20]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+-----+\n",
            "|hour|total|\n",
            "+----+-----+\n",
            "|  23|   14|\n",
            "+----+-----+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "df_hourly=(df  \n",
        "            .withColumn(\"hour\", f.hour(f.col(\"created_at\")))\n",
        "            .groupBy(\"hour\")\n",
        "            .count()\n",
        "            .withColumnRenamed(\"count\",\"total\")\n",
        "            .sort(\"hour\")\n",
        "          )\n",
        "\n",
        "df_hourly.show(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f0d5a8f",
      "metadata": {},
      "source": [
        "### 4.4 Top 10 User nach Tweet-Anzahl\n",
        "Schreibe eine Abfrage, die die **Top User** nach ihrer **Anzahl an Tweets** ausgibt. Bedenke dabei, deine Ausgabe auf **10** Einträge zu limitieren.\n",
        "<br>\n",
        "<br>\n",
        "<details>\n",
        "<summary> &#8964 Lösung </summary>\n",
        "<p>\n",
        "<code>df_top_user=(df\n",
        "                .groupBy(\"user_name\")\n",
        "                .agg(\n",
        "                    f.count(\"user_name\").alias(\"numberOfTweets\")\n",
        "                    )\n",
        "                .orderBy(f.col(\"numberOfTweets\").desc())\n",
        "                .limit(10)\n",
        "                .withColumnRenamed(\"user_name\",\"user\")\n",
        "                )\n",
        "df_top_user.show()</code>\n",
        "</details>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "c2a6ca1f",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 26:============================================>           (16 + 4) / 20]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------+--------------+\n",
            "|         user|numberOfTweets|\n",
            "+-------------+--------------+\n",
            "|  HacBrain247|             4|\n",
            "|Stanleyhacks2|             3|\n",
            "|    vinay_145|             1|\n",
            "| phillip4real|             1|\n",
            "|DevHighlights|             1|\n",
            "| PythonRoboto|             1|\n",
            "| greentechdon|             1|\n",
            "|     Richack_|             1|\n",
            "| Anasalmana55|             1|\n",
            "+-------------+--------------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "df_top_user=(df\n",
        "                .groupBy(\"user_name\")\n",
        "                .agg(\n",
        "                    f.count(\"user_name\").alias(\"numberOfTweets\")\n",
        "                    )\n",
        "                .orderBy(f.col(\"numberOfTweets\").desc())\n",
        "                .limit(10)\n",
        "                .withColumnRenamed(\"user_name\",\"user\")\n",
        "                )\n",
        "df_top_user.show()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "75828392",
      "metadata": {},
      "source": [
        "### 4.5 Umgang mit Arrays\n",
        "Für die folgenden Aufgabe wird die <code>explode</code>-Funktion benötigt. Schreibe eine Abfrage die das Hashtag-array mit <code>explode</code> teilt. Gebe dabei die Spalten \"user_name\", \"tweet_id\"und die explodierte\"hashtags\"- Spalte mit einem Limit von 20 Zeilen aus. \n",
        "<br>\n",
        "<br>\n",
        "<details>\n",
        "<summary> &#8964 Lösung </summary>\n",
        "<p>\n",
        "<code>df_hash=(df\n",
        "         .withColumn(\"hashtags\",explode(\"hashtags\"))\n",
        "        .limit(20)\n",
        "        .select(\"user_name\", \"tweet_id\", \"hashtags\")\n",
        "        )\n",
        "df_hash.show()</code>\n",
        "</details>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "bd44fd32",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 31:=====================================================>  (19 + 1) / 20]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------+-------------------+-----------------+\n",
            "|   user_name|           tweet_id|         hashtags|\n",
            "+------------+-------------------+-----------------+\n",
            "|   vinay_145|1600981511217459200| LeadDataEngineer|\n",
            "|   vinay_145|1600981511217459200|     DataEngineer|\n",
            "|   vinay_145|1600981511217459200|           Sydney|\n",
            "|   vinay_145|1600981511217459200|              NSW|\n",
            "|   vinay_145|1600981511217459200|        Australia|\n",
            "|   vinay_145|1600981511217459200|CareerOpportunity|\n",
            "|   vinay_145|1600981511217459200|        HiringNow|\n",
            "|   vinay_145|1600981511217459200|         TechJobs|\n",
            "|   vinay_145|1600981511217459200|   DataManagement|\n",
            "|   vinay_145|1600981511217459200|          BigData|\n",
            "|phillip4real|1600982807937155072|            Maths|\n",
            "|phillip4real|1600982807937155072|          BigData|\n",
            "|phillip4real|1600982807937155072|        Analytics|\n",
            "|phillip4real|1600982807937155072|      DataScience|\n",
            "|phillip4real|1600982807937155072|               AI|\n",
            "|phillip4real|1600982807937155072|  MachineLearning|\n",
            "|phillip4real|1600982807937155072|              IoT|\n",
            "|phillip4real|1600982807937155072|             IIoT|\n",
            "|phillip4real|1600982807937155072|           Python|\n",
            "|phillip4real|1600982807937155072|           RStats|\n",
            "+------------+-------------------+-----------------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "df_hash=(df\n",
        "         .withColumn(\"hashtags\",explode(\"hashtags\"))\n",
        "         .limit(20)\n",
        "         .select(\"user_name\", \"tweet_id\", \"hashtags\")\n",
        "        )\n",
        "df_hash.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58765db1",
      "metadata": {},
      "source": [
        "### 4.6 Top 5 Hashtags der Top 10 User\n",
        "Schreibe eine Abfrage, die die **Top 5 der Hashtags** der **10 User** mit den **meisten Tweets** ausgibt.\n",
        "<br>\n",
        "<br>\n",
        "<details>\n",
        "<summary> &#8964 Lösung </summary>\n",
        "<p>\n",
        "<code>df_top5_per_user=(df_top_user\n",
        "            # filter via join\n",
        "            .join(df,[df_top_user.user==df.user_name],how=\"left\")\n",
        "            # hashtags array in Zeilen Einträge exploden\n",
        "            .withColumn(\"hashtags\",explode(\"hashtags\"))\n",
        "            # hashtags lowercase schreiben um Doppelungen zu entfernen\n",
        "            .withColumn(\"hashtags\", lower(col('hashtags')))\n",
        "            # groupieren und counten by hashtag\n",
        "            .groupBy(\"hashtags\").agg(f.count(\"hashtags\"))\n",
        "            # rückwärts sortieren\n",
        "            .sort(f.col(\"count(hashtags)\").desc())\n",
        "            # top 5 selectieren\n",
        "            .limit(5) \n",
        "                 )\n",
        "df_top5_per_user.show()</code>\n",
        "</details>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "f0bd4d64",
      "metadata": {
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 41:=========================>                               (9 + 3) / 20]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------+---------------+\n",
            "|     hashtags|count(hashtags)|\n",
            "+-------------+---------------+\n",
            "|      bigdata|             14|\n",
            "|  datascience|             12|\n",
            "|       python|             11|\n",
            "|    analytics|              9|\n",
            "|cybersecurity|              9|\n",
            "+-------------+---------------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "df_top5_per_user=(df_top_user\n",
        "            # filter via join\n",
        "            .join(df,[df_top_user.user==df.user_name],how=\"left\")\n",
        "            # hashtags array in Zeilen Einträge exploden\n",
        "            .withColumn(\"hashtags\",explode(\"hashtags\"))\n",
        "            # hashtags lowercase schreiben um Doppelungen zu entfernen\n",
        "            .withColumn(\"hashtags\", lower(col('hashtags')))\n",
        "            # groupieren und counten by hashtag\n",
        "            .groupBy(\"hashtags\").agg(f.count(\"hashtags\"))      \n",
        "            # rückwärts sortieren\n",
        "            .sort(f.col(\"count(hashtags)\").desc())\n",
        "            # top 5 selectieren\n",
        "            .limit(5)\n",
        "                 )\n",
        "df_top5_per_user.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbb68e6c",
      "metadata": {},
      "source": [
        " ### 4.7 Top 10 Influencer (User mit #BigData-tweets mit den meisten Followern) \n",
        " Schreibe eine Abfrage, die die **Top 10 Influencer** mit den **meisten Follower** zählt und sortiert anzeigt.\n",
        " <br>\n",
        "<br>\n",
        "<details>\n",
        "<summary> &#8964 Lösung </summary>\n",
        "<p>\n",
        "<code>df_top_influencer=(df\n",
        "                .groupBy(\"user_name\")\n",
        "                .agg(\n",
        "                    f.max(\"user_follower_count\").alias(\"follower\")\n",
        "                    )\n",
        "                .orderBy(f.col(\"follower\").desc())\n",
        "                )\n",
        "df_top_influencer.show(10)</code>\n",
        "</details>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "79b5113b",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 46:====================================>                   (13 + 3) / 20]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------+--------+\n",
            "|    user_name|follower|\n",
            "+-------------+--------+\n",
            "|    vinay_145|   19009|\n",
            "|DevHighlights|   11094|\n",
            "| greentechdon|    5182|\n",
            "| PythonRoboto|    3043|\n",
            "|     Richack_|    1464|\n",
            "| phillip4real|     830|\n",
            "|  HacBrain247|     467|\n",
            "| Anasalmana55|     305|\n",
            "|Stanleyhacks2|     258|\n",
            "+-------------+--------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "df_top_influencer=(df\n",
        "                .groupBy(\"user_name\")\n",
        "                .agg(\n",
        "                    f.max(\"user_follower_count\").alias(\"follower\")\n",
        "                    )\n",
        "                .orderBy(f.col(\"follower\").desc())\n",
        "                   \n",
        "                )\n",
        "df_top_influencer.show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "935e6148",
      "metadata": {},
      "source": [
        "### 4.8 Top 10 Influencer und ihre Anzahl an tweets\n",
        "Schreibe eine Abfrage, die die **Top 10 Influencer**, ihre Follower und die **Anzahl ihrer Tweets** ausgibt. außeredem soll es sortiert nach den Anzahl ihrer Follower sein. \n",
        "<br>\n",
        "<br>\n",
        "<details>\n",
        "<summary> &#8964 Lösung </summary>\n",
        "<p>\n",
        "<code>df_withRetweets=(df_top_user\n",
        "            # filter via join auf die Top 10 Influencer\n",
        "            .join(df_top_influencer, [df_top_influencer.user_name==df_top_user.user],how=\"left\")\n",
        "            .orderBy(f.col(\"follower\").desc())\n",
        "            .limit(10)\n",
        "            .drop(\"user_name\")\n",
        "            .select(\"user\",\"follower\",\"numberOfTweets\")\n",
        "    )\n",
        "df_withRetweets.show()</code>\n",
        "</details>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "e47a7c26",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------+--------+--------------+\n",
            "|         user|follower|numberOfTweets|\n",
            "+-------------+--------+--------------+\n",
            "|    vinay_145|   19009|             1|\n",
            "|DevHighlights|   11094|             1|\n",
            "| greentechdon|    5182|             1|\n",
            "| PythonRoboto|    3043|             1|\n",
            "|     Richack_|    1464|             1|\n",
            "| phillip4real|     830|             1|\n",
            "|  HacBrain247|     467|             4|\n",
            "| Anasalmana55|     305|             1|\n",
            "|Stanleyhacks2|     258|             3|\n",
            "+-------------+--------+--------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_withRetweets=(df_top_user\n",
        "            # filter via join auf die Top 10 Influencer\n",
        "            .join(df_top_influencer, [df_top_influencer.user_name==df_top_user.user],how=\"left\") \n",
        "            .orderBy(f.col(\"follower\").desc())\n",
        "            .limit(10)\n",
        "            .drop(\"user_name\")   \n",
        "            .select(\"user\",\"follower\",\"numberOfTweets\")\n",
        "            \n",
        "    )\n",
        "\n",
        "df_withRetweets.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13676250",
      "metadata": {},
      "source": [
        "### Bonusaufgabe: Filter nach den Top 10 Locations und ihrem Top Hashtag\n",
        "Schreibe eine Abfrage, die die **Top 10 häufigsten Locations** ausgibt und das am **zweitmeisten verwendete Hashtag** dort. Da alle unsere Daten das Hashtag #BigData beinhalten. \n",
        "<br>\n",
        "<br>\n",
        "<details>\n",
        "<summary> &#8964 Lösung </summary>\n",
        "<p>\n",
        "<code>df3=(df\n",
        "    .select(\"user_location\")\n",
        "    .where(~f.col(\"user_location\").isin(\"\",\"null\",\"REMOTE\",\"Earth\"))\n",
        "    .groupBy(\"user_location\")\n",
        "    .count()\n",
        "    .withColumnRenamed(\"count\",\"location_total\")\n",
        "    .orderBy(f.col(\"location_total\").desc())\n",
        "    .limit(10)\n",
        "    )</code>\n",
        "    \n",
        "<code>df4=(df\n",
        "    .select(\"user_location\",\"hashtags\")\n",
        "    .withColumn(\"singletag\",f.explode(f.col(\"hashtags\")))\n",
        "    .groupBy(\"user_location\",\"singletag\")\n",
        "    .count()\n",
        "    .withColumnRenamed(\"count\",\"tags_total\")\n",
        "        )</code>\n",
        "    \n",
        "<code>df5=(df3.alias(\"a\")\n",
        "    .join(f.broadcast(df4.alias(\"b\")),[df3.user_location==df4.user_location],how=\"left\")\n",
        "    .select(\"a.user_location\",\"a.location_total\",\"b.singletag\",\"b.tags_total\")      \n",
        "    .withColumn(\"rank\",f.row_number().over(Window.partitionBy(\"a.user_location\")\n",
        "    .orderBy(f.col(\"b.tags_total\").desc())))\n",
        "    .filter(f.col(\"rank\")==1)\n",
        "    .sort(f.col(\"location_total\").desc())\n",
        "    .limit(10)\n",
        "    )\n",
        "df5.show()</code>\n",
        "</details>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "2f14e0c2",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------------+--------------+\n",
            "|  user_location|location_total|\n",
            "+---------------+--------------+\n",
            "|London, Ontario|             4|\n",
            "|  United States|             3|\n",
            "|  Cleveland, OH|             1|\n",
            "|       Virginia|             1|\n",
            "|    Chicago, IL|             1|\n",
            "+---------------+--------------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "df3=(df\n",
        "    .select(\"user_location\")\n",
        "    .where(~f.col(\"user_location\").isin(\"\",\"null\",\"REMOTE\",\"Earth\"))\n",
        "    .groupBy(\"user_location\")\n",
        "    .count()\n",
        "    .withColumnRenamed(\"count\",\"location_total\")\n",
        "    .orderBy(f.col(\"location_total\").desc())\n",
        "    .limit(10)\n",
        "    )\n",
        "\n",
        "df3.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "eb05cf17",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 65:=================================>                      (12 + 2) / 20]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------+-----------------+----------+\n",
            "|user_location|        singletag|tags_total|\n",
            "+-------------+-----------------+----------+\n",
            "|         null|CareerOpportunity|         1|\n",
            "|         null|     DataEngineer|         1|\n",
            "|         null|           Sydney|         1|\n",
            "|         null|        HiringNow|         1|\n",
            "|         null|          BigData|         4|\n",
            "|         null|   DataManagement|         1|\n",
            "|         null|         TechJobs|         1|\n",
            "|         null|        Australia|         1|\n",
            "|         null| LeadDataEngineer|         1|\n",
            "|         null|              NSW|         1|\n",
            "|Cleveland, OH|           Python|         1|\n",
            "|Cleveland, OH|      DataScience|         1|\n",
            "|Cleveland, OH|          BigData|         1|\n",
            "|Cleveland, OH|           RStats|         1|\n",
            "|Cleveland, OH|             IIoT|         1|\n",
            "|Cleveland, OH|        Analytics|         1|\n",
            "|Cleveland, OH|            Maths|         1|\n",
            "|Cleveland, OH|  MachineLearning|         1|\n",
            "|Cleveland, OH|              IoT|         1|\n",
            "|Cleveland, OH|               AI|         1|\n",
            "+-------------+-----------------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "df4=(df\n",
        "    .select(\"user_location\",\"hashtags\")\n",
        "    .withColumn(\"singletag\",f.explode(f.col(\"hashtags\")))\n",
        "    .groupBy(\"user_location\",\"singletag\")\n",
        "    .count()\n",
        "    .withColumnRenamed(\"count\",\"tags_total\")\n",
        "    )\n",
        "\n",
        "df4.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "132149f4",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------------+--------------+---------------+----------+----+\n",
            "|  user_location|location_total|      singletag|tags_total|rank|\n",
            "+---------------+--------------+---------------+----------+----+\n",
            "|London, Ontario|             4|        BigData|         4|   2|\n",
            "|  United States|             3|MachineLearning|         3|   2|\n",
            "|    Chicago, IL|             1|       WhatsApp|         1|   2|\n",
            "|  Cleveland, OH|             1|            IoT|         1|   2|\n",
            "|       Virginia|             1|    DataScience|         1|   2|\n",
            "+---------------+--------------+---------------+----------+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df5=(df3.alias(\"a\")\n",
        "    .join(f.broadcast(df4.alias(\"b\")),[df3.user_location==df4.user_location],how=\"left\")\n",
        "    .select(\"a.user_location\",\"a.location_total\",\"b.singletag\",\"b.tags_total\")\n",
        "    .withColumn(\"rank\",f.row_number().over(Window.partitionBy(\"a.user_location\").orderBy(f.col(\"b.tags_total\").desc())))\n",
        "    .filter(f.col(\"rank\")==2)\n",
        "    .sort(f.col(\"location_total\").desc())\n",
        "    .limit(10)\n",
        "    )\n",
        "df5.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a081f82e",
      "metadata": {},
      "source": [
        "## 5. Delta History and Time Travel\n",
        "Führe den folgenden Code aus um die aktuelle Delta-Daten-Version upzudaten. Wenn du mehrere Versionen sehen willst schreibe öfter raus mit <code>writer_delta()</code> mit einigen Minuten Abstand."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "287ada1c",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "writer_delta=(df\n",
        "                #.filter(f.array_contains(f.col(\"hashtags\"),\"DataScience\")==True)\n",
        "                .write.partitionBy(\"language\")\n",
        "                .mode(\"overwrite\")\n",
        "                .format(\"delta\")\n",
        "                .option(\"overwriteSchema\", \"true\")\n",
        "                .option(\"userMetadata\", \"Update Ladung\")\n",
        "                .save(\"s3a://twitter/delta\")\n",
        "             )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52d10e78",
      "metadata": {},
      "source": [
        "### 5.1 Delta Tabelle ausgeben\n",
        "Lade die Delta-Tabelle und lasse dir die ersten 2 Einträge ausgeben.\n",
        "<br>\n",
        "<br>\n",
        "<details>\n",
        "<summary> &#8964 Lösung </summary>\n",
        "<p>\n",
        "<code># Load Delta file in s3 into Delta Table Object\n",
        "dt = DeltaTable.forPath(spark, \"s3a://twitter/delta\")\n",
        "dt.toDF().show(2)</code>\n",
        "</details>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "34f34013",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 90:>                                                         (0 + 1) / 1]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------+-------------------+--------------------+-----------+---------------+-------------------+------------------+-------------+--------+--------------------+\n",
            "|           tweet_id|         created_at|       tweet_message|  user_name|  user_location|user_follower_count|user_friends_count|retweet_count|language|            hashtags|\n",
            "+-------------------+-------------------+--------------------+-----------+---------------+-------------------+------------------+-------------+--------+--------------------+\n",
            "|1600981933282168832|2022-12-08 23:33:32|SQL Cheat Sheet ?...|HacBrain247|London, Ontario|                466|                41|            0|      en|[BigData, Analyti...|\n",
            "|1600982339454001152|2022-12-08 23:35:09|Any hacking servi...|HacBrain247|London, Ontario|                466|                41|            0|      en|[MachineLearning,...|\n",
            "+-------------------+-------------------+--------------------+-----------+---------------+-------------------+------------------+-------------+--------+--------------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Load Delta file in s3 into Delta Table Object\n",
        "dt = DeltaTable.forPath(spark, \"s3a://twitter/delta\")\n",
        "dt.toDF().show(2)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "c7862d91",
      "metadata": {},
      "source": [
        "### 5.2  Auslese der Historie aus den Metadaten\n",
        "1. Führe mehrmals Write to Delta aus und prüfe, wie die Historie neue Einträge hinzufügt  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "5127f540",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+-----------+-------------------+------+---------+--------------------+--------------------+\n",
            "|version|readVersion|          timestamp|userId|operation| operationParameters|    operationMetrics|\n",
            "+-------+-----------+-------------------+------+---------+--------------------+--------------------+\n",
            "|      1|          0|2022-12-08 23:42:16|  null|    WRITE|{mode -> Overwrit...|{numFiles -> 14, ...|\n",
            "|      0|       null|2022-12-08 23:40:12|  null|    WRITE|{mode -> Overwrit...|{numFiles -> 14, ...|\n",
            "+-------+-----------+-------------------+------+---------+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# get the metadata for the full history of the table\n",
        "fullHistoryDF = dt.history()    \n",
        "\n",
        "# get the metadata for the last operation\n",
        "lastOperationDF = dt.history(1) \n",
        "\n",
        "fullHistoryDF.select(\"version\",\"readVersion\",\"timestamp\",\"userId\",\"operation\",\"operationParameters\",\"operationMetrics\").show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "39f0a855",
      "metadata": {},
      "source": [
        "### 5.3 Laden einer  Versionen \n",
        "Lade eine der Versionen und lasse dir alle `languages` anzeigen (via distinct().show())\n",
        "<br>\n",
        "<br>\n",
        "<details>\n",
        "<summary> &#8964 Lösung </summary>\n",
        "<p>\n",
        "<code>df = spark.read.format(\"delta\").load(\"s3a://twitter/delta\")\n",
        "df.select(\"language\").distinct().show()</code>\n",
        "</details>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "f92161ce",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+\n",
            "|language|\n",
            "+--------+\n",
            "|      en|\n",
            "|     und|\n",
            "|     qht|\n",
            "|      es|\n",
            "|      it|\n",
            "|      ar|\n",
            "|      sv|\n",
            "|     qme|\n",
            "|      fr|\n",
            "|      pl|\n",
            "|      pt|\n",
            "|      in|\n",
            "|      tr|\n",
            "|      de|\n",
            "|      no|\n",
            "|      ro|\n",
            "|      tl|\n",
            "|      hu|\n",
            "|      ca|\n",
            "|      ht|\n",
            "+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------+-------------------+--------------------+---------------+--------------------+-------------------+------------------+-------------+--------+--------------------+\n",
            "|           tweet_id|         created_at|       tweet_message|      user_name|       user_location|user_follower_count|user_friends_count|retweet_count|language|            hashtags|\n",
            "+-------------------+-------------------+--------------------+---------------+--------------------+-------------------+------------------+-------------+--------+--------------------+\n",
            "|1600475534689026049|2022-12-07 14:01:17|AIM spoke to @Ree...|Analyticsindiam|    Bengaluru, India|              15350|               483|            0|      en|[quantumcomputing...|\n",
            "|1600475603202981889|2022-12-07 14:01:33|Micro Focus and J...|     jenna_loup|         Houston, TX|                 97|               102|            0|      en|[sustainability, ...|\n",
            "|1600475677743980544|2022-12-07 14:01:51|RT @Khulood_Alman...| Khulood_Almani|Kingdom of Saudi ...|              44910|              2259|            0|      en|[DataScientist, J...|\n",
            "|1600475711029989376|2022-12-07 14:01:59|RT @ghostx1010: E...|   falconX_1010|                null|                  3|               203|            0|      en|[MachineLearning,...|\n",
            "|1600475715505373184|2022-12-07 14:02:00|How to Transform ...|   DivergentCIO|         Kansas City|              29734|             28024|            0|      en|[EmergingTech, AI...|\n",
            "|1600475778503696384|2022-12-07 14:02:15|RT @Khulood_Alman...|   HAMDANLAVI89| Hafr Al Batin, KSA.|                225|               458|            0|      en|[Data, AI, Python...|\n",
            "|1600475800909717505|2022-12-07 14:02:20|RT @Khulood_Alman...| Khulood_Almani|Kingdom of Saudi ...|              44910|              2259|            0|      en|[DataAnalytics, S...|\n",
            "|1600475803912994816|2022-12-07 14:02:21|RT @Khulood_Alman...|  flutterbyamey|        Flutterverse|                855|                22|            0|      en|[Data, AI, Python...|\n",
            "|1600475805641064448|2022-12-07 14:02:22|Six Ways Artifici...|  terence_mills|New York | London...|              16043|              8615|            0|      en|[AI, AIio, BigDat...|\n",
            "|1600476059228524544|2022-12-07 14:03:22|RT @HaroldSinnott...|     Untukmassa|Yogyakarta, Indon...|                100|              1481|            0|      en|[SQL, MachineLear...|\n",
            "|1600475661529133057|2022-12-07 14:01:47|RT @Khulood_Alman...| Khulood_Almani|Kingdom of Saudi ...|              43619|              2259|            0|      en|[Data, AI, Python...|\n",
            "|1600476054220857344|2022-12-07 14:03:21|A China-linked na...| HexBuddy127001|                null|                 17|               106|            0|      en|[MachineLearning,...|\n",
            "|1600501676586270721|2022-12-07 15:45:10|Why We Migrated F...|   dataclaudius|           Zürich CH|               3215|              2839|            0|      en|[BigData, Analyti...|\n",
            "|1600501681703342080|2022-12-07 15:45:11|Federated Machine...|          goq7v|                null|               1201|              1200|            0|      en|[BigData, learnin...|\n",
            "|1600540645235605506|2022-12-07 18:20:01|Applications of s...|     ingliguori|         Italia 🇮🇹|              48693|              9137|            0|      en|[DigitalTransform...|\n",
            "|1600524775952379904|2022-12-07 17:16:57|RT @Sheraj99: #Li...|  gavaskart1996|        புதுக்கோட்டை|                354|               301|            0|      en|[Linux, MachineLe...|\n",
            "|1600524782201556997|2022-12-07 17:16:59|The first one, yo...|JoannBr21047226|              Canada|                588|              1864|            0|      en|[FraudDetection, ...|\n",
            "|1600615238977601536|2022-12-07 23:16:25|RT @Eli_Krumova: ...|    Eli_Krumova|                  UK|              31372|             23507|            0|      en|[Python, CheatShe...|\n",
            "|1600615259613851648|2022-12-07 23:16:30|RT @m2mtechconnec...|abhinav_singwal|                null|                206|               898|            0|      en|[VR, AR, Cloud, B...|\n",
            "|1600615273144393746|2022-12-07 23:16:33|RT @Eli_Krumova: ...|      RpatoolsC|                null|                879|                31|            0|      en|[DataScientist, D...|\n",
            "+-------------------+-------------------+--------------------+---------------+--------------------+-------------------+------------------+-------------+--------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# load latest delta version\n",
        "df = spark.read.format(\"delta\").load(\"s3a://twitter/delta\")\n",
        "df.select(\"language\").distinct().show()\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54cee9d7",
      "metadata": {},
      "source": [
        "### 5.4. Laden einer ältere Versionen \n",
        "Lade eine ältere Version und bestätige, dass noch alle Daten vorhanden sind.\n",
        "<br>\n",
        "<br>\n",
        "<details>\n",
        "<summary> &#8964 Lösung </summary>\n",
        "<p>\n",
        "<code>df_timetravel_old = spark.read.format(\"delta\").option(\"versionAsOf\", 2).load(\"s3a://twitter/delta\")\n",
        "df_timetravel_old.select(\"language\").distinct().show()\n",
        "df_timetravel_old.show()</code>\n",
        "</details>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "85e31aad",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+\n",
            "|language|\n",
            "+--------+\n",
            "|      en|\n",
            "|     und|\n",
            "|     qht|\n",
            "|      es|\n",
            "|      it|\n",
            "|      ar|\n",
            "|      sv|\n",
            "|     qme|\n",
            "|      fr|\n",
            "|      pl|\n",
            "|      pt|\n",
            "|      in|\n",
            "|      tr|\n",
            "|      de|\n",
            "|      no|\n",
            "|      ro|\n",
            "|      tl|\n",
            "|      hu|\n",
            "|      ca|\n",
            "|      ht|\n",
            "+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------+-------------------+--------------------+---------------+--------------------+-------------------+------------------+-------------+--------+--------------------+\n",
            "|           tweet_id|         created_at|       tweet_message|      user_name|       user_location|user_follower_count|user_friends_count|retweet_count|language|            hashtags|\n",
            "+-------------------+-------------------+--------------------+---------------+--------------------+-------------------+------------------+-------------+--------+--------------------+\n",
            "|1600475534689026049|2022-12-07 14:01:17|AIM spoke to @Ree...|Analyticsindiam|    Bengaluru, India|              15350|               483|            0|      en|[quantumcomputing...|\n",
            "|1600475603202981889|2022-12-07 14:01:33|Micro Focus and J...|     jenna_loup|         Houston, TX|                 97|               102|            0|      en|[sustainability, ...|\n",
            "|1600475677743980544|2022-12-07 14:01:51|RT @Khulood_Alman...| Khulood_Almani|Kingdom of Saudi ...|              44910|              2259|            0|      en|[DataScientist, J...|\n",
            "|1600475711029989376|2022-12-07 14:01:59|RT @ghostx1010: E...|   falconX_1010|                null|                  3|               203|            0|      en|[MachineLearning,...|\n",
            "|1600475715505373184|2022-12-07 14:02:00|How to Transform ...|   DivergentCIO|         Kansas City|              29734|             28024|            0|      en|[EmergingTech, AI...|\n",
            "|1600475778503696384|2022-12-07 14:02:15|RT @Khulood_Alman...|   HAMDANLAVI89| Hafr Al Batin, KSA.|                225|               458|            0|      en|[Data, AI, Python...|\n",
            "|1600475800909717505|2022-12-07 14:02:20|RT @Khulood_Alman...| Khulood_Almani|Kingdom of Saudi ...|              44910|              2259|            0|      en|[DataAnalytics, S...|\n",
            "|1600475803912994816|2022-12-07 14:02:21|RT @Khulood_Alman...|  flutterbyamey|        Flutterverse|                855|                22|            0|      en|[Data, AI, Python...|\n",
            "|1600475805641064448|2022-12-07 14:02:22|Six Ways Artifici...|  terence_mills|New York | London...|              16043|              8615|            0|      en|[AI, AIio, BigDat...|\n",
            "|1600476059228524544|2022-12-07 14:03:22|RT @HaroldSinnott...|     Untukmassa|Yogyakarta, Indon...|                100|              1481|            0|      en|[SQL, MachineLear...|\n",
            "|1600475661529133057|2022-12-07 14:01:47|RT @Khulood_Alman...| Khulood_Almani|Kingdom of Saudi ...|              43619|              2259|            0|      en|[Data, AI, Python...|\n",
            "|1600476054220857344|2022-12-07 14:03:21|A China-linked na...| HexBuddy127001|                null|                 17|               106|            0|      en|[MachineLearning,...|\n",
            "|1600501676586270721|2022-12-07 15:45:10|Why We Migrated F...|   dataclaudius|           Zürich CH|               3215|              2839|            0|      en|[BigData, Analyti...|\n",
            "|1600501681703342080|2022-12-07 15:45:11|Federated Machine...|          goq7v|                null|               1201|              1200|            0|      en|[BigData, learnin...|\n",
            "|1600540645235605506|2022-12-07 18:20:01|Applications of s...|     ingliguori|         Italia 🇮🇹|              48693|              9137|            0|      en|[DigitalTransform...|\n",
            "|1600524775952379904|2022-12-07 17:16:57|RT @Sheraj99: #Li...|  gavaskart1996|        புதுக்கோட்டை|                354|               301|            0|      en|[Linux, MachineLe...|\n",
            "|1600524782201556997|2022-12-07 17:16:59|The first one, yo...|JoannBr21047226|              Canada|                588|              1864|            0|      en|[FraudDetection, ...|\n",
            "|1600615238977601536|2022-12-07 23:16:25|RT @Eli_Krumova: ...|    Eli_Krumova|                  UK|              31372|             23507|            0|      en|[Python, CheatShe...|\n",
            "|1600615259613851648|2022-12-07 23:16:30|RT @m2mtechconnec...|abhinav_singwal|                null|                206|               898|            0|      en|[VR, AR, Cloud, B...|\n",
            "|1600615273144393746|2022-12-07 23:16:33|RT @Eli_Krumova: ...|      RpatoolsC|                null|                879|                31|            0|      en|[DataScientist, D...|\n",
            "+-------------------+-------------------+--------------------+---------------+--------------------+-------------------+------------------+-------------+--------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#load specific historic version\n",
        "df_timetravel_old = spark.read.format(\"delta\").option(\"versionAsOf\", 2).load(\"s3a://twitter/delta\")\n",
        "df_timetravel_old.select(\"language\").distinct().show()\n",
        "df_timetravel_old.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9144a895",
      "metadata": {},
      "source": [
        "### 5.5 Überschreiben von neueren Version\n",
        "Überschreibe nun mit der älteren Version die Aktuellste. \n",
        "<br>\n",
        "<br>\n",
        "<details>\n",
        "<summary> &#8964 Lösung </summary>\n",
        "<p>\n",
        "<code>df_pasttopresent = (spark\n",
        "                   .read.format(\"delta\").option(\"versionAsOf\", 0).load(\"s3a://twitter/delta\")\n",
        "                   .write.partitionBy(\"language\").mode(\"overwrite\").format(\"delta\").save(\"s3a://twitter/delta\")\n",
        "                   )\n",
        "df = spark.read.format(\"delta\").load(\"s3a://solution/twitter_delta\")\n",
        "df.show()</code>\n",
        "</details>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "49e561fa",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------+-------------------+--------------------+---------------+--------------------+-------------------+------------------+-------------+--------+--------------------+\n",
            "|           tweet_id|         created_at|       tweet_message|      user_name|       user_location|user_follower_count|user_friends_count|retweet_count|language|            hashtags|\n",
            "+-------------------+-------------------+--------------------+---------------+--------------------+-------------------+------------------+-------------+--------+--------------------+\n",
            "|1600475534689026049|2022-12-07 14:01:17|AIM spoke to @Ree...|Analyticsindiam|    Bengaluru, India|              15350|               483|            0|      en|[quantumcomputing...|\n",
            "|1600475603202981889|2022-12-07 14:01:33|Micro Focus and J...|     jenna_loup|         Houston, TX|                 97|               102|            0|      en|[sustainability, ...|\n",
            "|1600475677743980544|2022-12-07 14:01:51|RT @Khulood_Alman...| Khulood_Almani|Kingdom of Saudi ...|              44910|              2259|            0|      en|[DataScientist, J...|\n",
            "|1600475711029989376|2022-12-07 14:01:59|RT @ghostx1010: E...|   falconX_1010|                null|                  3|               203|            0|      en|[MachineLearning,...|\n",
            "|1600475715505373184|2022-12-07 14:02:00|How to Transform ...|   DivergentCIO|         Kansas City|              29734|             28024|            0|      en|[EmergingTech, AI...|\n",
            "|1600475778503696384|2022-12-07 14:02:15|RT @Khulood_Alman...|   HAMDANLAVI89| Hafr Al Batin, KSA.|                225|               458|            0|      en|[Data, AI, Python...|\n",
            "|1600475800909717505|2022-12-07 14:02:20|RT @Khulood_Alman...| Khulood_Almani|Kingdom of Saudi ...|              44910|              2259|            0|      en|[DataAnalytics, S...|\n",
            "|1600475803912994816|2022-12-07 14:02:21|RT @Khulood_Alman...|  flutterbyamey|        Flutterverse|                855|                22|            0|      en|[Data, AI, Python...|\n",
            "|1600475805641064448|2022-12-07 14:02:22|Six Ways Artifici...|  terence_mills|New York | London...|              16043|              8615|            0|      en|[AI, AIio, BigDat...|\n",
            "|1600476059228524544|2022-12-07 14:03:22|RT @HaroldSinnott...|     Untukmassa|Yogyakarta, Indon...|                100|              1481|            0|      en|[SQL, MachineLear...|\n",
            "|1600475661529133057|2022-12-07 14:01:47|RT @Khulood_Alman...| Khulood_Almani|Kingdom of Saudi ...|              43619|              2259|            0|      en|[Data, AI, Python...|\n",
            "|1600476054220857344|2022-12-07 14:03:21|A China-linked na...| HexBuddy127001|                null|                 17|               106|            0|      en|[MachineLearning,...|\n",
            "|1600501676586270721|2022-12-07 15:45:10|Why We Migrated F...|   dataclaudius|           Zürich CH|               3215|              2839|            0|      en|[BigData, Analyti...|\n",
            "|1600501681703342080|2022-12-07 15:45:11|Federated Machine...|          goq7v|                null|               1201|              1200|            0|      en|[BigData, learnin...|\n",
            "|1600540645235605506|2022-12-07 18:20:01|Applications of s...|     ingliguori|         Italia 🇮🇹|              48693|              9137|            0|      en|[DigitalTransform...|\n",
            "|1600524775952379904|2022-12-07 17:16:57|RT @Sheraj99: #Li...|  gavaskart1996|        புதுக்கோட்டை|                354|               301|            0|      en|[Linux, MachineLe...|\n",
            "|1600524782201556997|2022-12-07 17:16:59|The first one, yo...|JoannBr21047226|              Canada|                588|              1864|            0|      en|[FraudDetection, ...|\n",
            "|1600615238977601536|2022-12-07 23:16:25|RT @Eli_Krumova: ...|    Eli_Krumova|                  UK|              31372|             23507|            0|      en|[Python, CheatShe...|\n",
            "|1600615259613851648|2022-12-07 23:16:30|RT @m2mtechconnec...|abhinav_singwal|                null|                206|               898|            0|      en|[VR, AR, Cloud, B...|\n",
            "|1600615273144393746|2022-12-07 23:16:33|RT @Eli_Krumova: ...|      RpatoolsC|                null|                879|                31|            0|      en|[DataScientist, D...|\n",
            "+-------------------+-------------------+--------------------+---------------+--------------------+-------------------+------------------+-------------+--------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# write old version back as latest\n",
        "df_pasttopresent = (spark\n",
        "                   .read.format(\"delta\").option(\"versionAsOf\", 0).load(\"s3a://twitter/delta\")\n",
        "                   .write.partitionBy(\"language\").mode(\"overwrite\").format(\"delta\").save(\"s3a://twitter/delta\")\n",
        "                   )\n",
        "df = spark.read.format(\"delta\").load(\"s3a://twitter/delta\")\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7cde3106",
      "metadata": {},
      "source": [
        "### 5.6 Zurück in die Zukunft\n",
        "Kehre zurück zum aktuellsten Timestamp, indem `timestampAsOf`anstelle von `versionAsOf`verwenden und einem aktuellen timestamp, anstelle der Versionsnummer.\n",
        "<br>\n",
        "<br>\n",
        "<details>\n",
        "<summary> &#8964 Lösung </summary>\n",
        "<p>\n",
        "<code>f_b2future = (spark\n",
        "                .read.format(\"delta\").option(\"timestampAsOf\", \"\\<aktuellsten Stand\\>\").load(\"s3a://twitter/delta\")\n",
        "               )\n",
        "f_b2future.show()</code>\n",
        "</details>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "8be4272d",
      "metadata": {
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------+-------------------+--------------------+---------------+--------------------+-------------------+------------------+-------------+--------+--------------------+\n",
            "|           tweet_id|         created_at|       tweet_message|      user_name|       user_location|user_follower_count|user_friends_count|retweet_count|language|            hashtags|\n",
            "+-------------------+-------------------+--------------------+---------------+--------------------+-------------------+------------------+-------------+--------+--------------------+\n",
            "|1600589057381306401|2022-12-07 21:32:23|@EstelaMandela @b...| Khulood_Almani|Kingdom of Saudi ...|              43586|              2261|            0|      en|[AI, Python, Data...|\n",
            "|1600525331085295616|2022-12-07 17:19:09|Free Udemy Certif...|       mikejo_m|                null|                 12|                 4|            0|      en|[Developers, DEVC...|\n",
            "|1600501676586270721|2022-12-07 15:45:10|Why We Migrated F...|   dataclaudius|           Zürich CH|               3215|              2839|            0|      en|[BigData, Analyti...|\n",
            "|1600832825015513089|2022-12-08 13:41:02|@enilev @Cointele...| Khulood_Almani|Kingdom of Saudi ...|              42057|              2262|            0|      en|[Metaverse, Web3,...|\n",
            "|1600701823567945728|2022-12-08 05:00:28|The Non-Programme...|    gp_pulipaka|   Redondo Beach, CA|             132870|             21261|            0|      en|[Programming, Big...|\n",
            "|1600503754817736707|2022-12-07 15:53:25|Colorado Rockies ...|    IainLJBrown|              Marlow|             113804|             96748|            0|      en|[ArtificialIntell...|\n",
            "|1600578334731038720|2022-12-07 20:49:46|https://t.co/Uzjc...|   chidambara09|Mysore & 𝗕𝗘𝗥𝗟...|              11093|                13|            0|      en|[TvTime, movieS, ...|\n",
            "|1600807412277075971|2022-12-08 12:00:03|RT @ValueCoders: ...|  RobotConsumer|                null|               4018|                36|            0|      en|[MachineLearning,...|\n",
            "|1600524901261123586|2022-12-07 17:17:27|https://t.co/gOsg...|algorithmchurch|                null|                214|                74|            0|      en|[AlgorithmChurch,...|\n",
            "|1600729365129482242|2022-12-08 06:49:55|Do #AutonomousVeh...|   PinakiLaskar|19.134884, 72.810591|               2668|               543|            0|      en|[AutonomousVehicl...|\n",
            "|1600586174002647040|2022-12-07 21:20:55|RT @Khulood_Alman...|      alffiguer|      Estados Unidos|                202|              1103|            0|      en|[DataManagement, ...|\n",
            "|1600586192189198341|2022-12-07 21:21:00|#SQL Cheat Sheet!...|      AI_Advice|Havre de Grace, M...|               9063|              9132|            0|      en|[SQL, 5G, 100Days...|\n",
            "|1600610727840157696|2022-12-07 22:58:30|@Marileewoodwar2 ...|SAMUELS02077409|                null|                  1|                15|            0|      en|[Airdrop, DataSci...|\n",
            "|1600477256266256386|2022-12-07 14:08:07|EU hosts $400,000...|  PDH_Metaverse|                 UK |              11996|             10191|            0|      en|[Metaverse, Web3,...|\n",
            "|1600524814795804672|2022-12-07 17:17:06|RT @BDAnalyticsne...|  gavaskart1996|        புதுக்கோட்டை|                354|               301|            0|      en|[BigData, Analyti...|\n",
            "|1600799103914594304|2022-12-08 11:27:02|RT @PawanSomanchi...|  NINJAGOHTESYT|     ,estados unidos|                808|              4609|            0|      en|           [BigData]|\n",
            "|1600531934295298049|2022-12-07 17:45:24|#Infographic: 11 ...| EnFuseSolution|       Mumbai, India|                704|               788|            0|      en|[Infographic, Mac...|\n",
            "|1600759853815058434|2022-12-08 08:51:04|Free Udemy Certif...|    LexiBiancia|                null|                  6|                 0|            0|      en|[Developers, DEVC...|\n",
            "|1600590862614159361|2022-12-07 21:39:33|RT @gp_pulipaka: ...|1000dayscodingb|                null|                717|                 1|            0|      en|[BigData, Analyti...|\n",
            "|1600606290463625224|2022-12-07 22:40:52|@sonu_monika @dan...| Khulood_Almani|Kingdom of Saudi ...|              43584|              2261|            0|      en|[Data, strategy, ...|\n",
            "+-------------------+-------------------+--------------------+---------------+--------------------+-------------------+------------------+-------------+--------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "f_b2future = (spark\n",
        "                .read.format(\"delta\").option(\"timestampAsOf\", \"2022-12-08 16:37:54\").load(\"s3a://twitter/delta\")\n",
        "               )\n",
        "f_b2future.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d5df865",
      "metadata": {},
      "source": [
        "# 6. Ausschalten der Spark-App\n",
        "**Bitte schließe am Ende die Spark-App wieder mit dem folgenden Befehl `spark.stop()`, wenn du fertig mit der Bearbeitung der Aufgaben bist.** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "bfdb31e3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "22/12/08 16:32:34 WARN ExecutorPodsWatchSnapshotSource: Kubernetes client has been closed.\n"
          ]
        }
      ],
      "source": [
        "spark.stop()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb598b6c",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
