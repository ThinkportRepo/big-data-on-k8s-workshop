{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5765a06e",
   "metadata": {},
   "source": [
    "# Spark ETL Aufgaben\n",
    "1. Daten von Avro laden und nach Delta rausschreiben\n",
    "\n",
    "\n",
    "## Wichtige Hinweise\n",
    "1. Führe alle Anweisungen in der vorgegebenen Reihenfolge aus. Die einzelnen Programmierzellen bauen aufeinander auf.\n",
    "2. **Beende unbedingt am Ende die Spark-Anwendung mit dem untersten Befehl \"spark.stop()\" , wenn du aufhörst an den Daten zu arbeiten, damit die Resourcen (cpu und memory) wieder freigegeben werden**\n",
    "3. Du kannst jederzeit das Notebook wieder hochfahren, wenn du Schritt 1 & 2 (Laden der Imports & Jupyter Spark und seine Konfigurationen hochfahren) ausführen.\n",
    "4. Mit **\"Strg\" + \"Enter\"** führst du einzelne Zellen direkt aus.\n",
    "5. In der oberen Leiste kannst du über **\"Insert\"** weitere Zellen hinzufügen, um weitere Test-Funktionen zu schreiben. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "03c9d1f3",
   "metadata": {},
   "source": [
    "## 1. Module Laden\n",
    "Hier werden alle benötigten Libraries für dieses Lab heruntergeladen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eebe3ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21/2953329136.py:16: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container {width:100% !important; }<style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "from delta import *\n",
    "\n",
    "\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "\n",
    "# use 95% of the screen for jupyter cell\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container {width:100% !important; }<style>\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2579b387",
   "metadata": {},
   "source": [
    "## 2.  Spark starten\n",
    "Hier wird die App jupyter-spark konfiguriert und hochgefahren, welche unsere weiteren Schritte ausführt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6d15c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/07/03 14:17:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.kubernetes.namespace = frontend\n",
      "spark.master = k8s://https://kubernetes.default.svc.cluster.local:443\n",
      "spark.app.name = jupyter-spark\n",
      "spark.executor.memory = 1G\n",
      "spark.driver.host = jupyter-spark-driver.frontend.svc.cluster.local\n",
      "spark.executor.cores = 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://jupyter-spark-driver.frontend.svc.cluster.local:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>k8s://https://kubernetes.default.svc.cluster.local:443</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>jupyter-spark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7febfeba0e50>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "appName=\"jupyter-spark\"\n",
    "\n",
    "conf = SparkConf()\n",
    "\n",
    "# CLUSTER MANAGER\n",
    "################################################################################\n",
    "# set Kubernetes Master as Cluster Manager(“k8s://https://” is NOT a typo, this is how Spark knows the “provider” type).\n",
    "conf.setMaster(\"k8s://https://kubernetes.default.svc.cluster.local:443\")\n",
    "\n",
    "# CONFIGURE KUBERNETES\n",
    "################################################################################\n",
    "# set the namespace that will be used for running the driver and executor pods.\n",
    "conf.set(\"spark.kubernetes.namespace\",\"frontend\")\n",
    "# set the docker image from which the Worker pods are created\n",
    "conf.set(\"spark.kubernetes.container.image\", \"thinkportgmbh/workshops:spark-3.3.2\")\n",
    "conf.set(\"spark.kubernetes.container.image.pullPolicy\", \"Always\")\n",
    "\n",
    "# set service account to be used\n",
    "conf.set(\"spark.kubernetes.authenticate.driver.serviceAccountName\", \"spark\")\n",
    "# authentication for service account(required to create worker pods):\n",
    "conf.set(\"spark.kubernetes.authenticate.caCertFile\", \"/var/run/secrets/kubernetes.io/serviceaccount/ca.crt\")\n",
    "conf.set(\"spark.kubernetes.authenticate.oauthTokenFile\", \"/var/run/secrets/kubernetes.io/serviceaccount/token\")\n",
    "\n",
    "\n",
    "# CONFIGURE SPARK\n",
    "################################################################################\n",
    "conf.set(\"spark.sql.session.timeZone\", \"Europe/Berlin\")\n",
    "# set driver host. In this case the ingres service for the spark driver\n",
    "# find name of the driver service with 'kubectl get services' or in the helm chart configuration\n",
    "conf.set(\"spark.driver.host\", \"jupyter-spark-driver.frontend.svc.cluster.local\")\n",
    "# set the port, If this port is busy, spark-shell tries to bind to another port.\n",
    "conf.set(\"spark.driver.port\", \"29413\")\n",
    "# add the postgres driver jars into session\n",
    "conf.set(\"spark.jars\", \"/opt/spark/jars/spark-avro_2.12-3.3.2.jar\")\n",
    "conf.set(\"spark.executor.extraClassPath\",\"/opt/spark/jars/spark-avro_2.12-3.3.2.jar\")\n",
    "\n",
    "# CONFIGURE S3 CONNECTOR\n",
    "conf.set(\"spark.hadoop.fs.s3a.endpoint\", \"minio.minio.svc.cluster.local:9000\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.access.key\", \"trainadm\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.secret.key\", \"train@thinkport\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "conf.set(\"spark.hadoop.fs.s3.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "\n",
    "#conf.set(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension, org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions, org.apache.spark.sql.hudi.HoodieSparkSessionExtension\")\n",
    "conf.set(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "conf.set(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "# CONFIGURE WORKER (Customize based on workload)\n",
    "################################################################################\n",
    "# set number of worker pods\n",
    "conf.set(\"spark.executor.instances\", \"3\")\n",
    "# set memory of each worker pod\n",
    "conf.set(\"spark.executor.memory\", \"1G\")\n",
    "# set cpu of each worker pod\n",
    "conf.set(\"spark.executor.cores\", \"1\")\n",
    "# Number of possible tasks = cores * executores\n",
    "\n",
    "## Deltalake\n",
    "# conf.set(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "\n",
    "# SPARK SESSION\n",
    "################################################################################\n",
    "# and last, create the spark session and pass it the config object\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .config(conf=conf) \\\n",
    "    .config('spark.sql.session.timeZone', 'Europe/Berlin') \\\n",
    "    .appName(appName)\\\n",
    "    .getOrCreate()\n",
    "\n",
    "# also get the spark context\n",
    "sc=spark.sparkContext\n",
    "# change the log level to warning, to see less output\n",
    "sc.setLogLevel('ERROR')\n",
    "\n",
    "# get the configuration object to check all the configurations the session was startet with\n",
    "for entry in sc.getConf().getAll():\n",
    "        if entry[0] in [\"spark.app.name\",\"spark.kubernetes.namespace\",\"spark.executor.memory\",\"spark.executor.cores\",\"spark.driver.host\",\"spark.master\"]:\n",
    "            print(entry[0],\"=\",entry[1])\n",
    "            \n",
    "spark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9f42ed16",
   "metadata": {},
   "source": [
    "## 3. ETL: Einlesen - Transformieren - Schreiben"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "93cbac52",
   "metadata": {},
   "source": [
    "### 3.1 Einlesen der Avro Daten (Extract)\n",
    "Lade die Daten aus dem Bucket `s3a://twitter/avro` in einen DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc875f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avro=(\n",
    "    #<< HIER CODE EINFÜGEN >>\n",
    "    )\n",
    "\n",
    "# Count und Inhalt anzeigen\n",
    "print(\"Anzahl aller Tweets: \",df_avro.count())\n",
    "df_avro.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "712ddf5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl aller Tweets:  261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+--------------------+---------+--------------+-------------------+------------------+-------------+--------+--------------------+\n",
      "|            tweet_id|         created_at|       tweet_message|user_name| user_location|user_follower_count|user_friends_count|retweet_count|language|            hashtags|\n",
      "+--------------------+-------------------+--------------------+---------+--------------+-------------------+------------------+-------------+--------+--------------------+\n",
      "|bab7cc66-19ab-11e...|2023-07-03 16:13:05|Similique ducimus...|    Jamie|           USA|               4227|             41226|          409|      EN|           [BigData]|\n",
      "|bbe966e4-19ab-11e...|2023-07-03 16:13:07|Temporibus quam b...|    Karen|           USA|               2981|              6421|          902|      EN|[BigData, BigData...|\n",
      "|bc823dba-19ab-11e...|2023-07-03 16:13:08|Dolores incidunt ...|Charmaine|        Brasil|               1645|             26460|          740|      PT|[IoT, DataScience...|\n",
      "|bdb38888-19ab-11e...|2023-07-03 16:13:10|Velit sed consect...|  Michael|       Germany|                928|             24183|          126|      DE|[BigData, BigData...|\n",
      "|c6a67d56-19ab-11e...|2023-07-03 16:13:25|Incidunt fugit co...|     Mary|United Kingdom|               3578|              7737|          328|      EN|[DataScience, Dat...|\n",
      "+--------------------+-------------------+--------------------+---------+--------------+-------------------+------------------+-------------+--------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "CPU times: user 18.5 ms, sys: 25.2 ms, total: 43.7 ms\n",
      "Wall time: 33.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "############################\n",
    "# LÖSUNG\n",
    "############################\n",
    "df_avro=(spark\n",
    "    .read\n",
    "    .format(\"avro\")\n",
    "    # Pfad zu Bucket\n",
    "    .load(\"s3a://twitter/avro\")\n",
    "    # repartition auf 20 um optimierter mit den wenigen cpu zu arbeiten\n",
    "    #.repartition(20)\n",
    "   ).cache()\n",
    "\n",
    "\n",
    "\n",
    "# Count und Inhalt anzeigen\n",
    "print(\"Anzahl aller Tweets: \",df_avro.count())\n",
    "df_avro.show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "067a22f8",
   "metadata": {},
   "source": [
    "### 3.2 Transformieren des Dataframes (Transform)\n",
    "\n",
    "1. Filter nur auf die Zeilen, die das Hashtag \"BigData\" enthalten, mit Hilfe der passenden **Array Function** (https://spark.apache.org/docs/latest/sql-ref-functions-builtin.html#array-functions)\n",
    "2. Benenne folgende Spalten mit kürzeren Namen um, damti später weniger getippt werden muss.\n",
    "   - `user_name` -> `user`\n",
    "   - `user_location` -> `country`\n",
    "   - `user_follower_count` -> `follower`\n",
    "   - `user_friends_count` -> `friends`\n",
    "   - `retweet_count` -> `retweets´\n",
    "3. Füge eine neue Spalte `hastag_count` hinzu in der die Anzahl der Hashtags steht\n",
    "2. Entferne die Spalte `tweet_message` aus dem Resultset, da diese als langer String viel Speicher benötigt aber für die weiteren Analyse nicht benötigt wird\n",
    "\n",
    "\n",
    "Dokumentation: https://spark.apache.org/docs/3.1.1/api/python/reference/pyspark.sql.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "08800479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl aller Tweets:  1141\n",
      "Anzahl nach Transformation:  1141\n",
      "+--------------------+-------------------+--------------------+---------+--------------+-------------------+------------------+-------------+--------+--------------------+\n",
      "|            tweet_id|         created_at|       tweet_message|user_name| user_location|user_follower_count|user_friends_count|retweet_count|language|            hashtags|\n",
      "+--------------------+-------------------+--------------------+---------+--------------+-------------------+------------------+-------------+--------+--------------------+\n",
      "|8a1b1be6-199a-11e...|2023-07-03 14:10:02|Tenetur quas veri...|  Roberta|United Kingdom|                490|             30916|          688|      EN|[MachineLearning,...|\n",
      "|8b4c7a96-199a-11e...|2023-07-03 14:10:04|Dolorem provident...|  Bernard|           USA|                673|             10096|          256|      EN|                [AI]|\n",
      "|8daf525e-199a-11e...|2023-07-03 14:10:08|Deleniti fuga vol...|  Crystal|           USA|               2179|              5337|          662|      EN|[BigData, AI, Big...|\n",
      "|8e4824b6-199a-11e...|2023-07-03 14:10:09|Repellendus fuga ...|  Cynthia|        France|               3678|             19434|          159|      FR|[AI, BigData, Dat...|\n",
      "|90aba8f4-199a-11e...|2023-07-03 14:10:13|Iste nostrum fugi...|    Marie|           USA|               1838|             22264|          140|      EN|         [Analytics]|\n",
      "+--------------------+-------------------+--------------------+---------+--------------+-------------------+------------------+-------------+--------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_transformed = (df_avro\n",
    "                  # filter die Zeilen mit Hashtag BigData raus\n",
    "                  #<< HIER CODE EINFÜGEN >>\n",
    "                  # benenne die Spalten mit kürzeren Namen um\n",
    "                  #<< HIER CODE EINFÜGEN >>\n",
    "                  # füge eine neue Spalte mit der Anzahl der Hasthags hinzu\n",
    "                  #<< HIER CODE EINFÜGEN >>\n",
    "                  # falls nicht schon entfernt, entferne die Spalte `tweet_message`\n",
    "                  #<< HIER CODE EINFÜGEN >>\n",
    "              )\n",
    "\n",
    "\n",
    "# Dataframe anzeigen\n",
    "print(\"Anzahl aller Tweets: \",df_avro.count())\n",
    "print(\"Anzahl nach Transformation: \",df_transformed.count())\n",
    "df_transformed.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2e11975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+---------+--------+-------+--------+--------+--------------------+------------+\n",
      "|            tweet_id|               date|     user|follower|friends|retweets|language|            hashtags|hastag_count|\n",
      "+--------------------+-------------------+---------+--------+-------+--------+--------+--------------------+------------+\n",
      "|bab7cc66-19ab-11e...|2023-07-03 16:13:05|    Jamie|    4227|  41226|     409|      EN|           [BigData]|           1|\n",
      "|bbe966e4-19ab-11e...|2023-07-03 16:13:07|    Karen|    2981|   6421|     902|      EN|[BigData, BigData...|           4|\n",
      "|bc823dba-19ab-11e...|2023-07-03 16:13:08|Charmaine|    1645|  26460|     740|      PT|[IoT, DataScience...|           5|\n",
      "|bdb38888-19ab-11e...|2023-07-03 16:13:10|  Michael|     928|  24183|     126|      DE|[BigData, BigData...|           3|\n",
      "|c6a67d56-19ab-11e...|2023-07-03 16:13:25|     Mary|    3578|   7737|     328|      EN|[DataScience, Dat...|           4|\n",
      "+--------------------+-------------------+---------+--------+-------+--------+--------+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "CPU times: user 19.3 ms, sys: 15.3 ms, total: 34.7 ms\n",
      "Wall time: 2.17 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "############################\n",
    "# LÖSUNG\n",
    "############################\n",
    "\n",
    "df_transformed = (df_avro\n",
    "                  # filter die Zeilen mit Hashtag BigData raus\n",
    "                  .filter(f.array_contains(f.col(\"hashtags\"),\"BigData\")==True)\n",
    "                  # benenne die Spalten mit kürzeren Namen um\n",
    "                  .withColumnRenamed(\"user_location\",\"country\")\n",
    "                  # oder mit select und alias\n",
    "                  .select(\"tweet_id\",\"tweet_message\",\n",
    "                          f.col(\"user_name\").alias(\"user\"),\n",
    "                          f.col(\"user_follower_count\").alias(\"follower\"),\n",
    "                          f.col(\"user_friends_count\").alias(\"friends\"),\n",
    "                          f.col(\"retweet_count\").alias(\"retweets\"),\n",
    "                          \"language\",\"country\",\"hashtags\",\n",
    "                         )\n",
    "                 # füge eine neue Spalte mit der Anzahl der Hasthags hinzu\n",
    "                 .withColumn(\"hastag_count\",f.size(f.col(\"hashtags\")))\n",
    "                 # falls nicht schon entfernt, entferne die Spalte `tweet_message`\n",
    "                 .drop(\"tweet_message\")\n",
    "              )\n",
    "\n",
    "\n",
    "\n",
    "# Count und Inhalt anzeigen\n",
    "df_transformed.show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7da71da4",
   "metadata": {},
   "source": [
    "### 3.3 Rausschreiben der Daten in das Deltaformat (Load)\n",
    "Schreibe die Daten im Delta Format in das Bucke S3-Bucket `s3a://twitter/delta`   \n",
    "verwende herbei folgende Spezifikationen:\n",
    "- partitioniere die Daten nach der Spalte `language` \n",
    "- setze den Mode auf `append` (nicht `overwrite`)\n",
    "- setzt die Option `overwriteSchema`auf `true`\n",
    "- füge einen Kommentar `load from Spark` in den Metadaten hinzu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c13f331",
   "metadata": {},
   "outputs": [],
   "source": [
    "write=(df_transformed\n",
    "        .write\n",
    "        \n",
    "        # partitioniere nach Spalte `language`\n",
    "        \n",
    "        # schreibe mit mode=append\n",
    "        \n",
    "        # Füge Optionen und Kommentar dazu\n",
    "        \n",
    "        \n",
    "        .save(\"WOHIN?\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6249e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 64.9 ms, sys: 30.1 ms, total: 95 ms\n",
      "Wall time: 41.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "############################\n",
    "# LÖSUNG\n",
    "############################\n",
    "\n",
    "\n",
    "write=(df_transformed\n",
    "        .write\n",
    "        .format(\"delta\")\n",
    "        # partitioniere nach Spalte `language`\n",
    "        .partitionBy(\"language\")\n",
    "        # schreibe mit mode=append\n",
    "        .mode(\"append\") # overwrite\n",
    "        # Füge Optionen und Kommentar dazu\n",
    "        .option(\"overwriteSchema\", \"true\")\n",
    "        # .option(\"mergeSchema\", \"true\")\n",
    "        .option(\"userMetadata\", \"ETL Beladung\")\n",
    "        .save(\"s3a://twitter/delta\")\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "07465e47",
   "metadata": {},
   "source": [
    "## 4. Ergebnis überprüfen\n",
    "Überprüfe mit einer Methode deiner Wahl ob die Daten erfolgreich auf s3 angekommen sind und ob sie Partitioniert wurden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4edb9bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|partitionColumns|\n",
      "+----------------+\n",
      "|      [language]|\n",
      "+----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|language|\n",
      "+--------+\n",
      "|      PT|\n",
      "|      EN|\n",
      "|      DE|\n",
      "|      ES|\n",
      "|      FR|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 33:====================================================>   (47 + 2) / 50]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows:  157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "############################\n",
    "# LÖSUNG\n",
    "############################\n",
    "\n",
    "# im VSCode Terminal\n",
    "\n",
    "# s3 ls s3://twitter/delta/\n",
    "    \n",
    "# in Spark\n",
    "#dt_delta=spark.read.format(\"delta\").load(\"s3a://twitter/delta\")\n",
    "#print(\"Count: \",df_delta.count())\n",
    "\n",
    "deltaTable = DeltaTable.forPath(spark, \"s3a://twitter/delta\")\n",
    "deltaTable.detail().select(\"partitionColumns\").show()\n",
    "deltaTable.toDF().select(\"language\").distinct().show()\n",
    "print(\"Rows: \", deltaTable.toDF().count())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5d5df865",
   "metadata": {},
   "source": [
    "# 6. Ausschalten der Spark-App\n",
    "**Bitte schließe am Ende die Spark-App wieder mit dem folgenden Befehl `spark.stop()`, wenn du fertig mit der Bearbeitung der Aufgaben bist.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdb31e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
