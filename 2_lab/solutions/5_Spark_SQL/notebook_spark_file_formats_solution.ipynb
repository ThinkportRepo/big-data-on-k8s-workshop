{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "610d267c",
   "metadata": {},
   "source": [
    "# Aufgaben zu Big Data File Formats\n",
    "\n",
    "Folgende Aufgaben haben zum Ziel mit den verschiedenen Dateiformaten vertraut zu werden und insbesondere die speziellen Eigenschaften und Funktionen der Formate zu verstehen\n",
    "\n",
    "### CSV and JSON\n",
    "Klassische Datei Formate für Datenverarbeitung  \n",
    "**Typische Eigenschaften:** Einfache Struktur, human-readable, Zeilenformat\n",
    "\n",
    "### Avro, ORC, Parquet\n",
    "Big Data optimierte Formate um schnell große Datenmengen zu lesen und zu schreiben  \n",
    "**Typische Eigenschaften:** teilbar in kleine Dateien (splittable), komprimierbar (compressible), überspringbar (skippable), selbsterklärend (self describing with schema), Schema erweiterbar (Schema Evolution), Schema erzwingend (Schema Enforcment), Filter Pushdown\n",
    "\n",
    "### Delta, Iceberg, Hudi\n",
    "Erweiterte Big Data Formate um die ACID und Tracing Eigenschaften einer klassichen SQL Datenbank zu erfüllen  \n",
    "**Typische Eigenschaften:** Erweiterung um zusätzliche Metadaten und spezielle Treiber zum lesen/schreiben, Time Travel Funktion, Merge und Update Funktionen, Audit Log Funktionalitäten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592e56dd",
   "metadata": {},
   "source": [
    "###  Import Python Modules\n",
    "Hier werden alle benötigten Libraries für dieses Lab heruntergeladen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0aa4fda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container {width:100% !important; }<style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "from delta import *\n",
    "\n",
    "import datetime\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "\n",
    "import boto3\n",
    "from botocore.client import Config\n",
    "\n",
    "# use 95% of the screen for jupyter cell\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container {width:100% !important; }<style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3ba21e",
   "metadata": {},
   "source": [
    "### Launch Spark Jupyter and Configuration\n",
    "\n",
    "#### Configure a Spark session for Kubernetes cluster with S3 support\n",
    "### CLUSTER MANAGER\n",
    "- set the Kubernetes master URL as Cluster Manager(“k8s://https://” is NOT a typo, this is how Spark knows the “provider” type)\n",
    "\n",
    "### KUBERNETES\n",
    "- set the namespace that will be used for running the driver and executor pods\n",
    "- set the docker image from which the Worker/Exectutor pods are created\n",
    "- set the Kubernetes service account name and provide the authentication details for the service account (required to create worker pods)\n",
    "\n",
    "### SPARK\n",
    "- set the driver host and the driver port (find name of the driver service with 'kubectl get services' or in the helm chart configuration)\n",
    "- enable Delta Lake, Iceberg, and Hudi support by setting the spark.sql.extensions\n",
    "- configure Hive catalog for Iceberg\n",
    "- enable S3 connector\n",
    "- set the number of worker pods, their memory and cores (HINT: number of possible tasks = cores * executores)\n",
    "\n",
    "### SPARK SESSION\n",
    "- create the Spark session using the SparkSession.builder object\n",
    "- get the Spark context from the created session and set the log level to \"ERROR\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f9f9068",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/07/21 07:51:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.kubernetes.namespace = frontend\n",
      "spark.sql.extensions = io.delta.sql.DeltaSparkSessionExtension, org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions, org.apache.spark.sql.hudi.HoodieSparkSessionExtension\n",
      "spark.master = k8s://https://kubernetes.default.svc.cluster.local:443\n",
      "spark.executor.memory = 1G\n",
      "spark.executor.cores = 2\n",
      "spark.driver.host = jupyter-spark-driver.frontend.svc.cluster.local\n",
      "spark.app.name = jupyter-file-formats\n"
     ]
    }
   ],
   "source": [
    "appName=\"jupyter-file-formats\"\n",
    "\n",
    "conf = SparkConf()\n",
    "\n",
    "# CLUSTER MANAGER\n",
    "\n",
    "conf.setMaster(\"k8s://https://kubernetes.default.svc.cluster.local:443\")\n",
    "\n",
    "# CONFIGURE KUBERNETES\n",
    "\n",
    "conf.set(\"spark.kubernetes.namespace\",\"frontend\")\n",
    "conf.set(\"spark.kubernetes.container.image\", \"thinkportgmbh/workshops:spark-3.3.2\")\n",
    "conf.set(\"spark.kubernetes.container.image.pullPolicy\", \"Always\")\n",
    "\n",
    "conf.set(\"spark.kubernetes.authenticate.driver.serviceAccountName\", \"spark\")\n",
    "conf.set(\"spark.kubernetes.authenticate.caCertFile\", \"/var/run/secrets/kubernetes.io/serviceaccount/ca.crt\")\n",
    "conf.set(\"spark.kubernetes.authenticate.oauthTokenFile\", \"/var/run/secrets/kubernetes.io/serviceaccount/token\")\n",
    "\n",
    "# CONFIGURE SPARK\n",
    "\n",
    "conf.set(\"spark.sql.session.timeZone\", \"Europe/Berlin\")\n",
    "conf.set(\"spark.driver.host\", \"jupyter-spark-driver.frontend.svc.cluster.local\")\n",
    "conf.set(\"spark.driver.port\", \"29413\")\n",
    "\n",
    "conf.set(\"spark.jars\", \"/opt/spark/jars/spark-avro_2.12-3.3.2.jar\")\n",
    "conf.set(\"spark.driver.extraClassPath\",\"/opt/spark/jars/spark-avro_2.12-3.3.2.jar\")\n",
    "conf.set(\"spark.executor.extraClassPath\",\"/opt/spark/jars/spark-avro_2.12-3.3.2.jar\")\n",
    "\n",
    "conf.set(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension, org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions, org.apache.spark.sql.hudi.HoodieSparkSessionExtension\")\n",
    "\n",
    "######## Hive als Metastore einbinden\n",
    "#conf.set(\"hive.metastore.uris\", \"thrift://hive-metastore.hive.svc.cluster.local:9083\") \n",
    "\n",
    "######## Iceberg configs\n",
    "conf.set(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\")\n",
    "conf.set(\"spark.sql.catalog.ice\",\"org.apache.iceberg.spark.SparkCatalog\") \n",
    "conf.set(\"spark.sql.catalog.ice.type\",\"hive\") \n",
    "conf.set(\"spark.sql.catalog.ice.uri\",\"thrift://hive-metastore.hive.svc.cluster.local:9083\") \n",
    "\n",
    "####### Hudi configs\n",
    "conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "\n",
    "# CONFIGURE S3 CONNECTOR\n",
    "conf.set(\"spark.hadoop.fs.s3a.endpoint\", \"minio.minio.svc.cluster.local:9000\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.access.key\", \"trainadm\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.secret.key\", \"train@thinkport\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "conf.set(\"spark.hadoop.fs.s3.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "\n",
    "# CONFIGURE WORKER (Customize based on workload)\n",
    "\n",
    "conf.set(\"spark.executor.instances\", \"2\")\n",
    "conf.set(\"spark.executor.memory\", \"1G\")\n",
    "conf.set(\"spark.executor.cores\", \"2\")\n",
    "\n",
    "# SPARK SESSION\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .config(conf=conf) \\\n",
    "    .config('spark.sql.session.timeZone', 'Europe/Berlin') \\\n",
    "    .appName(appName)\\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "sc=spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "# get the configuration object to check all the configurations the session was startet with\n",
    "for entry in sc.getConf().getAll():\n",
    "        if entry[0] in [\"spark.app.name\",\"spark.kubernetes.namespace\",\"spark.executor.memory\",\"spark.executor.cores\",\"spark.driver.host\",\"spark.master\",\"spark.sql.extensions\"]:\n",
    "            print(entry[0],\"=\",entry[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdaa9936",
   "metadata": {},
   "source": [
    "### Configure Boto3 \n",
    "for simple s3 operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef2a9d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hilfsfunktionen um mit einfachen Befehlen auf s3 zu arbeiten \n",
    "# WICHTIG: Falls das bucket s3://fileformats noch nicht existiert muss dieses über das Terminal erst erzeugt werden\n",
    "# Command: s3 mb s3://fileformats\n",
    "\n",
    "# Bucket, muss zuerst in Minio oder via Terminal Befehl erstellt werden\n",
    "bucket = \"fileformats\"\n",
    "bucket_path=\"s3://\"+bucket\n",
    "\n",
    "options = {\n",
    "    'endpoint_url': 'http://minio.minio.svc.cluster.local:9000',\n",
    "    'aws_access_key_id': 'trainadm',\n",
    "    'aws_secret_access_key': 'train@thinkport',\n",
    "    'config': Config(signature_version='s3v4'),\n",
    "    'verify': False}\n",
    "\n",
    "s3_resource = boto3.resource('s3', **options)  \n",
    "s3_client = boto3.client('s3', **options)\n",
    "\n",
    "# show files on s3 bucket/prefix\n",
    "def ls(bucket,prefix):\n",
    "    '''List objects from bucket/prefix'''\n",
    "    try:\n",
    "        for obj in s3_resource.Bucket(bucket).objects.filter(Prefix=prefix):\n",
    "            print(obj.key)\n",
    "    except Exception as e: \n",
    "        print(e)\n",
    "    \n",
    "# show file content in files\n",
    "def cat(bucket,prefix,binary=False):\n",
    "    '''Show content of one or several files with same prefix/wildcard'''\n",
    "    try:\n",
    "        for obj in s3_resource.Bucket(bucket).objects.filter(Prefix=prefix):\n",
    "            print(\"File:\",obj.key)\n",
    "            print(\"----------------------\")\n",
    "            if binary==True:\n",
    "                print(obj.get()['Body'].read())\n",
    "            else: \n",
    "                print(obj.get()['Body'].read().decode())\n",
    "            print(\"######################\")\n",
    "    except Exception as e: \n",
    "        print(e)\n",
    "\n",
    "# delete files from bucket\n",
    "def rm(bucket,prefix):\n",
    "    '''Delete everything from bucket/prefix'''\n",
    "    for object in s3_resource.Bucket(bucket).objects.filter(Prefix=prefix):\n",
    "        print(object.key)\n",
    "        s3_client.delete_object(Bucket=bucket, Key=object.key)\n",
    "    print(f\"Deleted files from {bucket}/{prefix}*\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0328d660",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rm(bucket,'delta_scd2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04cc004e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iceberg/data/00000-7912-ace0e8c1-fb4a-4812-bdf3-6d4c0ad58a06-00001.parquet\n",
      "iceberg/data/00000-7920-9dd85db3-402a-4657-856c-f0e8526d3256-00001.parquet\n",
      "iceberg/data/00001-7913-df5b342a-250c-4439-a721-bf25810804f3-00001.parquet\n",
      "iceberg/data/00002-7914-112c31b7-0460-4fed-b4c9-8b9a5f1dbfd5-00001.parquet\n",
      "iceberg/metadata/00000-d4590277-2b88-433d-aa9f-8ba595501aef.metadata.json\n",
      "iceberg/metadata/00001-fb578df1-af09-4ce8-854a-d66befc4519b.metadata.json\n",
      "iceberg/metadata/00002-becfce86-8178-4223-8c6d-84baaf06b31b.metadata.json\n",
      "iceberg/metadata/0a07116f-9521-4d1d-bc4a-3a729535f621-m0.avro\n",
      "iceberg/metadata/4d380afb-46c9-4d4c-a877-135bf0ec57a9-m0.avro\n",
      "iceberg/metadata/snap-2832191032484601030-1-0a07116f-9521-4d1d-bc4a-3a729535f621.avro\n",
      "iceberg/metadata/snap-868766759905674702-1-4d380afb-46c9-4d4c-a877-135bf0ec57a9.avro\n",
      "All Folders #############################\n",
      "CSV Details #############################\n",
      "Delta Details #############################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show everything in bucket\n",
    "ls(bucket,\"\")\n",
    "print(\"All Folders #############################\")\n",
    "# show folder\n",
    "ls(bucket,\"csv\")\n",
    "print(\"CSV Details #############################\")\n",
    "# show subfolder\n",
    "ls(bucket,\"delta/_delta_log/\")\n",
    "print(\"Delta Details #############################\")\n",
    "print(\"\")\n",
    "# show content of one or several files with same prefix/wildcard\n",
    "cat(bucket,'csv/part')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27495f26",
   "metadata": {},
   "source": [
    "### Create sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1b8b7c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++ create new dataframe and show schema and data\n",
      "################################################\n",
      "++ start data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------+-------+\n",
      "|id |account|dt_transaction|balance|\n",
      "+---+-------+--------------+-------+\n",
      "|4  |maria  |2020-01-01    |5000   |\n",
      "|1  |alex   |2019-01-01    |1000   |\n",
      "|2  |alex   |2019-02-01    |1500   |\n",
      "|3  |alex   |2019-03-01    |1700   |\n",
      "+---+-------+--------------+-------+\n",
      "\n",
      "++ update row and add row\n",
      "+---+-------+--------------+-------+\n",
      "|id |account|dt_transaction|balance|\n",
      "+---+-------+--------------+-------+\n",
      "|2  |peter  |2021-01-01    |100    |\n",
      "|1  |alex   |2019-03-01    |3300   |\n",
      "+---+-------+--------------+-------+\n",
      "\n",
      "++ add new column\n",
      "+---+-------+--------------+-------+-------------+\n",
      "|id |account|dt_transaction|balance|new          |\n",
      "+---+-------+--------------+-------+-------------+\n",
      "|1  |otto   |2019-10-01    |4444   |neue Spalte 1|\n",
      "+---+-------+--------------+-------+-------------+\n",
      "\n",
      "++ add new row with wrong schema (id)\n",
      "+---+-------+--------------+-------+\n",
      "|id |account|dt_transaction|balance|\n",
      "+---+-------+--------------+-------+\n",
      "|5  |markus |2019-09-01    |555    |\n",
      "+---+-------+--------------+-------+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- account: string (nullable = true)\n",
      " |-- dt_transaction: date (nullable = true)\n",
      " |-- balance: long (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- account: string (nullable = true)\n",
      " |-- dt_transaction: date (nullable = true)\n",
      " |-- balance: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# initial Daten\n",
    "account_data1 = [\n",
    "    (1,\"alex\",\"2019-01-01\",1000),\n",
    "    (2,\"alex\",\"2019-02-01\",1500),\n",
    "    (3,\"alex\",\"2019-03-01\",1700),\n",
    "    (4,\"maria\",\"2020-01-01\",5000)\n",
    "    ]\n",
    "\n",
    "# Datensatz mit einem Update und einer neuen Zeile\n",
    "account_data2 = [\n",
    "    (1,\"alex\",\"2019-03-01\",3300),\n",
    "    (2,\"peter\",\"2021-01-01\",100)\n",
    "    ]\n",
    "\n",
    "# Datensatz mit neuer Zeile und neuer Spalte\n",
    "account_data3 = [\n",
    "    (1,\"otto\",\"2019-10-01\",4444,\"neue Spalte 1\")\n",
    "]\n",
    "\n",
    "# Datensatz mit neuer Zeile und neuer Spalte\n",
    "account_data4 = [\n",
    "    (5,\"markus\",\"2019-09-01\",555)\n",
    "]\n",
    "\n",
    "schema = [\"id\",\"account\",\"dt_transaction\",\"balance\"]\n",
    "schema3 = [\"id\",\"account\",\"dt_transaction\",\"balance\",\"new\"]\n",
    "\n",
    "df1 = spark.createDataFrame(data=account_data1, schema = schema).withColumn(\"dt_transaction\",f.col(\"dt_transaction\").cast(\"date\")).repartition(3)\n",
    "df2 = spark.createDataFrame(data=account_data2, schema = schema).withColumn(\"dt_transaction\",f.col(\"dt_transaction\").cast(\"date\")).repartition(2)\n",
    "df3 = spark.createDataFrame(data=account_data3, schema = schema3).withColumn(\"dt_transaction\",f.col(\"dt_transaction\").cast(\"date\")).repartition(1)\n",
    "df4 = spark.createDataFrame(data=account_data4, schema = schema).withColumn(\"dt_transaction\",f.col(\"dt_transaction\").cast(\"date\")).withColumn(\"id\",f.col(\"id\").cast(\"string\")).repartition(1)\n",
    "\n",
    "\n",
    "print(\"++ create new dataframe and show schema and data\")\n",
    "print(\"################################################\")\n",
    "\n",
    "# df1.printSchema()\n",
    "print(\"++ start data\")\n",
    "df1.show(truncate=False)\n",
    "print(\"++ update row and add row\")\n",
    "df2.show(truncate=False)\n",
    "print(\"++ add new column\")\n",
    "df3.show(truncate=False)\n",
    "print(\"++ add new row with wrong schema (id)\")\n",
    "df4.show(truncate=False)\n",
    "df1.printSchema()\n",
    "df4.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf0c3d1",
   "metadata": {},
   "source": [
    "<hr style=\"height: 3px; background: gray;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6fe705",
   "metadata": {},
   "source": [
    "## CSV\n",
    "\n",
    "### Aufgabe:\n",
    "Schreibe die Daten als CSV mit der Overwrite und Append Funktion\n",
    "\n",
    "1. Datenset 1 als csv schreiben (.format(\"csv\") und Pfad= .save(f\"s3://{bucket}/csv\"))\n",
    "2. Dateien und Inhalt anzeigen, vestehen was da passiert ist\n",
    "3. Daten wieder einlese und checken ob es ein Schema und Spaltennamen erhalten wurden\n",
    "4. Datenset 3 anfügen mit weiterer Spalte anfügen (append)\n",
    "5. Daten wieder einlesen und checken was mit der neuen Spalte passiert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "916ad0c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Partitions: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"Number of Partitions:\", df1.rdd.getNumPartitions())\n",
    "\n",
    "# Schreibe Datenset 1 als CSV Datei\n",
    "write_csv=(df1\n",
    "           .write\n",
    "           .format(\"csv\")\n",
    "           .mode(\"overwrite\") # append\n",
    "           .save(f\"s3://{bucket}/csv\")\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cebbc9fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv/_SUCCESS\n",
      "csv/part-00000-40f490ff-8cfe-404b-808e-5c2372751fb9-c000.csv\n",
      "csv/part-00001-40f490ff-8cfe-404b-808e-5c2372751fb9-c000.csv\n",
      "csv/part-00002-40f490ff-8cfe-404b-808e-5c2372751fb9-c000.csv\n"
     ]
    }
   ],
   "source": [
    "# Anzeigen der Dateien im Bucket/Prefix\n",
    "ls(bucket,\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82fb51b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: csv/part-00000-40f490ff-8cfe-404b-808e-5c2372751fb9-c000.csv\n",
      "----------------------\n",
      "1,alex,2019-01-01,1000\n",
      "4,maria,2020-01-01,5000\n",
      "\n",
      "######################\n",
      "File: csv/part-00001-40f490ff-8cfe-404b-808e-5c2372751fb9-c000.csv\n",
      "----------------------\n",
      "2,alex,2019-02-01,1500\n",
      "\n",
      "######################\n",
      "File: csv/part-00002-40f490ff-8cfe-404b-808e-5c2372751fb9-c000.csv\n",
      "----------------------\n",
      "3,alex,2019-03-01,1700\n",
      "\n",
      "######################\n"
     ]
    }
   ],
   "source": [
    "# Anzeigen der Inhalte jeder Datei im Bucket/Prefix\n",
    "cat(bucket,\"csv/part\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f31d31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 30:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----------+----+\n",
      "|_c0|  _c1|       _c2| _c3|\n",
      "+---+-----+----------+----+\n",
      "|  1| alex|2019-01-01|1000|\n",
      "|  4|maria|2020-01-01|5000|\n",
      "|  2| alex|2019-02-01|1500|\n",
      "|  3| alex|2019-03-01|1700|\n",
      "+---+-----+----------+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# lese die csv Datei wieder ein und prüfe das Schema\n",
    "read_csv=spark.read.format(\"csv\").load(f\"s3://{bucket}/csv\")\n",
    "\n",
    "read_csv.printSchema()\n",
    "read_csv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c5a5979",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# schreibe Datenset 3 (neue Spalte) in die gleiche Tabelle dazu\n",
    "write_csv=(df3\n",
    "           .write\n",
    "           .format(\"csv\")\n",
    "           .mode(\"append\")\n",
    "           .save(f\"s3://{bucket}/csv\")\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a771362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv/_SUCCESS\n",
      "csv/part-00000-40f490ff-8cfe-404b-808e-5c2372751fb9-c000.csv\n",
      "csv/part-00000-4ad5fd72-1534-41a4-8f87-f5eba25d4a99-c000.csv\n",
      "csv/part-00001-40f490ff-8cfe-404b-808e-5c2372751fb9-c000.csv\n",
      "csv/part-00002-40f490ff-8cfe-404b-808e-5c2372751fb9-c000.csv\n"
     ]
    }
   ],
   "source": [
    "# Anzeigen der Inhalte jeder Datei im Bucket/Prefix\n",
    "ls(bucket,\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "349331cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: csv/part-00000-40f490ff-8cfe-404b-808e-5c2372751fb9-c000.csv\n",
      "----------------------\n",
      "1,alex,2019-01-01,1000\n",
      "4,maria,2020-01-01,5000\n",
      "\n",
      "######################\n",
      "File: csv/part-00000-4ad5fd72-1534-41a4-8f87-f5eba25d4a99-c000.csv\n",
      "----------------------\n",
      "1,otto,2019-10-01,4444,neue Spalte 1\n",
      "\n",
      "######################\n",
      "File: csv/part-00001-40f490ff-8cfe-404b-808e-5c2372751fb9-c000.csv\n",
      "----------------------\n",
      "2,alex,2019-02-01,1500\n",
      "\n",
      "######################\n",
      "File: csv/part-00002-40f490ff-8cfe-404b-808e-5c2372751fb9-c000.csv\n",
      "----------------------\n",
      "3,alex,2019-03-01,1700\n",
      "\n",
      "######################\n"
     ]
    }
   ],
   "source": [
    "# Anzeigen der Inhalte jeder Datei im Bucket/Prefix\n",
    "cat(bucket,\"csv/part\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "31fd06f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      "\n",
      "+---+-----+----------+----+\n",
      "|_c0|  _c1|       _c2| _c3|\n",
      "+---+-----+----------+----+\n",
      "|  1| alex|2019-01-01|1000|\n",
      "|  4|maria|2020-01-01|5000|\n",
      "|  1| otto|2019-10-01|4444|\n",
      "|  2| alex|2019-02-01|1500|\n",
      "|  3| alex|2019-03-01|1700|\n",
      "+---+-----+----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# und lese alles nochmal ein um zu schauen ob die neue Spalte richtig erkannt wurde\n",
    "read_csv=spark.read.format(\"csv\").load(f\"s3://{bucket}/csv\")\n",
    "\n",
    "read_csv.printSchema()\n",
    "read_csv.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1b3038",
   "metadata": {},
   "source": [
    "#### Erkenntnisse CSV\n",
    "* In wieviele Dateien wird das Datenset aufgeteilt und warum?\n",
    "* Bleibt das Schema erhalten (Selbsterklärend)\n",
    "* Können neue Spalten angefügt werden (Schema Evolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148b0337",
   "metadata": {},
   "source": [
    "<hr style=\"height: 3px; background: gray;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55bcbfa",
   "metadata": {},
   "source": [
    "## JSON\n",
    "\n",
    "### Aufgabe:\n",
    "Wiederhole die gleichen Schritte mit dem JSON Format und schaue wie sich hier Schema und neue Spalten verhalten\n",
    "\n",
    "1. Datenset 1 als json schreiben (.format(\"json\") und Pfad= .save(f\"s3://{bucket}/json\"))\n",
    "2. Dateien und Inhalt anzeigen, vestehen was da passiert ist\n",
    "3. Daten wieder einlese und checken ob es ein Schema und Spaltennamen gibt\n",
    "4. Datenset 3 anfügen (append)\n",
    "5. Daten wieder einlesen und checken was mit der neuen Spalte passiert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e368413b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Partitions: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"Number of Partitions:\", df1.rdd.getNumPartitions())\n",
    "\n",
    "# Schreibe Datenset 1 als JSON Datei\n",
    "write_json=(df1\n",
    "           .write\n",
    "           .format(\"json\")\n",
    "           .mode(\"overwrite\") # append\n",
    "           .save(f\"s3://{bucket}/json\")\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23edb01",
   "metadata": {},
   "source": [
    "<details style=\"border: 1px solid #aaa; border-radius: 4px; padding: 0.5em 0.5em 0; background-color:#F5F5F5\" class=\"solution\" >\n",
    "<summary style=\"margin: -0.5em -0.5em 0; padding: 0.5em;\"></summary>\n",
    "Lösung:<br>\n",
    "<code>write_json=(df1\n",
    "           .write\n",
    "           .format(\"json\")\n",
    "           .mode(\"overwrite\") # append\n",
    "           .save(f\"s3://{bucket}/json\")\n",
    "          )</code>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "da9898f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "json/_SUCCESS\n",
      "json/part-00000-0db0c2eb-c91f-482e-a6bf-22043ff52fb6-c000.json\n",
      "json/part-00001-0db0c2eb-c91f-482e-a6bf-22043ff52fb6-c000.json\n",
      "json/part-00002-0db0c2eb-c91f-482e-a6bf-22043ff52fb6-c000.json\n"
     ]
    }
   ],
   "source": [
    "ls(bucket,\"json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3ba7f5c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: json/part-00000-0db0c2eb-c91f-482e-a6bf-22043ff52fb6-c000.json\n",
      "----------------------\n",
      "{\"id\":1,\"account\":\"alex\",\"dt_transaction\":\"2019-01-01\",\"balance\":1000}\n",
      "{\"id\":4,\"account\":\"maria\",\"dt_transaction\":\"2020-01-01\",\"balance\":5000}\n",
      "\n",
      "######################\n",
      "File: json/part-00001-0db0c2eb-c91f-482e-a6bf-22043ff52fb6-c000.json\n",
      "----------------------\n",
      "{\"id\":2,\"account\":\"alex\",\"dt_transaction\":\"2019-02-01\",\"balance\":1500}\n",
      "\n",
      "######################\n",
      "File: json/part-00002-0db0c2eb-c91f-482e-a6bf-22043ff52fb6-c000.json\n",
      "----------------------\n",
      "{\"id\":3,\"account\":\"alex\",\"dt_transaction\":\"2019-03-01\",\"balance\":1700}\n",
      "\n",
      "######################\n"
     ]
    }
   ],
   "source": [
    "cat(bucket,\"json/part\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0fe6715f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- account: string (nullable = true)\n",
      " |-- balance: long (nullable = true)\n",
      " |-- dt_transaction: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      "\n",
      "+-------+-------+--------------+---+\n",
      "|account|balance|dt_transaction| id|\n",
      "+-------+-------+--------------+---+\n",
      "|   alex|   1000|    2019-01-01|  1|\n",
      "|  maria|   5000|    2020-01-01|  4|\n",
      "|   alex|   1500|    2019-02-01|  2|\n",
      "|   alex|   1700|    2019-03-01|  3|\n",
      "+-------+-------+--------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Daten wieder einlese und checken ob es ein Schema und Spaltennamen gibt\n",
    "read_json=spark.read.format(\"json\").load(f\"s3://{bucket}/json\")\n",
    "\n",
    "read_json.printSchema()\n",
    "read_json.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7506f71a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# schreibe Datenset 3 (neue Spalte) in die gleiche Tabelle dazu (!! append NOT overwrite)\n",
    "\n",
    "write_json=(df3\n",
    "           .write\n",
    "           .format(\"json\")\n",
    "           .mode(\"append\") # append\n",
    "           .save(f\"s3://{bucket}/json\")\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046d241e",
   "metadata": {},
   "source": [
    "<details style=\"border: 1px solid #aaa; border-radius: 4px; padding: 0.5em 0.5em 0; background-color:#F5F5F5\" class=\"solution\" >\n",
    "<summary style=\"margin: -0.5em -0.5em 0; padding: 0.5em;\"></summary>\n",
    "Lösung:<br>\n",
    "<code>\n",
    "    <p>\n",
    "    write_json=(df3\n",
    "           .write\n",
    "           .format(\"json\")\n",
    "           .mode(\"append\") # append\n",
    "           .save(f\"s3://{bucket}/json\")\n",
    "          )\n",
    "    </p>\n",
    "</code>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ba940d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- account: string (nullable = true)\n",
      " |-- balance: long (nullable = true)\n",
      " |-- dt_transaction: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- new: string (nullable = true)\n",
      "\n",
      "+-------+-------+--------------+---+-------------+\n",
      "|account|balance|dt_transaction| id|          new|\n",
      "+-------+-------+--------------+---+-------------+\n",
      "|   alex|   1000|    2019-01-01|  1|         null|\n",
      "|  maria|   5000|    2020-01-01|  4|         null|\n",
      "|   otto|   4444|    2019-10-01|  1|neue Spalte 1|\n",
      "|   alex|   1500|    2019-02-01|  2|         null|\n",
      "|   alex|   1700|    2019-03-01|  3|         null|\n",
      "+-------+-------+--------------+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# alles nochmal einlesen und schauen ob die neue Spalte und die Schemas richtig erkannt wurden\n",
    "\n",
    "read_json=spark.read.format(\"json\").load(f\"s3://{bucket}/json\")\n",
    "\n",
    "read_json.printSchema()\n",
    "read_json.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd549ced",
   "metadata": {},
   "source": [
    "<details style=\"border: 1px solid #aaa; border-radius: 4px; padding: 0.5em 0.5em 0; background-color:#F5F5F5\" class=\"solution\" >\n",
    "<summary style=\"margin: -0.5em -0.5em 0; padding: 0.5em;\"></summary>\n",
    "Lösung:<br>\n",
    "<code>\n",
    "read_json=spark.read.format(\"json\").load(f\"s3://{bucket}/json\")\n",
    "read_json.printSchema()\n",
    "read_json.show()\n",
    "    </code>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad580792",
   "metadata": {},
   "source": [
    "#### Erkenntnisse JSON\n",
    "* Bleibt das Schema erhalten (Selbsterklärend)?\n",
    "* Können neue Spalten angefügt werden (Schema Evolution)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094e8bb0",
   "metadata": {},
   "source": [
    "## AVRO\n",
    "Avro ist ein Zeilenformat was für das schnelle Schreiben im Streaming Kontext optimiert ist.\n",
    "Avro ist selbsterklärend, hat ein Schema und unterstützt Schema Evolution\n",
    "\n",
    "### Aufgabe:\n",
    "Wiederhole die gleichen Schritte mit dem AVRO Format und schaue wie sich hier Schema und neue Spalten verhalten\n",
    "\n",
    "1. Datenset 1 als avro schreiben (.format(\"avro\") und Pfad= .save(f\"s3://{bucket}/avro\"))\n",
    "2. Dateien und Inhalt anzeigen, vestehen was da passiert ist\n",
    "3. Metadaten in Datei identifizieren\n",
    "3. Daten wieder einlese und checken ob es ein Schema und Spaltennamen gibt\n",
    "4. Schema Evolutiuon: Datenset 3 anfügen mit neuer Spalte anfügen\n",
    "5. Daten wieder einlesen und checken was mit der neuen Spalte passiert\n",
    "6. Schema Enforcement: Datentyp in bestehender Spalte ändern und schauen ob und wie dies gehandhabt wird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "44777375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Partitions: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"Number of Partitions:\", df1.rdd.getNumPartitions())\n",
    "# Schreibe Datenset 1 als AVRO Datei\n",
    "write_avro=(df1\n",
    "           .write\n",
    "           .format(\"avro\")\n",
    "           .mode(\"overwrite\") # append\n",
    "           .save(f\"s3://{bucket}/avro\")\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c89ea1",
   "metadata": {},
   "source": [
    "<details style=\"border: 1px solid #aaa; border-radius: 4px; padding: 0.5em 0.5em 0; background-color:#F5F5F5\" class=\"solution\" >\n",
    "<summary style=\"margin: -0.5em -0.5em 0; padding: 0.5em;\"></summary>\n",
    "Lösung:<br>\n",
    "<code>write_avro=(df1\n",
    "           .write\n",
    "           .format(\"avro\")\n",
    "           .mode(\"overwrite\") # append\n",
    "           .save(f\"s3://{bucket}/avro\")\n",
    "          )</code>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a7ebc8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avro/_SUCCESS\n",
      "avro/part-00000-afd8998c-ebe8-4d8d-b740-8e8e6f6fd1de-c000.avro\n",
      "avro/part-00001-afd8998c-ebe8-4d8d-b740-8e8e6f6fd1de-c000.avro\n",
      "avro/part-00002-afd8998c-ebe8-4d8d-b740-8e8e6f6fd1de-c000.avro\n"
     ]
    }
   ],
   "source": [
    "# sind die Daten auf s3 angekommen\n",
    "ls(bucket,\"avro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d82f2e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: avro/part-00000-afd8998c-ebe8-4d8d-b740-8e8e6f6fd1de-c000.avro\n",
      "----------------------\n",
      "b'Obj\\x01\\x06\\x16avro.schema\\xfa\\x03{\"type\":\"record\",\"name\":\"topLevelRecord\",\"fields\":[{\"name\":\"id\",\"type\":[\"long\",\"null\"]},{\"name\":\"account\",\"type\":[\"string\",\"null\"]},{\"name\":\"dt_transaction\",\"type\":[{\"type\":\"int\",\"logicalType\":\"date\"},\"null\"]},{\"name\":\"balance\",\"type\":[\"long\",\"null\"]}]}0org.apache.spark.version\\n3.3.2\\x14avro.codec\\x0csnappy\\x00R\\xfd<\\x14\\x967pE2\\xce\\x10n\\x03\\x18\\xee[\\x04J\\x1fx\\x00\\x02\\x00\\x08alex\\x00\\xd2\\x97\\x02\\x00\\xd0\\x0f\\x00\\x08\\x00\\nmaria\\x00\\xac\\x9d\\x02\\x00\\x90N\\x9d\\xdfwVR\\xfd<\\x14\\x967pE2\\xce\\x10n\\x03\\x18\\xee['\n",
      "######################\n",
      "File: avro/part-00001-afd8998c-ebe8-4d8d-b740-8e8e6f6fd1de-c000.avro\n",
      "----------------------\n",
      "b'Obj\\x01\\x06\\x16avro.schema\\xfa\\x03{\"type\":\"record\",\"name\":\"topLevelRecord\",\"fields\":[{\"name\":\"id\",\"type\":[\"long\",\"null\"]},{\"name\":\"account\",\"type\":[\"string\",\"null\"]},{\"name\":\"dt_transaction\",\"type\":[{\"type\":\"int\",\"logicalType\":\"date\"},\"null\"]},{\"name\":\"balance\",\"type\":[\"long\",\"null\"]}]}0org.apache.spark.version\\n3.3.2\\x14avro.codec\\x0csnappy\\x00\\xa6\\\\\\x02\\x93sGf\\xa4Kt\\x94\\xafa\\x01\\xb1m\\x02*\\x0f8\\x00\\x04\\x00\\x08alex\\x00\\x90\\x98\\x02\\x00\\xb8\\x17\\x02e\\n\\xa4\\xa6\\\\\\x02\\x93sGf\\xa4Kt\\x94\\xafa\\x01\\xb1m'\n",
      "######################\n",
      "File: avro/part-00002-afd8998c-ebe8-4d8d-b740-8e8e6f6fd1de-c000.avro\n",
      "----------------------\n",
      "b'Obj\\x01\\x06\\x16avro.schema\\xfa\\x03{\"type\":\"record\",\"name\":\"topLevelRecord\",\"fields\":[{\"name\":\"id\",\"type\":[\"long\",\"null\"]},{\"name\":\"account\",\"type\":[\"string\",\"null\"]},{\"name\":\"dt_transaction\",\"type\":[{\"type\":\"int\",\"logicalType\":\"date\"},\"null\"]},{\"name\":\"balance\",\"type\":[\"long\",\"null\"]}]}0org.apache.spark.version\\n3.3.2\\x14avro.codec\\x0csnappy\\x009\\x9c\\x15N\\r\\xc2S\\xab#\\xfb\\xbe\\x82vu\\xa9p\\x02*\\x0f8\\x00\\x06\\x00\\x08alex\\x00\\xc8\\x98\\x02\\x00\\xc8\\x1aRXA\\xd89\\x9c\\x15N\\r\\xc2S\\xab#\\xfb\\xbe\\x82vu\\xa9p'\n",
      "######################\n"
     ]
    }
   ],
   "source": [
    "# Finde in der Darstellung der Datei die Metadaten und die eigentlichen Daten\n",
    "# Da Avro ein Binärformat ist muss hier in cat die Flag auf True gesetzt werden\n",
    "cat(bucket,\"avro/part\",True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8ac72f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- account: string (nullable = true)\n",
      " |-- dt_transaction: date (nullable = true)\n",
      " |-- balance: long (nullable = true)\n",
      "\n",
      "+---+-------+--------------+-------+\n",
      "| id|account|dt_transaction|balance|\n",
      "+---+-------+--------------+-------+\n",
      "|  1|   alex|    2019-01-01|   1000|\n",
      "|  4|  maria|    2020-01-01|   5000|\n",
      "|  2|   alex|    2019-02-01|   1500|\n",
      "|  3|   alex|    2019-03-01|   1700|\n",
      "+---+-------+--------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "read_avro=spark.read.format(\"avro\").load(f\"s3://{bucket}/avro\")\n",
    "\n",
    "\n",
    "read_avro.printSchema()\n",
    "read_avro.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3e6e1b",
   "metadata": {},
   "source": [
    "### Avro: Schema Evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1b1e0ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# schreibe Datenset 3 (neue Spalte) in die gleiche Tabelle dazu (!! append NOT overwrite)\n",
    "\n",
    "write_avro=(df3\n",
    "           .write\n",
    "           .format(\"avro\")\n",
    "           .mode(\"append\") # append\n",
    "           .save(f\"s3://{bucket}/avro\")\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d9a03f",
   "metadata": {},
   "source": [
    "<details style=\"border: 1px solid #aaa; border-radius: 4px; padding: 0.5em 0.5em 0; background-color:#F5F5F5\" class=\"solution\" >\n",
    "<summary style=\"margin: -0.5em -0.5em 0; padding: 0.5em;\"></summary>\n",
    "Lösung:<br>\n",
    "<code>write_avro=(df3\n",
    "           .write\n",
    "           .format(\"avro\")\n",
    "           .mode(\"append\") # append\n",
    "           .save(f\"s3://{bucket}/avro\")\n",
    "          )</code>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "70570799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- account: string (nullable = true)\n",
      " |-- dt_transaction: date (nullable = true)\n",
      " |-- balance: long (nullable = true)\n",
      " |-- new: string (nullable = true)\n",
      "\n",
      "+---+-------+--------------+-------+-------------+\n",
      "| id|account|dt_transaction|balance|          new|\n",
      "+---+-------+--------------+-------+-------------+\n",
      "|  1|   otto|    2019-10-01|   4444|neue Spalte 1|\n",
      "|  1|   alex|    2019-01-01|   1000|         null|\n",
      "|  4|  maria|    2020-01-01|   5000|         null|\n",
      "|  2|   alex|    2019-02-01|   1500|         null|\n",
      "|  3|   alex|    2019-03-01|   1700|         null|\n",
      "+---+-------+--------------+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# alles nochmal einlesen und schauen ob die neue Spalte und die Schemas richtig erkannt wurden\n",
    "# wiederholt sich langsam gell?\n",
    "\n",
    "read_avro=spark.read.format(\"avro\").load(f\"s3://{bucket}/avro\")\n",
    "\n",
    "\n",
    "read_avro.printSchema()\n",
    "read_avro.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e98de2e",
   "metadata": {},
   "source": [
    "<details style=\"border: 1px solid #aaa; border-radius: 4px; padding: 0.5em 0.5em 0; background-color:#F5F5F5\" class=\"solution\" >\n",
    "<summary style=\"margin: -0.5em -0.5em 0; padding: 0.5em;\"></summary>\n",
    "Lösung AVRO wieder einlesen:<br>\n",
    "<code>\n",
    "read_avro=spark.read.format(\"avro\").load(f\"s3://{bucket}/json\")\n",
    "read_avro.printSchema()\n",
    "read_avro.show()\n",
    "    </code>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c32e83",
   "metadata": {},
   "source": [
    "### Avro: Schema Enforcement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "572f0409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema vorher:\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- account: string (nullable = true)\n",
      " |-- dt_transaction: date (nullable = true)\n",
      " |-- balance: long (nullable = true)\n",
      "\n",
      "Schema nachher:\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- account: string (nullable = true)\n",
      " |-- dt_transaction: date (nullable = true)\n",
      " |-- balance: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Füge eine Zeile (df2) zu der AVRO Tabelle hinzu aber ändere den Datentyp für die id von long zu string\n",
    "print(\"Schema vorher:\")\n",
    "df2.printSchema()\n",
    "\n",
    "\n",
    "df2a=(df2\n",
    "      # nur die Zeile Peter aus df2\n",
    "      .where(f.col(\"account\")==\"peter\")\n",
    "      # ID als string statt als long\n",
    "      .withColumn(\"id\", f.col(\"id\").cast(\"int\"))\n",
    "     )\n",
    "\n",
    "print(\"Schema nachher:\")\n",
    "df2a.printSchema()\n",
    "\n",
    "\n",
    "write_avro=(df2a\n",
    "            .write\n",
    "            .format(\"avro\")\n",
    "            .mode(\"append\")\n",
    "            .save(f\"s3://{bucket}/avro\")\n",
    "           )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c2d39ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- account: string (nullable = true)\n",
      " |-- dt_transaction: date (nullable = true)\n",
      " |-- balance: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 62:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/07/21 07:57:12 ERROR TaskSetManager: Task 0 in stage 62.0 failed 4 times; aborting job\n",
      "++ filter in Spark Error:\n",
      "apache.spark.sql.avro.IncompatibleSchemaException: Cannot convert Avro field 'id' to SQL field 'id' because schema is incompatible (avroType = \"long\", sqlType = INT)\n",
      "\tat\n",
      "Schema enforcement on read\n"
     ]
    }
   ],
   "source": [
    "# probiere das Verzeichnis jetzt mit verschiedenen Datentypen einzulesen\n",
    "read_avro=(spark\n",
    "               .read\n",
    "               .format(\"avro\")\n",
    "               .load(f\"s3://{bucket}/avro\"))\n",
    "read_avro.printSchema()\n",
    "\n",
    "try:\n",
    "    read_avro.show()\n",
    "except Exception as error:\n",
    "    error_str=str(error)\n",
    "    search=\"Cannot convert Avro field 'id' to SQL field\"\n",
    "    print(\"++ filter in Spark Error:\")\n",
    "    print(error_str[error_str.find(search)-51:error_str.find(search)+118])\n",
    "    print(\"Schema enforcement on read\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b1054d",
   "metadata": {},
   "source": [
    "#### Erkenntnisse AVRO\n",
    "* Werden Spaltennamen erhalten? \n",
    "* Gibt es ein Schema?\n",
    "* Schema Evolution: Kann das Schema erweitert werden, also eine neue Spalte angefügt werden?\n",
    "* Schema Enforcement on write: Kann eine Spalte mit falschem Datetyp einfach beim schreiben hinzugefügt werden? \n",
    "* Schema Enforcement on read: Kann ein Verzeichnis mit mehreren Avro Dateien bei der eine Spalte ein anderes Schema hat gelesen werden?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cd7f3e",
   "metadata": {},
   "source": [
    "<hr style=\"height: 3px; background: gray;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45de295",
   "metadata": {},
   "source": [
    "## Parquet\n",
    "\n",
    "### Aufgabe:\n",
    "Wiederhole die gleichen Schritte mit dem PARQUET Format und schaue wie sich hier Schema und neue Spalten verhalten\n",
    "\n",
    "1. Datenset 1 als parquet schreiben (.format(\"parquet\") und Pfad= .save(f\"s3://{bucket}/parquet\"))\n",
    "2. Dateien und Inhalt anzeigen, vestehen was da passiert ist\n",
    "3. Metadaten in Datei identifizieren\n",
    "3. Daten wieder einlese und checken ob es ein Schema und Spaltennamen gibt\n",
    "4. Schema Evolutiuon: Datenset 3 anfügen mit neuer Spalte anfügen\n",
    "5. Daten wieder einlesen und checken was mit der neuen Spalte passiert\n",
    "6. Partion & Pushdown Filter: Execution Plan für verschiedene Filter anzeigen\n",
    "6. Schema Enforcement: Datentyp in bestehender Spalte ändern und schauen ob und wie dies gehandhabt wird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fb546000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Partitions: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"Number of Partitions:\", df1.rdd.getNumPartitions())\n",
    "\n",
    "write_parquet=(df1\n",
    "           .write\n",
    "           # Fachliche Partitionierung beim Schreiben\n",
    "           .partitionBy(\"account\")\n",
    "           .format(\"parquet\")\n",
    "           .mode(\"overwrite\")\n",
    "           .save(f\"s3://{bucket}/parquet\")\n",
    "          )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6c9e08a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parquet/_SUCCESS\n",
      "parquet/account=alex/part-00000-8ac618ae-a8d0-412c-8be3-a918e3f9b0d2.c000.snappy.parquet\n",
      "parquet/account=alex/part-00001-8ac618ae-a8d0-412c-8be3-a918e3f9b0d2.c000.snappy.parquet\n",
      "parquet/account=alex/part-00002-8ac618ae-a8d0-412c-8be3-a918e3f9b0d2.c000.snappy.parquet\n",
      "parquet/account=maria/part-00000-8ac618ae-a8d0-412c-8be3-a918e3f9b0d2.c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "# sind die Daten auf s3 angekommen\n",
    "ls(bucket,\"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "39f5468e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: parquet/_SUCCESS\n",
      "----------------------\n",
      "b''\n",
      "######################\n",
      "File: parquet/account=alex/part-00000-8ac618ae-a8d0-412c-8be3-a918e3f9b0d2.c000.snappy.parquet\n",
      "----------------------\n",
      "b'PAR1\\x15\\x00\\x15\\x1c\\x15 \\x15\\xfe\\xc8\\xd0\\xb6\\x0b\\x1c\\x15\\x02\\x15\\x00\\x15\\x06\\x15\\x08\\x00\\x00\\x0e4\\x02\\x00\\x00\\x00\\x03\\x01\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x15\\x00\\x15\\x14\\x15\\x18\\x15\\xbd\\x9c\\xff\\xa7\\x07\\x1c\\x15\\x02\\x15\\x00\\x15\\x06\\x15\\x08\\x00\\x00\\n$\\x02\\x00\\x00\\x00\\x03\\x01\\xe9E\\x00\\x00\\x15\\x00\\x15\\x1c\\x15 \\x15\\xaf\\xa4\\x96\\x9e\\x01\\x1c\\x15\\x02\\x15\\x00\\x15\\x06\\x15\\x08\\x00\\x00\\x0e4\\x02\\x00\\x00\\x00\\x03\\x01\\xe8\\x03\\x00\\x00\\x00\\x00\\x00\\x00\\x19\\x11\\x02\\x19\\x18\\x08\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x19\\x18\\x08\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x15\\x02\\x19\\x16\\x00\\x00\\x19\\x11\\x02\\x19\\x18\\x04\\xe9E\\x00\\x00\\x19\\x18\\x04\\xe9E\\x00\\x00\\x15\\x02\\x19\\x16\\x00\\x00\\x19\\x11\\x02\\x19\\x18\\x08\\xe8\\x03\\x00\\x00\\x00\\x00\\x00\\x00\\x19\\x18\\x08\\xe8\\x03\\x00\\x00\\x00\\x00\\x00\\x00\\x15\\x02\\x19\\x16\\x00\\x00\\x19\\x1c\\x16\\x08\\x15N\\x16\\x00\\x00\\x00\\x19\\x1c\\x16V\\x15F\\x16\\x00\\x00\\x00\\x19\\x1c\\x16\\x9c\\x01\\x15N\\x16\\x00\\x00\\x00\\x15\\x02\\x19LH\\x0cspark_schema\\x15\\x06\\x00\\x15\\x04%\\x02\\x18\\x02id\\x00\\x15\\x02%\\x02\\x18\\x0edt_transaction%\\x0cLl\\x00\\x00\\x00\\x15\\x04%\\x02\\x18\\x07balance\\x00\\x16\\x02\\x19\\x1c\\x19<&\\x08\\x1c\\x15\\x04\\x195\\x06\\x00\\x08\\x19\\x18\\x02id\\x15\\x02\\x16\\x02\\x16J\\x16N&\\x08<\\x18\\x08\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x18\\x08\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x16\\x00(\\x08\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x18\\x08\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x19\\x1c\\x15\\x00\\x15\\x00\\x15\\x02\\x00\\x00\\x16\\x94\\x03\\x15\\x14\\x16\\xea\\x01\\x15>\\x00&V\\x1c\\x15\\x02\\x195\\x06\\x00\\x08\\x19\\x18\\x0edt_transaction\\x15\\x02\\x16\\x02\\x16B\\x16F&V<\\x18\\x04\\xe9E\\x00\\x00\\x18\\x04\\xe9E\\x00\\x00\\x16\\x00(\\x04\\xe9E\\x00\\x00\\x18\\x04\\xe9E\\x00\\x00\\x00\\x19\\x1c\\x15\\x00\\x15\\x00\\x15\\x02\\x00\\x00\\x16\\xa8\\x03\\x15\\x14\\x16\\xa8\\x02\\x15.\\x00&\\x9c\\x01\\x1c\\x15\\x04\\x195\\x06\\x00\\x08\\x19\\x18\\x07balance\\x15\\x02\\x16\\x02\\x16J\\x16N&\\x9c\\x01<\\x18\\x08\\xe8\\x03\\x00\\x00\\x00\\x00\\x00\\x00\\x18\\x08\\xe8\\x03\\x00\\x00\\x00\\x00\\x00\\x00\\x16\\x00(\\x08\\xe8\\x03\\x00\\x00\\x00\\x00\\x00\\x00\\x18\\x08\\xe8\\x03\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x19\\x1c\\x15\\x00\\x15\\x00\\x15\\x02\\x00\\x00\\x16\\xbc\\x03\\x15\\x16\\x16\\xd6\\x02\\x15>\\x00\\x16\\xd6\\x01\\x16\\x02&\\x08\\x16\\xe2\\x01\\x14\\x00\\x00\\x19,\\x18\\x18org.apache.spark.version\\x18\\x053.3.2\\x00\\x18)org.apache.spark.sql.parquet.row.metadata\\x18\\xdb\\x01{\"type\":\"struct\",\"fields\":[{\"name\":\"id\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"dt_transaction\",\"type\":\"date\",\"nullable\":true,\"metadata\":{}},{\"name\":\"balance\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}}]}\\x00\\x18Jparquet-mr version 1.12.2 (build 77e30c8093386ec52c3cfa6c34b7ef3321322c94)\\x19<\\x1c\\x00\\x00\\x1c\\x00\\x00\\x1c\\x00\\x00\\x00\\xf1\\x02\\x00\\x00PAR1'\n",
      "######################\n",
      "File: parquet/account=alex/part-00001-8ac618ae-a8d0-412c-8be3-a918e3f9b0d2.c000.snappy.parquet\n",
      "----------------------\n",
      "b'PAR1\\x15\\x00\\x15\\x1c\\x15 \\x15\\xc7\\xa8\\xd7\\xa1\\x05\\x1c\\x15\\x02\\x15\\x00\\x15\\x06\\x15\\x08\\x00\\x00\\x0e4\\x02\\x00\\x00\\x00\\x03\\x01\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x15\\x00\\x15\\x14\\x15\\x18\\x15\\xb5\\xfc\\xd3\\xef\\n\\x1c\\x15\\x02\\x15\\x00\\x15\\x06\\x15\\x08\\x00\\x00\\n$\\x02\\x00\\x00\\x00\\x03\\x01\\x08F\\x00\\x00\\x15\\x00\\x15\\x1c\\x15 \\x15\\xd5\\x86\\xad\\xbb\\r\\x1c\\x15\\x02\\x15\\x00\\x15\\x06\\x15\\x08\\x00\\x00\\x0e4\\x02\\x00\\x00\\x00\\x03\\x01\\xdc\\x05\\x00\\x00\\x00\\x00\\x00\\x00\\x19\\x11\\x02\\x19\\x18\\x08\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x19\\x18\\x08\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x15\\x02\\x19\\x16\\x00\\x00\\x19\\x11\\x02\\x19\\x18\\x04\\x08F\\x00\\x00\\x19\\x18\\x04\\x08F\\x00\\x00\\x15\\x02\\x19\\x16\\x00\\x00\\x19\\x11\\x02\\x19\\x18\\x08\\xdc\\x05\\x00\\x00\\x00\\x00\\x00\\x00\\x19\\x18\\x08\\xdc\\x05\\x00\\x00\\x00\\x00\\x00\\x00\\x15\\x02\\x19\\x16\\x00\\x00\\x19\\x1c\\x16\\x08\\x15N\\x16\\x00\\x00\\x00\\x19\\x1c\\x16V\\x15F\\x16\\x00\\x00\\x00\\x19\\x1c\\x16\\x9c\\x01\\x15N\\x16\\x00\\x00\\x00\\x15\\x02\\x19LH\\x0cspark_schema\\x15\\x06\\x00\\x15\\x04%\\x02\\x18\\x02id\\x00\\x15\\x02%\\x02\\x18\\x0edt_transaction%\\x0cLl\\x00\\x00\\x00\\x15\\x04%\\x02\\x18\\x07balance\\x00\\x16\\x02\\x19\\x1c\\x19<&\\x08\\x1c\\x15\\x04\\x195\\x06\\x00\\x08\\x19\\x18\\x02id\\x15\\x02\\x16\\x02\\x16J\\x16N&\\x08<\\x18\\x08\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x18\\x08\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x16\\x00(\\x08\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x18\\x08\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x19\\x1c\\x15\\x00\\x15\\x00\\x15\\x02\\x00\\x00\\x16\\x94\\x03\\x15\\x14\\x16\\xea\\x01\\x15>\\x00&V\\x1c\\x15\\x02\\x195\\x06\\x00\\x08\\x19\\x18\\x0edt_transaction\\x15\\x02\\x16\\x02\\x16B\\x16F&V<\\x18\\x04\\x08F\\x00\\x00\\x18\\x04\\x08F\\x00\\x00\\x16\\x00(\\x04\\x08F\\x00\\x00\\x18\\x04\\x08F\\x00\\x00\\x00\\x19\\x1c\\x15\\x00\\x15\\x00\\x15\\x02\\x00\\x00\\x16\\xa8\\x03\\x15\\x14\\x16\\xa8\\x02\\x15.\\x00&\\x9c\\x01\\x1c\\x15\\x04\\x195\\x06\\x00\\x08\\x19\\x18\\x07balance\\x15\\x02\\x16\\x02\\x16J\\x16N&\\x9c\\x01<\\x18\\x08\\xdc\\x05\\x00\\x00\\x00\\x00\\x00\\x00\\x18\\x08\\xdc\\x05\\x00\\x00\\x00\\x00\\x00\\x00\\x16\\x00(\\x08\\xdc\\x05\\x00\\x00\\x00\\x00\\x00\\x00\\x18\\x08\\xdc\\x05\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x19\\x1c\\x15\\x00\\x15\\x00\\x15\\x02\\x00\\x00\\x16\\xbc\\x03\\x15\\x16\\x16\\xd6\\x02\\x15>\\x00\\x16\\xd6\\x01\\x16\\x02&\\x08\\x16\\xe2\\x01\\x14\\x00\\x00\\x19,\\x18\\x18org.apache.spark.version\\x18\\x053.3.2\\x00\\x18)org.apache.spark.sql.parquet.row.metadata\\x18\\xdb\\x01{\"type\":\"struct\",\"fields\":[{\"name\":\"id\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"dt_transaction\",\"type\":\"date\",\"nullable\":true,\"metadata\":{}},{\"name\":\"balance\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}}]}\\x00\\x18Jparquet-mr version 1.12.2 (build 77e30c8093386ec52c3cfa6c34b7ef3321322c94)\\x19<\\x1c\\x00\\x00\\x1c\\x00\\x00\\x1c\\x00\\x00\\x00\\xf1\\x02\\x00\\x00PAR1'\n",
      "######################\n",
      "File: parquet/account=alex/part-00002-8ac618ae-a8d0-412c-8be3-a918e3f9b0d2.c000.snappy.parquet\n",
      "----------------------\n",
      "b'PAR1\\x15\\x00\\x15\\x1c\\x15 \\x15\\x84\\xd5\\xf8\\x94\\x03\\x1c\\x15\\x02\\x15\\x00\\x15\\x06\\x15\\x08\\x00\\x00\\x0e4\\x02\\x00\\x00\\x00\\x03\\x01\\x03\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x15\\x00\\x15\\x14\\x15\\x18\\x15\\xc6\\xc1\\x8c\\xbe\\x08\\x1c\\x15\\x02\\x15\\x00\\x15\\x06\\x15\\x08\\x00\\x00\\n$\\x02\\x00\\x00\\x00\\x03\\x01$F\\x00\\x00\\x15\\x00\\x15\\x1c\\x15 \\x15\\xa5\\xc4\\xc1\\xde\\x04\\x1c\\x15\\x02\\x15\\x00\\x15\\x06\\x15\\x08\\x00\\x00\\x0e4\\x02\\x00\\x00\\x00\\x03\\x01\\xa4\\x06\\x00\\x00\\x00\\x00\\x00\\x00\\x19\\x11\\x02\\x19\\x18\\x08\\x03\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x19\\x18\\x08\\x03\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x15\\x02\\x19\\x16\\x00\\x00\\x19\\x11\\x02\\x19\\x18\\x04$F\\x00\\x00\\x19\\x18\\x04$F\\x00\\x00\\x15\\x02\\x19\\x16\\x00\\x00\\x19\\x11\\x02\\x19\\x18\\x08\\xa4\\x06\\x00\\x00\\x00\\x00\\x00\\x00\\x19\\x18\\x08\\xa4\\x06\\x00\\x00\\x00\\x00\\x00\\x00\\x15\\x02\\x19\\x16\\x00\\x00\\x19\\x1c\\x16\\x08\\x15N\\x16\\x00\\x00\\x00\\x19\\x1c\\x16V\\x15F\\x16\\x00\\x00\\x00\\x19\\x1c\\x16\\x9c\\x01\\x15N\\x16\\x00\\x00\\x00\\x15\\x02\\x19LH\\x0cspark_schema\\x15\\x06\\x00\\x15\\x04%\\x02\\x18\\x02id\\x00\\x15\\x02%\\x02\\x18\\x0edt_transaction%\\x0cLl\\x00\\x00\\x00\\x15\\x04%\\x02\\x18\\x07balance\\x00\\x16\\x02\\x19\\x1c\\x19<&\\x08\\x1c\\x15\\x04\\x195\\x08\\x00\\x06\\x19\\x18\\x02id\\x15\\x02\\x16\\x02\\x16J\\x16N&\\x08<\\x18\\x08\\x03\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x18\\x08\\x03\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x16\\x00(\\x08\\x03\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x18\\x08\\x03\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x19\\x1c\\x15\\x00\\x15\\x00\\x15\\x02\\x00\\x00\\x16\\x94\\x03\\x15\\x14\\x16\\xea\\x01\\x15>\\x00&V\\x1c\\x15\\x02\\x195\\x08\\x00\\x06\\x19\\x18\\x0edt_transaction\\x15\\x02\\x16\\x02\\x16B\\x16F&V<\\x18\\x04$F\\x00\\x00\\x18\\x04$F\\x00\\x00\\x16\\x00(\\x04$F\\x00\\x00\\x18\\x04$F\\x00\\x00\\x00\\x19\\x1c\\x15\\x00\\x15\\x00\\x15\\x02\\x00\\x00\\x16\\xa8\\x03\\x15\\x14\\x16\\xa8\\x02\\x15.\\x00&\\x9c\\x01\\x1c\\x15\\x04\\x195\\x08\\x00\\x06\\x19\\x18\\x07balance\\x15\\x02\\x16\\x02\\x16J\\x16N&\\x9c\\x01<\\x18\\x08\\xa4\\x06\\x00\\x00\\x00\\x00\\x00\\x00\\x18\\x08\\xa4\\x06\\x00\\x00\\x00\\x00\\x00\\x00\\x16\\x00(\\x08\\xa4\\x06\\x00\\x00\\x00\\x00\\x00\\x00\\x18\\x08\\xa4\\x06\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x19\\x1c\\x15\\x00\\x15\\x00\\x15\\x02\\x00\\x00\\x16\\xbc\\x03\\x15\\x16\\x16\\xd6\\x02\\x15>\\x00\\x16\\xd6\\x01\\x16\\x02&\\x08\\x16\\xe2\\x01\\x14\\x00\\x00\\x19,\\x18\\x18org.apache.spark.version\\x18\\x053.3.2\\x00\\x18)org.apache.spark.sql.parquet.row.metadata\\x18\\xdb\\x01{\"type\":\"struct\",\"fields\":[{\"name\":\"id\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"dt_transaction\",\"type\":\"date\",\"nullable\":true,\"metadata\":{}},{\"name\":\"balance\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}}]}\\x00\\x18Jparquet-mr version 1.12.2 (build 77e30c8093386ec52c3cfa6c34b7ef3321322c94)\\x19<\\x1c\\x00\\x00\\x1c\\x00\\x00\\x1c\\x00\\x00\\x00\\xf1\\x02\\x00\\x00PAR1'\n",
      "######################\n",
      "File: parquet/account=maria/part-00000-8ac618ae-a8d0-412c-8be3-a918e3f9b0d2.c000.snappy.parquet\n",
      "----------------------\n",
      "b'PAR1\\x15\\x00\\x15\\x1c\\x15 \\x15\\xb6\\xf1\\xd0\\xb8\\x02\\x1c\\x15\\x02\\x15\\x00\\x15\\x06\\x15\\x08\\x00\\x00\\x0e4\\x02\\x00\\x00\\x00\\x03\\x01\\x04\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x15\\x00\\x15\\x14\\x15\\x18\\x15\\xf9\\xb6\\xd3\\xce\\x0f\\x1c\\x15\\x02\\x15\\x00\\x15\\x06\\x15\\x08\\x00\\x00\\n$\\x02\\x00\\x00\\x00\\x03\\x01VG\\x00\\x00\\x15\\x00\\x15\\x1c\\x15 \\x15\\xba\\xaa\\xc0\\x92\\x08\\x1c\\x15\\x02\\x15\\x00\\x15\\x06\\x15\\x08\\x00\\x00\\x0e4\\x02\\x00\\x00\\x00\\x03\\x01\\x88\\x13\\x00\\x00\\x00\\x00\\x00\\x00\\x19\\x11\\x02\\x19\\x18\\x08\\x04\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x19\\x18\\x08\\x04\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x15\\x02\\x19\\x16\\x00\\x00\\x19\\x11\\x02\\x19\\x18\\x04VG\\x00\\x00\\x19\\x18\\x04VG\\x00\\x00\\x15\\x02\\x19\\x16\\x00\\x00\\x19\\x11\\x02\\x19\\x18\\x08\\x88\\x13\\x00\\x00\\x00\\x00\\x00\\x00\\x19\\x18\\x08\\x88\\x13\\x00\\x00\\x00\\x00\\x00\\x00\\x15\\x02\\x19\\x16\\x00\\x00\\x19\\x1c\\x16\\x08\\x15N\\x16\\x00\\x00\\x00\\x19\\x1c\\x16V\\x15F\\x16\\x00\\x00\\x00\\x19\\x1c\\x16\\x9c\\x01\\x15N\\x16\\x00\\x00\\x00\\x15\\x02\\x19LH\\x0cspark_schema\\x15\\x06\\x00\\x15\\x04%\\x02\\x18\\x02id\\x00\\x15\\x02%\\x02\\x18\\x0edt_transaction%\\x0cLl\\x00\\x00\\x00\\x15\\x04%\\x02\\x18\\x07balance\\x00\\x16\\x02\\x19\\x1c\\x19<&\\x08\\x1c\\x15\\x04\\x195\\x06\\x00\\x08\\x19\\x18\\x02id\\x15\\x02\\x16\\x02\\x16J\\x16N&\\x08<\\x18\\x08\\x04\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x18\\x08\\x04\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x16\\x00(\\x08\\x04\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x18\\x08\\x04\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x19\\x1c\\x15\\x00\\x15\\x00\\x15\\x02\\x00\\x00\\x16\\x94\\x03\\x15\\x14\\x16\\xea\\x01\\x15>\\x00&V\\x1c\\x15\\x02\\x195\\x06\\x00\\x08\\x19\\x18\\x0edt_transaction\\x15\\x02\\x16\\x02\\x16B\\x16F&V<\\x18\\x04VG\\x00\\x00\\x18\\x04VG\\x00\\x00\\x16\\x00(\\x04VG\\x00\\x00\\x18\\x04VG\\x00\\x00\\x00\\x19\\x1c\\x15\\x00\\x15\\x00\\x15\\x02\\x00\\x00\\x16\\xa8\\x03\\x15\\x14\\x16\\xa8\\x02\\x15.\\x00&\\x9c\\x01\\x1c\\x15\\x04\\x195\\x06\\x00\\x08\\x19\\x18\\x07balance\\x15\\x02\\x16\\x02\\x16J\\x16N&\\x9c\\x01<\\x18\\x08\\x88\\x13\\x00\\x00\\x00\\x00\\x00\\x00\\x18\\x08\\x88\\x13\\x00\\x00\\x00\\x00\\x00\\x00\\x16\\x00(\\x08\\x88\\x13\\x00\\x00\\x00\\x00\\x00\\x00\\x18\\x08\\x88\\x13\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x19\\x1c\\x15\\x00\\x15\\x00\\x15\\x02\\x00\\x00\\x16\\xbc\\x03\\x15\\x16\\x16\\xd6\\x02\\x15>\\x00\\x16\\xd6\\x01\\x16\\x02&\\x08\\x16\\xe2\\x01\\x14\\x00\\x00\\x19,\\x18\\x18org.apache.spark.version\\x18\\x053.3.2\\x00\\x18)org.apache.spark.sql.parquet.row.metadata\\x18\\xdb\\x01{\"type\":\"struct\",\"fields\":[{\"name\":\"id\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"dt_transaction\",\"type\":\"date\",\"nullable\":true,\"metadata\":{}},{\"name\":\"balance\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}}]}\\x00\\x18Jparquet-mr version 1.12.2 (build 77e30c8093386ec52c3cfa6c34b7ef3321322c94)\\x19<\\x1c\\x00\\x00\\x1c\\x00\\x00\\x1c\\x00\\x00\\x00\\xf1\\x02\\x00\\x00PAR1'\n",
      "######################\n"
     ]
    }
   ],
   "source": [
    "# schaue eine Datei im Detail an und finde die Metadaten\n",
    "cat(bucket,\"parquet\",True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f538dc",
   "metadata": {},
   "source": [
    "<details style=\"border: 1px solid #aaa; border-radius: 4px; padding: 0.5em 0.5em 0; background-color:#F5F5F5\" class=\"solution\" >\n",
    "<summary style=\"margin: -0.5em -0.5em 0; padding: 0.5em;\"></summary>\n",
    "Lösung PARQUET Metadaten anzeigen:<br>\n",
    "<code>\n",
    "cat(bucket,\"parquet/account=maria\",True)\n",
    "</code>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba707c84",
   "metadata": {},
   "source": [
    "### Parquet: Filter Pushdown\n",
    "Da das Parquet Format spalten basiert ist und für jede Spalte Metadaten vorhällt, können Programme die diese Dateien einlesen vor der Serialisierung (dem kompletten Einlesen in den Arbeitsspeicher) erst die Header scannen und entscheiden welche Dateien tatsächlich benötigt werden.  \n",
    "Dies nennt man Attribut oder Filter Pushdown.  \n",
    "Spark kann außerdem, wenn die Daten in Partition im Format `PartitionKey=value` diese automatisch erkennen und wenn ein Filter auf die Partition gelegt ist nur diesen Unterordner einlesen.  \n",
    "\n",
    "**Aufgabe:** Untersuche den Execution Plan der Filter Operation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e00a1e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition Filter\n",
      "#######################\n",
      "== Physical Plan ==\n",
      "*(1) ColumnarToRow\n",
      "+- FileScan parquet [id#579L,dt_transaction#580,balance#581L,account#582] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[s3://fileformats/parquet], PartitionFilters: [isnotnull(account#582), (account#582 = alex)], PushedFilters: [], ReadSchema: struct<id:bigint,dt_transaction:date,balance:bigint>\n",
      "\n",
      "\n",
      "Pushdow Filter\n",
      "#######################\n",
      "== Physical Plan ==\n",
      "*(1) Project [balance#590L]\n",
      "+- *(1) Filter (isnotnull(balance#590L) AND (balance#590L > 1500))\n",
      "   +- *(1) ColumnarToRow\n",
      "      +- FileScan parquet [balance#590L,account#591] Batched: true, DataFilters: [isnotnull(balance#590L), (balance#590L > 1500)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[s3://fileformats/parquet], PartitionFilters: [], PushedFilters: [IsNotNull(balance), GreaterThan(balance,1500)], ReadSchema: struct<balance:bigint>\n",
      "\n",
      "\n",
      "All Filter\n",
      "#######################\n",
      "== Physical Plan ==\n",
      "*(1) Project [account#601, balance#600L]\n",
      "+- *(1) Filter (isnotnull(balance#600L) AND (balance#600L > 1500))\n",
      "   +- *(1) ColumnarToRow\n",
      "      +- FileScan parquet [balance#600L,account#601] Batched: true, DataFilters: [isnotnull(balance#600L), (balance#600L > 1500)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[s3://fileformats/parquet], PartitionFilters: [isnotnull(account#601), (account#601 = alex)], PushedFilters: [IsNotNull(balance), GreaterThan(balance,1500)], ReadSchema: struct<balance:bigint>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parquet Datei mit PartitionFilter laden\n",
    "read_parquet=(spark\n",
    "              .read.format(\"parquet\")\n",
    "              .load(f\"s3://{bucket}/parquet\")\n",
    "              # Filter auf die Spalte über die partitioniert wurde\n",
    "              .filter(f.col(\"account\")==\"alex\")\n",
    "             )\n",
    "\n",
    "# Parquet mit normalem Filter laden\n",
    "read_parquet2=(spark\n",
    "              .read.format(\"parquet\")\n",
    "              .load(f\"s3://{bucket}/parquet\")\n",
    "              # Filter auf die Spalte über eine normale Spalte\n",
    "              .filter(f.col(\"balance\")>1500)\n",
    "              .select(\"balance\")\n",
    "             )\n",
    "\n",
    "# Parquet mit normalem Filter laden\n",
    "read_parquet3=(spark\n",
    "              .read.format(\"parquet\")\n",
    "              .load(f\"s3://{bucket}/parquet\")\n",
    "              # Filter auf die Spalte über die partitioniert wurde\n",
    "              .filter(f.col(\"account\")==\"alex\")\n",
    "              # Filter auf die Spalte über eine normale Spalte\n",
    "              .filter(f.col(\"balance\")>1500)\n",
    "              .select(\"account\",\"balance\")\n",
    "             )\n",
    "\n",
    "# Anzeigen des physischen Execution Plans um zu sehen welche Filter ins Dateisystem bzw. in die Parquet Datei gepusht werden\n",
    "print(\"Partition Filter\")\n",
    "print(\"#######################\")\n",
    "read_parquet.explain(\"simple\")\n",
    "print(\"Pushdow Filter\")\n",
    "print(\"#######################\")\n",
    "read_parquet2.explain(\"simple\")\n",
    "print(\"All Filter\")\n",
    "print(\"#######################\")\n",
    "read_parquet3.explain(\"simple\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1114d68f",
   "metadata": {},
   "source": [
    "### Parquet: Schema Evolution\n",
    "Schema Evolution ermöglicht es das Schema der Tabelle zu erweitern.  \n",
    "Der Spark Parquet reader bietet verschiedenen Möglichkeiten mit Schemaerweiterungen umzugehen  \n",
    "https://spark.apache.org/docs/latest/sql-data-sources-parquet.html#schema-merging\n",
    "\n",
    "**Aufgabe:** Lese die Daten so ein, dass das Schema korrekt erweitert wird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "74705fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zeile mit neuer Spalte anfügen\n",
    "write_parquet=(df3\n",
    "           .write\n",
    "           .format(\"parquet\")\n",
    "           .mode(\"append\") # append\n",
    "           # schreibe ohne zu Partitionieren direkt in ein neues Unterverzeichnis\n",
    "           .save(f\"s3://{bucket}/parquet/account=otto\")\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b3c8e59d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- dt_transaction: date (nullable = true)\n",
      " |-- balance: long (nullable = true)\n",
      " |-- account: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 74:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+-------+-------+\n",
      "| id|dt_transaction|balance|account|\n",
      "+---+--------------+-------+-------+\n",
      "|  1|    2019-10-01|   4444|   otto|\n",
      "|  1|    2019-01-01|   1000|   alex|\n",
      "|  2|    2019-02-01|   1500|   alex|\n",
      "|  3|    2019-03-01|   1700|   alex|\n",
      "|  4|    2020-01-01|   5000|  maria|\n",
      "+---+--------------+-------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# einlesen mit der mergeSchema Option\n",
    "read_parquet=(spark\n",
    "              .read.format(\"parquet\")\n",
    "              # setzte die mergeSchema auf true/false um den Unterschied beim Einlesen zu sehen\n",
    "              # Vegleiche: https://spark.apache.org/docs/latest/sql-data-sources-parquet.html#schema-merging\n",
    "              #.option(\"mergeSchema\", \"false\")\n",
    "              .load(f\"s3://{bucket}/parquet\")\n",
    "             )\n",
    "\n",
    "read_parquet.printSchema()\n",
    "read_parquet.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244785ff",
   "metadata": {},
   "source": [
    "### Parquet: Schema Enforcement\n",
    "Schema Enforcement sorgt dafür, dass in ein bestehendes Schema keine Daten mit falschen Typen geschrieben werden können"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b3dad2a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Datensatz mit falschem Datentyp anfügen\n",
    "df2a=(df2.where(f.col(\"account\")==\"peter\").withColumn(\"id\", f.col(\"id\").cast(\"string\")))\n",
    "\n",
    "\n",
    "# Zeile mit falschem Typ anfügen\n",
    "write_parquet=(df2a\n",
    "           .write\n",
    "           .partitionBy(\"account\")\n",
    "           .format(\"parquet\")\n",
    "           .mode(\"append\") \n",
    "           .save(f\"s3://{bucket}/parquet\")\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "660c9fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- dt_transaction: date (nullable = true)\n",
      " |-- balance: long (nullable = true)\n",
      " |-- account: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 80:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/07/21 07:57:37 ERROR TaskSetManager: Task 1 in stage 80.0 failed 4 times; aborting job\n",
      "++ filter in Spark Error:\n",
      "pache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file s3://fileformats/parquet/account=peter/part-00000-b86c829d-5c70-41c9-9153-d6d74c540e84.c000.snappy.parquet. Column: [id], Expected: bigint, Found: BINARY\n",
      "\t\n",
      "++ this is schema enforcement on read\n"
     ]
    }
   ],
   "source": [
    "# einlesen mit der mergeSchema Option\n",
    "read_parquet=(spark\n",
    "              .read.format(\"parquet\")\n",
    "              # setzte die mergeSchema auf true/false um den Unterschied beim Einlesen zu sehen\n",
    "              .option(\"mergeSchema\", \"false\")\n",
    "              .load(f\"s3://{bucket}/parquet\")\n",
    "             )\n",
    "\n",
    "read_parquet.printSchema()\n",
    "\n",
    "\n",
    "try:\n",
    "    read_parquet.show()\n",
    "except Exception as error:\n",
    "    error_str=str(error)\n",
    "    #print(error_str)\n",
    "    search=\"Parquet column cannot be converted in file s3\"\n",
    "    print(\"++ filter in Spark Error:\")\n",
    "    print(error_str[error_str.find(search)-51:error_str.find(search)+198])\n",
    "    print(\"++ this is schema enforcement on read\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab7dd65",
   "metadata": {},
   "source": [
    "#### Erkenntnisse Parquet\n",
    "* Sind Parquet Dateien selbsterklärend (haben ein Spalten und Typenschema )\n",
    "* Partitioning and Partion Discovery: werden die Daten in Verzeichnisse geschriebe und wieder als Partitionen erkannt?\n",
    "* Schema Evolution: Kann das Schema erweitert werden, also eine neue Spalte angefügt werden?\n",
    "* Schema Enforcement on write: Kann eine Spalte mit falschem Datetyp einfach beim schreiben hinzugefügt werden? \n",
    "* Schema Enforcement on read: Kann ein Verzeichnis mit mehreren Parquet Dateien bei der eine Spalte ein anderes Schema hat gelesen werden?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80ea102",
   "metadata": {},
   "source": [
    "<hr style=\"height: 3px; background: gray;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eadd53c",
   "metadata": {},
   "source": [
    "# Delta\n",
    "Das Delta Format fügt Parquet Dateien einen zusätzlichen Layer an Metadatan hinzu und erfüllt mit dem entsprechenden Treiber alle ACID Eigenschaften einer Datenbank und mehr. \n",
    "\n",
    "A = **Atomic** heißt alle Datenänderungen werden wie eine einzige Operation verarbeitet. Dies bedeutet, dass entweder alle Änderungen durchgeführt werden oder keine. Wenn das schreiben also mitten drin Fehlschlägt werden alle Daten dieser Schreiboperation die bereits geschrieben wurden wieder entfernt, bzw. nicht als erfolgreich geschrieben markiert.  \n",
    "T = **Consistency** bedeutet, wenn eine Transaktion beginnt und wenn eine Transaktion endet, befinden sich die Daten in einem konsistenten Zustand.  \n",
    "I = **Isolation** und bedeutet, dass der Übergangszustand einer Transaktion für andere Transaktionen nicht sichtbar ist. Dies führt dazu, dass Transaktionen, die gleichzeitig ablaufen, sich nicht gegenseitig beeinflussen oder blockieren.  \n",
    "D =**Durability** heißt, das die Datenänderungen nach erfolgreich abgeschlossener Transaktion erhalten bleiben und werden nicht rückgängig gemacht werden, selbst wenn ein Systemausfall auftritt.  \n",
    "\n",
    "**Time Travel** bei Delta bedeutet, dass jede Datenänderung als eigene Version aufgezeichnet wird und jederzeit zu einer alten Version zurück gegegangen werden kann.\n",
    "\n",
    "### Aufgabe:\n",
    "Wiederhole die gleichen Schritte mit dem DELTA Format und schaue wie sich hier Schema und neue Spalten verhalten\n",
    "\n",
    "1. Datenset 1 als DELTA schreiben (.format(\"delta\") und Pfad= .save(f\"s3://{bucket}/delta\"))\n",
    "2. Dateien und Inhalt anzeigen, vestehen was da passiert ist\n",
    "3. Metadaten und Deltalog in Datei verstehen\n",
    "3. Daten wieder einlese und checken ob es ein Schema und Spaltennamen gibt\n",
    "4. Schema Evolutiuon: Datenset 3 anfügen mit neuer Spalte anfügen\n",
    "5. Daten wieder einlesen und checken was mit der neuen Spalte passiert\n",
    "6. Partion & Pushdown Filter: Execution Plan für verschiedene Filter anzeigen\n",
    "6. Schema Enforcement: Datentyp in bestehender Spalte ändern und schauen ob und wie dies gehandhabt wird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "513d213c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Schreibe die Daten df1 als Delta Datei in den Pfad bucket/delta\n",
    "write_delta=(df1\n",
    "           .write\n",
    "           .format(\"delta\")\n",
    "           .option(\"overwriteSchema\", \"true\")\n",
    "           .mode(\"overwrite\") \n",
    "           .save(f\"s3://{bucket}/delta\")\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "70cd6f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta/_delta_log/00000000000000000000.json\n",
      "delta/part-00000-a8a8ace7-c5fb-4f8a-9ddc-d0ebfc4e1181-c000.snappy.parquet\n",
      "delta/part-00001-84e3dcfd-61b2-4cf1-8207-6d438e6439f7-c000.snappy.parquet\n",
      "delta/part-00002-c92c8bd2-8907-474d-b48c-420e6b8ef06e-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "ls(bucket,\"delta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2da732ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta/_delta_log/00000000000000000000.json\n"
     ]
    }
   ],
   "source": [
    "ls(bucket,\"delta/_delta_log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0c3dbb33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: delta/_delta_log/00000000000000000000.json\n",
      "----------------------\n",
      "{\"commitInfo\":{\"timestamp\":1689926289159,\"operation\":\"WRITE\",\"operationParameters\":{\"mode\":\"Overwrite\",\"partitionBy\":\"[]\"},\"isolationLevel\":\"Serializable\",\"isBlindAppend\":false,\"operationMetrics\":{\"numFiles\":\"3\",\"numOutputRows\":\"4\",\"numOutputBytes\":\"3727\"},\"engineInfo\":\"Apache-Spark/3.3.2 Delta-Lake/2.3.0\",\"txnId\":\"599b6d5d-abe5-4072-979b-fbc8afbc62d5\"}}\n",
      "{\"protocol\":{\"minReaderVersion\":1,\"minWriterVersion\":2}}\n",
      "{\"metaData\":{\"id\":\"a0ab151a-0f86-49ee-b19c-a01418b1daa4\",\"format\":{\"provider\":\"parquet\",\"options\":{}},\"schemaString\":\"{\\\"type\\\":\\\"struct\\\",\\\"fields\\\":[{\\\"name\\\":\\\"id\\\",\\\"type\\\":\\\"long\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"account\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"dt_transaction\\\",\\\"type\\\":\\\"date\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"balance\\\",\\\"type\\\":\\\"long\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}}]}\",\"partitionColumns\":[],\"configuration\":{},\"createdTime\":1689926270564}}\n",
      "{\"add\":{\"path\":\"part-00000-a8a8ace7-c5fb-4f8a-9ddc-d0ebfc4e1181-c000.snappy.parquet\",\"partitionValues\":{},\"size\":1255,\"modificationTime\":1689926273000,\"dataChange\":true,\"stats\":\"{\\\"numRecords\\\":2,\\\"minValues\\\":{\\\"id\\\":1,\\\"account\\\":\\\"alex\\\",\\\"dt_transaction\\\":\\\"2019-01-01\\\",\\\"balance\\\":1000},\\\"maxValues\\\":{\\\"id\\\":4,\\\"account\\\":\\\"maria\\\",\\\"dt_transaction\\\":\\\"2020-01-01\\\",\\\"balance\\\":5000},\\\"nullCount\\\":{\\\"id\\\":0,\\\"account\\\":0,\\\"dt_transaction\\\":0,\\\"balance\\\":0}}\"}}\n",
      "{\"add\":{\"path\":\"part-00001-84e3dcfd-61b2-4cf1-8207-6d438e6439f7-c000.snappy.parquet\",\"partitionValues\":{},\"size\":1236,\"modificationTime\":1689926273000,\"dataChange\":true,\"stats\":\"{\\\"numRecords\\\":1,\\\"minValues\\\":{\\\"id\\\":2,\\\"account\\\":\\\"alex\\\",\\\"dt_transaction\\\":\\\"2019-02-01\\\",\\\"balance\\\":1500},\\\"maxValues\\\":{\\\"id\\\":2,\\\"account\\\":\\\"alex\\\",\\\"dt_transaction\\\":\\\"2019-02-01\\\",\\\"balance\\\":1500},\\\"nullCount\\\":{\\\"id\\\":0,\\\"account\\\":0,\\\"dt_transaction\\\":0,\\\"balance\\\":0}}\"}}\n",
      "{\"add\":{\"path\":\"part-00002-c92c8bd2-8907-474d-b48c-420e6b8ef06e-c000.snappy.parquet\",\"partitionValues\":{},\"size\":1236,\"modificationTime\":1689926273000,\"dataChange\":true,\"stats\":\"{\\\"numRecords\\\":1,\\\"minValues\\\":{\\\"id\\\":3,\\\"account\\\":\\\"alex\\\",\\\"dt_transaction\\\":\\\"2019-03-01\\\",\\\"balance\\\":1700},\\\"maxValues\\\":{\\\"id\\\":3,\\\"account\\\":\\\"alex\\\",\\\"dt_transaction\\\":\\\"2019-03-01\\\",\\\"balance\\\":1700},\\\"nullCount\\\":{\\\"id\\\":0,\\\"account\\\":0,\\\"dt_transaction\\\":0,\\\"balance\\\":0}}\"}}\n",
      "\n",
      "######################\n"
     ]
    }
   ],
   "source": [
    "# untersuche die Dateien des Deltalogs und versuche zu verstehen wie die Versionierung und Schema Validierung damit funktioniert\n",
    "cat(bucket,\"delta/_delta_log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a714c074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- account: string (nullable = true)\n",
      " |-- dt_transaction: date (nullable = true)\n",
      " |-- balance: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------+-------+\n",
      "| id|account|dt_transaction|balance|\n",
      "+---+-------+--------------+-------+\n",
      "|  1|   alex|    2019-01-01|   1000|\n",
      "|  4|  maria|    2020-01-01|   5000|\n",
      "|  2|   alex|    2019-02-01|   1500|\n",
      "|  3|   alex|    2019-03-01|   1700|\n",
      "+---+-------+--------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lese die gerade geschriebenen Delta Tabelle wieder ein und zeige sie an, überprüfe das Schema\n",
    "read_delta=(spark.read.format(\"delta\").load(f\"s3://{bucket}/delta\"))\n",
    "\n",
    "read_delta.printSchema()\n",
    "read_delta.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba9e1f8",
   "metadata": {},
   "source": [
    "<details style=\"border: 1px solid #aaa; border-radius: 4px; padding: 0.5em 0.5em 0; background-color:#F5F5F5\" class=\"solution\" >\n",
    "<summary style=\"margin: -0.5em -0.5em 0; padding: 0.5em;\"></summary>\n",
    "<code>\n",
    "read_delta=spark.read.format(\"delta\").load(f\"s3://{bucket}/delta\")\n",
    "read_delta.show()\n",
    "</code>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649e7d11",
   "metadata": {},
   "source": [
    "### Delta: Schema Evolution\n",
    "**Aufgabe:** Verstehe die Option *mergeSchema* on write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e8d72dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------+-------+-------------+\n",
      "| id|account|dt_transaction|balance|          new|\n",
      "+---+-------+--------------+-------+-------------+\n",
      "|  1|   otto|    2019-10-01|   4444|neue Spalte 1|\n",
      "|  1|   alex|    2019-01-01|   1000|         null|\n",
      "|  4|  maria|    2020-01-01|   5000|         null|\n",
      "|  2|   alex|    2019-02-01|   1500|         null|\n",
      "|  3|   alex|    2019-03-01|   1700|         null|\n",
      "+---+-------+--------------+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Zeile mit zusätzlicher Spalte anfügen (df3)\n",
    "write_delta=(df3\n",
    "           .write\n",
    "           .format(\"delta\")\n",
    "           # Bei Delta kann bein Schreiben gesetzt werden ob die Tabelle erweitert werden soll oder nicht, Default ist false. \n",
    "           # Führe den Code zuerst ohne diese Option aus und schaue das Ergebnis, an \n",
    "           .option(\"mergeSchema\", \"true\")\n",
    "           .mode(\"append\") # append\n",
    "           .save(f\"s3://{bucket}/delta\")\n",
    "          )\n",
    "\n",
    "\n",
    "# überprüfe ob die neue Spalte korrekt angefügt wurde\n",
    "read_delta=spark.read.format(\"delta\").load(f\"s3://{bucket}/delta\")\n",
    "read_delta.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a5bf92d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: delta/_delta_log/00000000000000000000.json\n",
      "----------------------\n",
      "{\"commitInfo\":{\"timestamp\":1689926289159,\"operation\":\"WRITE\",\"operationParameters\":{\"mode\":\"Overwrite\",\"partitionBy\":\"[]\"},\"isolationLevel\":\"Serializable\",\"isBlindAppend\":false,\"operationMetrics\":{\"numFiles\":\"3\",\"numOutputRows\":\"4\",\"numOutputBytes\":\"3727\"},\"engineInfo\":\"Apache-Spark/3.3.2 Delta-Lake/2.3.0\",\"txnId\":\"599b6d5d-abe5-4072-979b-fbc8afbc62d5\"}}\n",
      "{\"protocol\":{\"minReaderVersion\":1,\"minWriterVersion\":2}}\n",
      "{\"metaData\":{\"id\":\"a0ab151a-0f86-49ee-b19c-a01418b1daa4\",\"format\":{\"provider\":\"parquet\",\"options\":{}},\"schemaString\":\"{\\\"type\\\":\\\"struct\\\",\\\"fields\\\":[{\\\"name\\\":\\\"id\\\",\\\"type\\\":\\\"long\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"account\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"dt_transaction\\\",\\\"type\\\":\\\"date\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"balance\\\",\\\"type\\\":\\\"long\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}}]}\",\"partitionColumns\":[],\"configuration\":{},\"createdTime\":1689926270564}}\n",
      "{\"add\":{\"path\":\"part-00000-a8a8ace7-c5fb-4f8a-9ddc-d0ebfc4e1181-c000.snappy.parquet\",\"partitionValues\":{},\"size\":1255,\"modificationTime\":1689926273000,\"dataChange\":true,\"stats\":\"{\\\"numRecords\\\":2,\\\"minValues\\\":{\\\"id\\\":1,\\\"account\\\":\\\"alex\\\",\\\"dt_transaction\\\":\\\"2019-01-01\\\",\\\"balance\\\":1000},\\\"maxValues\\\":{\\\"id\\\":4,\\\"account\\\":\\\"maria\\\",\\\"dt_transaction\\\":\\\"2020-01-01\\\",\\\"balance\\\":5000},\\\"nullCount\\\":{\\\"id\\\":0,\\\"account\\\":0,\\\"dt_transaction\\\":0,\\\"balance\\\":0}}\"}}\n",
      "{\"add\":{\"path\":\"part-00001-84e3dcfd-61b2-4cf1-8207-6d438e6439f7-c000.snappy.parquet\",\"partitionValues\":{},\"size\":1236,\"modificationTime\":1689926273000,\"dataChange\":true,\"stats\":\"{\\\"numRecords\\\":1,\\\"minValues\\\":{\\\"id\\\":2,\\\"account\\\":\\\"alex\\\",\\\"dt_transaction\\\":\\\"2019-02-01\\\",\\\"balance\\\":1500},\\\"maxValues\\\":{\\\"id\\\":2,\\\"account\\\":\\\"alex\\\",\\\"dt_transaction\\\":\\\"2019-02-01\\\",\\\"balance\\\":1500},\\\"nullCount\\\":{\\\"id\\\":0,\\\"account\\\":0,\\\"dt_transaction\\\":0,\\\"balance\\\":0}}\"}}\n",
      "{\"add\":{\"path\":\"part-00002-c92c8bd2-8907-474d-b48c-420e6b8ef06e-c000.snappy.parquet\",\"partitionValues\":{},\"size\":1236,\"modificationTime\":1689926273000,\"dataChange\":true,\"stats\":\"{\\\"numRecords\\\":1,\\\"minValues\\\":{\\\"id\\\":3,\\\"account\\\":\\\"alex\\\",\\\"dt_transaction\\\":\\\"2019-03-01\\\",\\\"balance\\\":1700},\\\"maxValues\\\":{\\\"id\\\":3,\\\"account\\\":\\\"alex\\\",\\\"dt_transaction\\\":\\\"2019-03-01\\\",\\\"balance\\\":1700},\\\"nullCount\\\":{\\\"id\\\":0,\\\"account\\\":0,\\\"dt_transaction\\\":0,\\\"balance\\\":0}}\"}}\n",
      "\n",
      "######################\n",
      "File: delta/_delta_log/00000000000000000001.json\n",
      "----------------------\n",
      "{\"commitInfo\":{\"timestamp\":1689926320865,\"operation\":\"WRITE\",\"operationParameters\":{\"mode\":\"Append\",\"partitionBy\":\"[]\"},\"readVersion\":0,\"isolationLevel\":\"Serializable\",\"isBlindAppend\":true,\"operationMetrics\":{\"numFiles\":\"1\",\"numOutputRows\":\"1\",\"numOutputBytes\":\"1528\"},\"engineInfo\":\"Apache-Spark/3.3.2 Delta-Lake/2.3.0\",\"txnId\":\"1a358d1d-bc2e-4943-a92a-986afa84ef2c\"}}\n",
      "{\"metaData\":{\"id\":\"a0ab151a-0f86-49ee-b19c-a01418b1daa4\",\"format\":{\"provider\":\"parquet\",\"options\":{}},\"schemaString\":\"{\\\"type\\\":\\\"struct\\\",\\\"fields\\\":[{\\\"name\\\":\\\"id\\\",\\\"type\\\":\\\"long\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"account\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"dt_transaction\\\",\\\"type\\\":\\\"date\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"balance\\\",\\\"type\\\":\\\"long\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"new\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}}]}\",\"partitionColumns\":[],\"configuration\":{},\"createdTime\":1689926270564}}\n",
      "{\"add\":{\"path\":\"part-00000-46ec8d48-d308-4d2c-b8a1-4df18d133d5b-c000.snappy.parquet\",\"partitionValues\":{},\"size\":1528,\"modificationTime\":1689926320000,\"dataChange\":true,\"stats\":\"{\\\"numRecords\\\":1,\\\"minValues\\\":{\\\"id\\\":1,\\\"account\\\":\\\"otto\\\",\\\"dt_transaction\\\":\\\"2019-10-01\\\",\\\"balance\\\":4444,\\\"new\\\":\\\"neue Spalte 1\\\"},\\\"maxValues\\\":{\\\"id\\\":1,\\\"account\\\":\\\"otto\\\",\\\"dt_transaction\\\":\\\"2019-10-01\\\",\\\"balance\\\":4444,\\\"new\\\":\\\"neue Spalte 1\\\"},\\\"nullCount\\\":{\\\"id\\\":0,\\\"account\\\":0,\\\"dt_transaction\\\":0,\\\"balance\\\":0,\\\"new\\\":0}}\"}}\n",
      "\n",
      "######################\n"
     ]
    }
   ],
   "source": [
    "cat(bucket,\"delta/_delta_log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae03a75",
   "metadata": {},
   "source": [
    "### Delta: Schema Enforcement\n",
    "Schema Enforcement bedeutet soll garantieren, dass keine Daten mit falschen Datentyp der Tabelle abgefügt werden  \n",
    "\n",
    "**Aufgabe:** verstehe die Option *mergeSchema* im Kontext von Datentyp Änderungen bei bestehenden Spalten, funktioniert das?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7ca43040",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Failed to merge fields 'id' and 'id'. Failed to merge incompatible data types LongType and StringType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# füge eine Zeile mit falschen Datetyp für eine bestehenden Spalte an\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m write_delta\u001b[38;5;241m=\u001b[39m(\u001b[43mdf2\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m           \u001b[49m\u001b[38;5;66;43;03m# Eine Zeile aus dem df2 filtern\u001b[39;49;00m\n\u001b[1;32m      4\u001b[0m \u001b[43m           \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maccount\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpeter\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m           \u001b[49m\u001b[38;5;66;43;03m# Bestehenden Spaltentyp ändern\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m           \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstring\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m           \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m           \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdelta\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m           \u001b[49m\u001b[38;5;66;43;03m# Bei Delta kann beim Schreiben gesetzt werden ob die Tabelle erweitert werden kann oder nicht, Default ist false\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m           \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmergeSchema\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m           \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m           \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ms3://\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mbucket\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/delta\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m           )\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/readwriter.py:968\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave()\n\u001b[1;32m    967\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 968\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Failed to merge fields 'id' and 'id'. Failed to merge incompatible data types LongType and StringType"
     ]
    }
   ],
   "source": [
    "# füge eine Zeile mit falschen Datetyp für eine bestehenden Spalte an\n",
    "write_delta=(df2\n",
    "           # Eine Zeile aus dem df2 filtern\n",
    "           .where(f.col(\"account\")==\"peter\")\n",
    "           # Bestehenden Spaltentyp ändern\n",
    "           .withColumn(\"id\", f.col(\"id\").cast(\"string\"))\n",
    "           .write\n",
    "           .format(\"delta\")\n",
    "           # Bei Delta kann beim Schreiben gesetzt werden ob die Tabelle erweitert werden kann oder nicht, Default ist false\n",
    "           .option(\"mergeSchema\", \"true\")\n",
    "           .mode(\"overwrite\") \n",
    "           .save(f\"s3://{bucket}/delta\")\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09fd745",
   "metadata": {},
   "source": [
    "#### Ergebnis: geht nicht! \n",
    "Delta garantiert Schema Enforcement on write. Die Option \"mergeSchema\", \"true\" gilt nur für Schema Erweiterung, also dem Anfügen neuer Spalten, \n",
    "nicht aber dem ändern von bestehenden Datetypen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11007f7c",
   "metadata": {},
   "source": [
    "### Delta: Schema Replacement\n",
    "Delta bieten die Möglichkeit das Schema einer Tabelle zu ändern, also z.B. eine bestehende Spalte umzubenennen und deren Datentyp zu ändern.   \n",
    "Dokumentation mit Beispielen: https://docs.delta.io/latest/delta-batch.html#replace-table-schema  \n",
    "\n",
    "**Aufgabe:** Ändere in der bestehenden Tabelle den Datentyp der Spalte `id` auf `string` und den Spaltennamen `new` zu `comment`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8608f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update das Schema der bestehenen Delta option(\"overwriteSchema\", \"true\")\n",
    "write_delta=(df2\n",
    "           # Eine Zeile aus dem df2 filtern\n",
    "           .where(f.col(\"account\")==\"peter\")\n",
    "           # Bestehenden Spaltentyp ändern\n",
    "           .withColumn(\"id\", f.col(\"id\").cast(\"string\"))\n",
    "           .write\n",
    "           .format(\"delta\") \n",
    "           .option(\"overwriteSchema\", \"true\") # wichtig Overwrite Schema geht nur mit dem Mode overwrite, also ein neues Schema für die neuen Dateien, es geht nicht mit einem Append alle Dateien zu altern\n",
    "           .mode(\"overwrite\") \n",
    "           .save(f\"s3://{bucket}/delta\")\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d0905b",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_delta=spark.read.format(\"delta\").load(f\"s3://{bucket}/delta\")\n",
    "read_delta.printSchema()\n",
    "read_delta.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee87f0a1",
   "metadata": {},
   "source": [
    "### Delta: History und Metadaten\n",
    "Im Delta Log werde alle Transaktionen mit zahlreichen Metadaten gespeichert. Das Spark Modul, der Treiber, um diese Daten auszulesen bietet zahlreiche Möglichkeiten diese Daten zu analysieren\n",
    "\n",
    "**Aufgabe:** Lese den History Log ein und verstehe was in den einzelnen Attributen steht. Treffe eine Auswahl der interessanten Informationen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb3480c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erzeuge ein DeltaTable Objekt was alle Zusatzeigenschaften von Delta bereitstellt\n",
    "deltaTable = DeltaTable.forPath(spark, f\"s3://{bucket}/delta\")\n",
    "\n",
    "# Historie aus den Delta Logs erzeugen\n",
    "fullHistoryDF = deltaTable.history() \n",
    "\n",
    "# Alle verfügbaren Spalten anzeigen\n",
    "fullHistoryDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac186df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wähle die wichtigen Felder aus der Historie aus und zeige sie an\n",
    "fullHistoryDF.select(\"version\",\"readVersion\",\"timestamp\",\"userId\",\"operation\",\"operationParameters\",\"operationMetrics\",\"userMetadata\").show(truncate=False)\n",
    "\n",
    "# Löse die genested Spalten in eigene Spalten auf und zeige folgende Attribute in eigenen Spalten `mode`, `numFiles` und `numOutputRows` \n",
    "fullHistoryDF.select(\"version\",\"readVersion\",\"timestamp\",\"operation\",\"operationParameters.mode\",\"operationMetrics.numFiles\",\"operationMetrics.numOutputRows\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f89cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erkunde die Metadaten aus der Funktion deltaTable.detail()\n",
    "\n",
    "deltaTable.detail().select(\"format\",\"name\",\"location\",\"createdAt\",\"lastModified\",\"numFiles\",\"tableFeatures\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd35736",
   "metadata": {},
   "source": [
    "<details style=\"border: 1px solid #aaa; border-radius: 4px; padding: 0.5em 0.5em 0; background-color:#F5F5F5\" class=\"solution\" >\n",
    "<summary style=\"margin: -0.5em -0.5em 0; padding: 0.5em;\"></summary>\n",
    "<code>\n",
    "deltaTable = DeltaTable.forPath(spark, f\"s3://{bucket}/delta\")\n",
    "detailsDF = deltaTable.detail()\n",
    "detailsDF.show()\n",
    "detailsDF.select(\"location\",\"numFiles\",\"tableFeatures\").show(truncate=False)\n",
    "</code>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc61efc",
   "metadata": {},
   "source": [
    "### Delta: Time Travel\n",
    "Die Time Travel Funktion von Delta ermöglicht es den Zustand der Datentabelle zu einem bestimmten Zeitpunkt in der Vergangenheit wiederherzustellen oder Änderungen zu verfolgen.   \n",
    "Time Travel ermöglicht es, vorherige Versionen der Daten abzufragen und historische Analysen durchzuführen, ohne auf separate Backups oder Snapshots angewiesen zu sein.\n",
    "\n",
    "**Aufgabe:** lese verschiedenen Datenstände nach Versionsnummer oder Timestap ein.  \n",
    "Weitere Informationen hierzu finden sich in https://delta.io/blog/2023-02-01-delta-lake-time-travel/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6617afc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read by version\n",
    "spark.read.format(\"delta\").option(\"versionAsOf\", \"8\").load(f\"s3://{bucket}/delta\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7e463b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read by timestamp\n",
    "spark.read.format(\"delta\").option(\"timestampAsOf\", \"2023-07-20 15:55:00\").load(f\"s3://{bucket}/delta\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bfe526",
   "metadata": {},
   "source": [
    "### Delta: Roleback\n",
    "Delta bietet die Möglichkeiten direkt auf den Datenstand einer bestimmten Version oder eines Zeitpunktes zurück zu gehen\n",
    "\n",
    "**Aufgabe:** Setzte die aktuelle Version der deltaTable auf den Anfangszusatand des df1 mit vier Zeilen zurück"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d1ea2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verwende die DeltaTable Funktion restoreToVersion(0) oder restoreToTimestamp(\"yyyy-mm-dd\")\n",
    "deltaTable2 = DeltaTable.forPath(spark, f\"s3://{bucket}/delta\")\n",
    "\n",
    "deltaTable2.restoreToVersion(0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb86cae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "deltaTable2.toDF().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85fcee1",
   "metadata": {},
   "source": [
    "<details style=\"border: 1px solid #aaa; border-radius: 4px; padding: 0.5em 0.5em 0; background-color:#F5F5F5\" class=\"solution\" >\n",
    "<summary style=\"margin: -0.5em -0.5em 0; padding: 0.5em;\">&#8964 Hilfreiche Befehle</summary>\n",
    "<code>\n",
    "deltaTable2.toDF().show()\n",
    "\n",
    "deltaTable2.restoreToVersion(0)\n",
    "#restoreToTimestamp\n",
    "#isDeltaTable\n",
    "deltaTable2.toDF().show()\n",
    "spark.read.format(\"delta\").load(f\"s3://{bucket}/delta\").show()\n",
    "\n",
    "</code>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55d249d",
   "metadata": {},
   "source": [
    "### Delta: Merge (Upsert)\n",
    "Delta bietet die Möglichkeiten auf bestehenden Dateien ein Daten Upsert durchzuführen.   \n",
    "Ubsert oder Merge bedeutet zu prüfen ob es die neue Zeile für einen bestimmten Schlüssel schon in den Daten gibt und wenn ja diese zu updaten und wenn nein sie neu hinzuzufügen\n",
    "\n",
    "**Aufgabe:** Merge den Datensatz df2 mit dem der Änderung der `balance` für `Alex` und überprüfe ob das Update korrekt war"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aea3fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spalte anfügen, da merge nur funktioniert wenn das Schema stimmt\n",
    "df2a=df2\n",
    "\n",
    "print(\"++ Datensatz der auf bestehende Daten upserted/merged werden soll\")\n",
    "df2a.show()\n",
    "print(\"++ Bestehender Datensatz auf s3\")\n",
    "deltaTable2.toDF().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1688108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verwendung der merge Funktion (es gibt auch eine update() oder delete() Funktion)\n",
    "dt3=(deltaTable2.alias(\"oldData\")\n",
    "      .merge(df2a.alias(\"newData\"),\n",
    "            \"oldData.account = newData.account AND oldData.dt_transaction = newData.dt_transaction\")\n",
    "            .whenMatchedUpdateAll()\n",
    "            .whenNotMatchedInsertAll()\n",
    "      .execute()\n",
    "    )\n",
    "\n",
    "deltaTable2.toDF().sort(f.col(\"account\"),f.col(\"dt_transaction\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c827448",
   "metadata": {},
   "source": [
    "### Delta: SCD2 Historisierung\n",
    "Mit der Merge Funktion von Delta lässt sich z.B. einfach eine Datenhistorisierung nach dem SCD2 Prinzip realisieren  \n",
    "(Slowly Changing Dimensions Typ 2, https://de.wikipedia.org/wiki/Slowly_Changing_Dimensions) \n",
    "SCD2 ermöglicht es Änderungen einzelner Datensätze zu tracken. Typischerweise werden neben dem Änderungsdatum auch noch Metadaten wie Grund und User mit gespeichert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dde08bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+-------+-------------------+--------+----------+\n",
      "|account|dt_transaction|balance|valid_from         |valid_to|is_current|\n",
      "+-------+--------------+-------+-------------------+--------+----------+\n",
      "|maria  |2020-01-01    |5000   |2023-06-01 00:00:00|null    |true      |\n",
      "|alex   |2019-01-01    |1000   |2023-06-01 00:00:00|null    |true      |\n",
      "|alex   |2019-03-01    |1700   |2023-06-01 00:00:00|null    |true      |\n",
      "|alex   |2019-02-01    |1500   |2023-06-01 00:00:00|null    |true      |\n",
      "+-------+--------------+-------+-------------------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initial Dataframe erstellen\n",
    "df_initial = df1\n",
    "\n",
    "\n",
    "init_load_ts = \"2023-06-01\"\n",
    "\n",
    "# Die drei SCD2 Spalten valid_from, valid_to und is_current hinzufügen\n",
    "df_initial = (df_initial\n",
    "              .drop(\"id\")\n",
    "              .withColumn('valid_from', f.lit(init_load_ts).cast(\"timestamp\"))\n",
    "              .withColumn('valid_to', f.lit(None).cast(\"timestamp\"))\n",
    "              .withColumn('is_current', f.lit(True))\n",
    "             )\n",
    "\n",
    "# in neue Deltatabelle=Datei schreiben\n",
    "df_initial.write.format('delta').mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(f\"s3://{bucket}/delta_scd2\")\n",
    "dTable_initial = DeltaTable.forPath(spark, f\"s3://{bucket}/delta_scd2\")\n",
    "df_initial=dTable_initial.toDF()\n",
    "\n",
    "df_initial.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "10424bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+-------+--------------------------+--------+----------+------+\n",
      "|account|dt_transaction|balance|valid_from                |valid_to|is_current|action|\n",
      "+-------+--------------+-------+--------------------------+--------+----------+------+\n",
      "|peter  |2021-01-01    |100    |2023-07-21 10:16:35.890389|null    |true      |insert|\n",
      "|alex   |2019-03-01    |3300   |2023-07-21 10:16:35.890389|null    |true      |insert|\n",
      "|maria  |2020-01-01    |5000   |2023-07-21 10:16:35.890389|null    |true      |insert|\n",
      "+-------+--------------+-------+--------------------------+--------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    " # Update Dataframe vorbereiten\n",
    "df_current = (df2\n",
    "          # Duplikat hinzufügen\n",
    "          .unionByName(df1.where(f.col(\"account\")==\"maria\"))\n",
    "          # komplexität reduzieren\n",
    "          .drop(\"id\")\n",
    "          # der neue Datensatz gilt ab jetzt\n",
    "          .withColumn('valid_from', f.lit(f.current_timestamp()))\n",
    "          # und gilt wieder bis in die unendlichkeit\n",
    "          .withColumn('valid_to', f.lit(None).cast(\"timestamp\"))\n",
    "          .withColumn('is_current', f.lit(True))\n",
    "          # Hilfsspalte um zu markieren was mit der Zeile gemacht werden soll, new insert, update existing, delete row. Erstmal alles auf update\n",
    "          .withColumn('action', f.lit('insert'))\n",
    "         ).cache()\n",
    "\n",
    "df_current.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c3eb6b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++ Zeilen die es schon gibt aber deren Wert sich geändert haben werden deaktiviert\n",
      "+-------+--------------+-------+-------------------+--------------------------+----------+----------+\n",
      "|account|dt_transaction|balance|valid_from         |valid_to                  |is_current|action    |\n",
      "+-------+--------------+-------+-------------------+--------------------------+----------+----------+\n",
      "|alex   |2019-03-01    |1700   |2023-06-01 00:00:00|2023-07-21 10:16:37.421142|false     |deactivate|\n",
      "+-------+--------------+-------+-------------------+--------------------------+----------+----------+\n",
      "\n",
      "Zeilen die es schon gibt, wo sich aber nichts ändert müssen von der Ubsert Liste entfernt werden\n",
      "+-------+--------------+-------+-------------------+--------+----------+---------+\n",
      "|account|dt_transaction|balance|valid_from         |valid_to|is_current|action   |\n",
      "+-------+--------------+-------+-------------------+--------+----------+---------+\n",
      "|maria  |2020-01-01    |5000   |2023-06-01 00:00:00|null    |true      |duplicate|\n",
      "+-------+--------------+-------+-------------------+--------+----------+---------+\n",
      "\n",
      "Jetzt werden die neuen Zeilen mit den Deaktivierten Zeilen vereint und dann die Duplicate entfernt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+-------+--------------------------+--------------------------+----------+----------+\n",
      "|account|dt_transaction|balance|valid_from                |valid_to                  |is_current|action    |\n",
      "+-------+--------------+-------+--------------------------+--------------------------+----------+----------+\n",
      "|peter  |2021-01-01    |100    |2023-07-21 10:16:42.551346|null                      |true      |insert    |\n",
      "|alex   |2019-03-01    |3300   |2023-07-21 10:16:42.551346|null                      |true      |insert    |\n",
      "|alex   |2019-03-01    |1700   |2023-06-01 00:00:00       |2023-07-21 10:16:42.551346|false     |deactivate|\n",
      "+-------+--------------+-------+--------------------------+--------------------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Finde Zeilen die es schon gibt und die einen neuen Wert für Balance haben. \n",
    "# Erzeuge den Update Eintrag um diese Zeile zu deaktivieren\n",
    "df_rows_to_deactivate = (df_initial.alias(\"initial\")\n",
    "                .join(df_current.alias(\"current\"), \n",
    "                      (df_initial.account == df_current.account) & \n",
    "                      (df_initial.dt_transaction == df_current.dt_transaction) \n",
    "                      , how='inner'\n",
    "                     )\n",
    "                .where((f.col(\"initial.balance\")!=f.col(\"current.balance\")) & (f.col(\"initial.valid_to\").isNull()))\n",
    "                .select(\"initial.*\")\n",
    "                .withColumn(\"action\",f.lit(\"deactivate\"))\n",
    "                .withColumn(\"valid_to\",f.current_timestamp())\n",
    "                .withColumn(\"is_current\",f.lit(False))\n",
    "               )\n",
    "\n",
    "print(\"++ Zeilen die es schon gibt aber deren Wert sich geändert haben werden deaktiviert\")\n",
    "df_rows_to_deactivate.show(truncate=False)\n",
    "\n",
    "# Finde Zeilen die es schon gibt und wo sich kein Wert geändert hat, sie bleiben erhalten\n",
    "df_rows_do_not_touch= (df_initial.alias(\"initial\")\n",
    "                .join(df_current.alias(\"current\"), \n",
    "                      (df_initial.account == df_current.account) & \n",
    "                      (df_initial.dt_transaction == df_current.dt_transaction) \n",
    "                      , how='inner'\n",
    "                     )\n",
    "                .where((f.col(\"initial.balance\")==f.col(\"current.balance\")))\n",
    "                .select(\"initial.*\")\n",
    "                .withColumn(\"action\",f.lit(\"duplicate\"))\n",
    "               )\n",
    "\n",
    "\n",
    "print(\"Zeilen die es schon gibt, wo sich aber nichts ändert müssen von der Ubsert Liste entfernt werden\")\n",
    "df_rows_do_not_touch.show(truncate=False)\n",
    "\n",
    "\n",
    "print(\"Jetzt werden die neuen Zeilen mit den Deaktivierten Zeilen vereint und dann die Duplicate entfernt\")\n",
    "df_rows_for_upsert=(df_current\n",
    "                # füge als extra Zeile die deaktiverten Einträge hinzu\n",
    "                .union(df_rows_to_deactivate)\n",
    "                # bereits existierende Zeilen entfernen\n",
    "                .join(df_rows_do_not_touch,\n",
    "                      (df_current.account == df_rows_do_not_touch.account) & \n",
    "                      (df_current.dt_transaction == df_rows_do_not_touch.dt_transaction) &\n",
    "                      (df_current.balance == df_rows_do_not_touch.balance),\n",
    "                      how=\"left_anti\"\n",
    "                     )\n",
    "                 )\n",
    "\n",
    "\n",
    "df_rows_for_upsert.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "74f78063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+-------+-------------------+--------+----------+\n",
      "|account|dt_transaction|balance|         valid_from|valid_to|is_current|\n",
      "+-------+--------------+-------+-------------------+--------+----------+\n",
      "|  maria|    2020-01-01|   5000|2023-06-01 00:00:00|    null|      true|\n",
      "|   alex|    2019-01-01|   1000|2023-06-01 00:00:00|    null|      true|\n",
      "|   alex|    2019-03-01|   1700|2023-06-01 00:00:00|    null|      true|\n",
      "|   alex|    2019-02-01|   1500|2023-06-01 00:00:00|    null|      true|\n",
      "+-------+--------------+-------+-------------------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fc165ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+-------+--------------------+--------------------+----------+\n",
      "|account|dt_transaction|balance|          valid_from|            valid_to|is_current|\n",
      "+-------+--------------+-------+--------------------+--------------------+----------+\n",
      "|   alex|    2019-01-01|   1000| 2023-06-01 00:00:00|                null|      true|\n",
      "|   alex|    2019-02-01|   1500| 2023-06-01 00:00:00|                null|      true|\n",
      "|   alex|    2019-03-01|   1700| 2023-06-01 00:00:00|2023-07-21 10:18:...|     false|\n",
      "|   alex|    2019-03-01|   3300|2023-07-21 10:18:...|                null|      true|\n",
      "|  maria|    2020-01-01|   5000| 2023-06-01 00:00:00|                null|      true|\n",
      "|  peter|    2021-01-01|    100|2023-07-21 10:18:...|                null|      true|\n",
      "+-------+--------------+-------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mergeTable=(dTable_initial.alias(\"old\")\n",
    "      .merge(df_rows_for_upsert.drop(\"action\").alias(\"new\"),\n",
    "             condition = \"old.account = new.account AND old.dt_transaction = new.dt_transaction AND old.balance = new.balance AND old.is_current = True\")\n",
    "            .whenMatchedUpdate(\n",
    "                set = {\n",
    "                \"dt_transaction\": f.col(\"old.dt_transaction\"),\n",
    "                \"account\": f.col(\"old.account\"),\n",
    "                \"balance\": f.col(\"old.balance\"),\n",
    "                \"valid_from\":f.col(\"old.valid_from\"),\n",
    "                \"valid_to\": f.current_timestamp(),\n",
    "                \"is_current\": f.lit(False)\n",
    "            })\n",
    "            .whenNotMatchedInsertAll()\n",
    "      .execute()\n",
    "    )\n",
    "            \n",
    "dTable_initial.toDF().sort(\"account\",\"dt_transaction\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9266229a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++ Vorher ohne deactivierte Zeile und ohne neue Zeilen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+-------+--------------------+--------------------+----------+\n",
      "|account|dt_transaction|balance|          valid_from|            valid_to|is_current|\n",
      "+-------+--------------+-------+--------------------+--------------------+----------+\n",
      "|   alex|    2019-03-01|   1700| 2023-06-01 00:00:00|2023-07-21 10:18:...|     false|\n",
      "|   alex|    2019-03-01|   3300|2023-07-21 10:18:...|                null|      true|\n",
      "|  peter|    2021-01-01|    100|2023-07-21 10:18:...|                null|      true|\n",
      "|  maria|    2020-01-01|   5000| 2023-06-01 00:00:00|                null|      true|\n",
      "|   alex|    2019-01-01|   1000| 2023-06-01 00:00:00|                null|      true|\n",
      "|   alex|    2019-02-01|   1500| 2023-06-01 00:00:00|                null|      true|\n",
      "+-------+--------------+-------+--------------------+--------------------+----------+\n",
      "\n",
      "++ Nach Upsert mit SCD2 Historisierung\n",
      "+-------+--------------+-------+--------------------------+--------------------------+----------+\n",
      "|account|dt_transaction|balance|valid_from                |valid_to                  |is_current|\n",
      "+-------+--------------+-------+--------------------------+--------------------------+----------+\n",
      "|alex   |2019-01-01    |1000   |2023-06-01 00:00:00       |null                      |true      |\n",
      "|alex   |2019-02-01    |1500   |2023-06-01 00:00:00       |null                      |true      |\n",
      "|alex   |2019-03-01    |1700   |2023-06-01 00:00:00       |2023-07-21 10:18:18.101335|false     |\n",
      "|alex   |2019-03-01    |3300   |2023-07-21 10:18:18.101335|null                      |true      |\n",
      "|maria  |2020-01-01    |5000   |2023-06-01 00:00:00       |null                      |true      |\n",
      "|peter  |2021-01-01    |100    |2023-07-21 10:18:18.101335|null                      |true      |\n",
      "+-------+--------------+-------+--------------------------+--------------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"++ Vorher ohne deactivierte Zeile und ohne neue Zeilen\")\n",
    "dTable_initial.toDF().show()\n",
    "\n",
    "print(\"++ Nach Upsert mit SCD2 Historisierung\")\n",
    "dTable_initial.toDF().sort(\"account\",\"dt_transaction\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2b755b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++ aktuell gültiger Rand\n",
      "+-------+--------------+-------+--------------------------+--------+----------+\n",
      "|account|dt_transaction|balance|valid_from                |valid_to|is_current|\n",
      "+-------+--------------+-------+--------------------------+--------+----------+\n",
      "|alex   |2019-01-01    |1000   |2023-06-01 00:00:00       |null    |true      |\n",
      "|alex   |2019-02-01    |1500   |2023-06-01 00:00:00       |null    |true      |\n",
      "|alex   |2019-03-01    |3300   |2023-07-21 10:18:18.101335|null    |true      |\n",
      "|maria  |2020-01-01    |5000   |2023-06-01 00:00:00       |null    |true      |\n",
      "|peter  |2021-01-01    |100    |2023-07-21 10:18:18.101335|null    |true      |\n",
      "+-------+--------------+-------+--------------------------+--------+----------+\n",
      "\n",
      "++ Veränderung der Daten von account alex am ersten März\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+-------+--------------------------+--------------------------+----------+\n",
      "|account|dt_transaction|balance|valid_from                |valid_to                  |is_current|\n",
      "+-------+--------------+-------+--------------------------+--------------------------+----------+\n",
      "|alex   |2019-03-01    |1700   |2023-06-01 00:00:00       |2023-07-21 10:18:18.101335|false     |\n",
      "|alex   |2019-03-01    |3300   |2023-07-21 10:18:18.101335|null                      |true      |\n",
      "+-------+--------------+-------+--------------------------+--------------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"++ aktuell gültiger Rand\")\n",
    "dTable_initial.toDF().where(f.col(\"is_current\")==True).sort(\"account\",\"dt_transaction\").show(truncate=False)\n",
    "\n",
    "print(\"++ Veränderung der Daten von account alex am ersten März\")\n",
    "dTable_initial.toDF().where((f.col(\"account\")==\"alex\") & (f.col(\"dt_transaction\")==\"2019-03-01\") ).sort(\"account\",\"valid_from\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe344799",
   "metadata": {},
   "source": [
    "#### Erkenntnisse Delta\n",
    "* Wie funktioniert das Metadatenmanagement und was steht im Delta Log?\n",
    "* Schema Evolution: Kann das Schema erweitert werden, also eine neue Spalte angefügt werden?\n",
    "* Schema Enforcement on write: Kann eine Spalte mit falschem Datetyp einfach beim schreiben hinzugefügt werden? \n",
    "* Schema Enforcement on read: Kann ein Verzeichnis mit mehreren Parquet Dateien bei der eine Spalte ein anderes Schema hat gelesen werden? (um die Ecke Denk Frage)\n",
    "* Was ermöglichen mir die Metadaten (Historie, Audit etc)\n",
    "* Was ist der Vorteil der Merge Funktion? Wie müsste ich sonst Dateibasiert einen Merge/Update durchführen?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5956eb",
   "metadata": {},
   "source": [
    "<hr style=\"height: 3px; background: gray;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b85012",
   "metadata": {},
   "source": [
    "## Iceberg\n",
    "Das Iceberg Format Ist vergleichbar mit dem Delta Format Parquet, Avro oder ORC Dateien einen zusätzlichen Layer an Metadatan hinzu und erfüllt mit dem entsprechenden Treiber alle ACID Eigenschaften einer Datenbank.   \n",
    "**Unterschied zu Delta:**  \n",
    "* Das Delta Format verwendet ein Transaktionsprotokoll, um inkrementelle Updates zu erfassen und eine vollständige Versionierung der Daten zu ermöglichen. Dies bedeutet, dass Delta Änderungen an den Daten in einem einzelnen Parquet-Dateisystem erfasst. Delta ist eng mit Spark und der Delta Lake Plattform von Databricks verbunden und verwedet als darunterliegendes Datenformat immer Parqeut Dateien\n",
    "* Iceberg hat ebenfalls eine append-only-Architektur, bei der Daten in sogenannten \"Snapshot-Dateien\" gespeichert werden. Iceberg ist älter und unabhängig von einer speziellen Plattform. Daher es gibt wesentlich mehr Big Data Engines die Iceberg voll unterstützen (Spark, Hive, Trino, Drill, Presto).\n",
    "\n",
    "**Naming Unterschiede:**  \n",
    "Bei Delta spricht man von `Versionen` und `Time Travel`. \n",
    "Bei Iceberg heißt dies `Snapshots` und `Snapshot roleback`  \n",
    "\n",
    "Eine weitere Besonderheit bei Iceberg ist, dass es für Daten die Möglichkeiten von Branching und Tagging gibt. Es können also Variationen der Daten in einem z.B. Development Branch gepflegt und bearbeitet weiter entwickelt werden (https://iceberg.apache.org/docs/latest/branching/)\n",
    "\n",
    "Im Unterchied zu Delta benötigt Spark zum arbeiten mit Iceberg immer einen Catalog oder Metastore um Details über die Dateien abzulegen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db30780",
   "metadata": {},
   "source": [
    "### Iceberg: Catalog und Metastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cb24f4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|      catalog|\n",
      "+-------------+\n",
      "|spark_catalog|\n",
      "+-------------+\n",
      "\n",
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|     data|\n",
      "|  default|\n",
      "|   export|\n",
      "|  iceberg|\n",
      "+---------+\n",
      "\n",
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "|  iceberg|  iceberg|      false|\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Anzeigen der in Spark eingebundenen Metadaten Cataloge\n",
    "# Diese Spark Session wurde mit einer Anbindung an den Hive Metastore gestartet\n",
    "## Konfiguration aus für die Spark Session erzeugt einen Catalog ice vom Typ hive\n",
    "## conf.set(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\")\n",
    "## conf.set(\"spark.sql.catalog.ice\",\"org.apache.iceberg.spark.SparkCatalog\") \n",
    "## conf.set(\"spark.sql.catalog.ice.type\",\"hive\") \n",
    "## conf.set(\"spark.sql.catalog.ice.uri\",\"thrift://hive-metastore.hive.svc.cluster.local:9083\") \n",
    "\n",
    "\n",
    "spark.sql(\"SHOW catalogs\").show()\n",
    "spark.sql(\"SHOW databases from ice\").show()\n",
    "spark.sql(\"show tables from ice.iceberg\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7377b8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zunächst muss in dem Ice Catalog eine Datenbankabstraktion angelegt werden mit dem s3 Pfad wo die Daten zu dieser Tabellen liegen\n",
    "# create a Database(name=<db_name>, locationUri='s3a://<bucket>/')\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS ice.iceberg LOCATION 's3a://{bucket}/'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "59b8232c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|     data|\n",
      "|  default|\n",
      "|   export|\n",
      "|  iceberg|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Jetzt erscheint diese Datenbank im Iceberg Catalog\n",
    "spark.sql(\"SHOW databases from ice\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5010f061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "|  iceberg|  iceberg|      false|\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Es gibt aber noch keine Tabellen in dieser Datenbankabstraktion\n",
    "spark.sql(\"show tables from ice.iceberg\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cba4b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Die Tabelle kann ach wieder aus dem Hive Metastore, aus dem Catalog gelöscht werden\n",
    "# !! Wichtig immer erst die Tabelle UND das Schema aus dem Metastore löschen bevor die Daten in s3 entfernt werden\n",
    "#spark.sql(\"drop table iceberg.iceberg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafa7b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Um Daten unter Verwendung des Catalogs nach s3 zu schreiben wird nicht der direkte Pfad verwendet sondern die Datenbank Referenz wo der Pfad hinterlegt ist\n",
    "# Die Datenbank iceberg -> s3://bucket/\n",
    "# die Tabelle soll jetzt nach ->s3://bucket/iceberg\n",
    "# geschrieben wird mit der Methode `saveAsTable(\"datenbank.tabelle\")` und der Tabellennamen wir dann als Prefix auf s3 verwendet\n",
    "write_iceberg=(df1\n",
    "                  .write\n",
    "                  .format(\"iceberg\")\n",
    "                  .mode(\"overwrite\")\n",
    "                  .saveAsTable(\"ice.iceberg.iceberg\")\n",
    "               )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "494fb421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iceberg/data/00000-7912-ace0e8c1-fb4a-4812-bdf3-6d4c0ad58a06-00001.parquet\n",
      "iceberg/data/00000-7920-9dd85db3-402a-4657-856c-f0e8526d3256-00001.parquet\n",
      "iceberg/data/00001-7913-df5b342a-250c-4439-a721-bf25810804f3-00001.parquet\n",
      "iceberg/data/00002-7914-112c31b7-0460-4fed-b4c9-8b9a5f1dbfd5-00001.parquet\n",
      "iceberg/metadata/00000-d4590277-2b88-433d-aa9f-8ba595501aef.metadata.json\n",
      "iceberg/metadata/00001-fb578df1-af09-4ce8-854a-d66befc4519b.metadata.json\n",
      "iceberg/metadata/00002-becfce86-8178-4223-8c6d-84baaf06b31b.metadata.json\n",
      "iceberg/metadata/0a07116f-9521-4d1d-bc4a-3a729535f621-m0.avro\n",
      "iceberg/metadata/4d380afb-46c9-4d4c-a877-135bf0ec57a9-m0.avro\n",
      "iceberg/metadata/snap-2832191032484601030-1-0a07116f-9521-4d1d-bc4a-3a729535f621.avro\n",
      "iceberg/metadata/snap-868766759905674702-1-4d380afb-46c9-4d4c-a877-135bf0ec57a9.avro\n"
     ]
    }
   ],
   "source": [
    "# Versuche auf dem Dateisystem mit ls und cat die Struktur der Metadaten zu verstehen\n",
    "# Dieser Artikel kann dabei helfen https://medium.com/snowflake/understanding-iceberg-table-metadata-b1209fbcc7c3\n",
    "ls(bucket,\"iceberg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6c844c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------+-------+-------------+\n",
      "| id|account|dt_transaction|balance|          new|\n",
      "+---+-------+--------------+-------+-------------+\n",
      "|  1|   otto|    2019-10-01|   4444|neue Spalte 1|\n",
      "|  1|   alex|    2019-01-01|   1000|         null|\n",
      "|  2|   alex|    2019-02-01|   1500|         null|\n",
      "|  4|  maria|    2020-01-01|   5000|         null|\n",
      "|  3|   alex|    2019-03-01|   1700|         null|\n",
      "+---+-------+--------------+-------+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 312:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------+-------+-------------+\n",
      "| id|account|dt_transaction|balance|          new|\n",
      "+---+-------+--------------+-------+-------------+\n",
      "|  1|   alex|    2019-01-01|   1000|         null|\n",
      "|  2|   alex|    2019-02-01|   1500|         null|\n",
      "|  4|  maria|    2020-01-01|   5000|         null|\n",
      "|  3|   alex|    2019-03-01|   1700|         null|\n",
      "|  1|   otto|    2019-10-01|   4444|neue Spalte 1|\n",
      "+---+-------+--------------+-------+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# zeige die Daten wieder an\n",
    "iceberg_df = spark.read.table(\"ice.iceberg.iceberg\")\n",
    "iceberg_df.show()\n",
    "\n",
    "# Da es sich bereits um eine Tabellenabstraktion handelt, kann auch direkt mit Spark SQL gelesen werden\n",
    "spark.sql(\"SELECT * FROM ice.iceberg.iceberg;\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a6911c",
   "metadata": {},
   "source": [
    "### Iceberg: Schema Evolution\n",
    "**Aufgabe:** Verstehe wie eine Spaltenerweiterung bei Iceberg gemacht wird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92013c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Füge Datensatz 2 mit einer zusätzlichen Spalte hinzu\n",
    "write_iceberg=(df3\n",
    "               .write\n",
    "               .format(\"iceberg\")\n",
    "               .option(\"mergeSchema\",\"true\")\n",
    "               .mode(\"append\")\n",
    "               .saveAsTable(\"ice.iceberg.iceberg\")\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3946b31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Um eine neue Spalte hinzuzufügen muss das Schema wie bei einer SQL Tabelle zuerst geändert werden. Es gibt keine automatische Anpassung wie bei Delta via die Option mergeSchema\n",
    "# Ändere das Schema mit folgendem Befehl und führe dann obige Schreiboperation nochmal durch\n",
    "spark.sql(\"ALTER TABLE ice.iceberg.iceberg ADD COLUMNS (new VARCHAR(50))\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a31c068c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------+-------+-------------+\n",
      "| id|account|dt_transaction|balance|          new|\n",
      "+---+-------+--------------+-------+-------------+\n",
      "|  1|   otto|    2019-10-01|   4444|neue Spalte 1|\n",
      "|  1|   alex|    2019-01-01|   1000|         null|\n",
      "|  2|   alex|    2019-02-01|   1500|         null|\n",
      "|  4|  maria|    2020-01-01|   5000|         null|\n",
      "|  3|   alex|    2019-03-01|   1700|         null|\n",
      "+---+-------+--------------+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM ice.iceberg.iceberg;\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab84274",
   "metadata": {},
   "source": [
    "### Iceberg: Schema Enforcement\n",
    "\n",
    "**Aufgabe:** verstehe wie Schema Enforcement bei Iceberg funktioniert. Korrigiere den Code bis die Zeile korrekt angefügt werden kann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e813db3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR Writing to Iceberg\n",
      "Cannot write incompatible data to table 'ice.iceberg.iceberg':\n",
      "- Cannot safely cast 'id': string to bigint\n",
      "- Cannot find data for output column 'new'\n"
     ]
    }
   ],
   "source": [
    "# füge eine Zeile mit falschen Datetyp für eine bestehenden Spalte an\n",
    "try:\n",
    "    write_iceberg=(df2\n",
    "           # Eine Zeile aus dem df2 filtern\n",
    "           .where(f.col(\"account\")==\"peter\")\n",
    "           # Bestehenden Spaltentyp ändern\n",
    "           .withColumn(\"id\", f.col(\"id\").cast(\"string\"))\n",
    "           #.withColumn(\"new\", f.lit(\"peter\").cast(\"string\"))\n",
    "           .write\n",
    "           .format(\"iceberg\")\n",
    "           .mode(\"append\") # append\n",
    "           .saveAsTable(\"ice.iceberg.iceberg\")\n",
    "          )\n",
    "    print(\"Zeile angefügt\")\n",
    "except Exception as error:\n",
    "    print(\"ERROR Writing to Iceberg\")\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75633ab2",
   "metadata": {},
   "source": [
    "In Iceberg kann der Typ einer bestehenden Tabelle nicht geändert werden.   \n",
    "Einzige Möglichkeit ist es die Tabelle einzulesen, neu zu casten und in einen neue Tabelle zu schreiben.   \n",
    "Diese kann dann anschließend wieder umbenannt werden auf den alten Namen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fbbf4d",
   "metadata": {},
   "source": [
    "### Iceberg: History und Metadaten\n",
    "Iceberg bietet zahlreiche Funktionen um die verschiedenen Metadaten auszulesen.   \n",
    "Die Funktionen werden immer an die Tabelle angefügt und funktionieren sowohl in der Dataframe als auch in der SQL Syntax\n",
    "```\n",
    "spark.read.table(\"ice.iceberg.iceberg.history\").show()\n",
    "spark.sql(\"SELECT * FROM ice.iceberg.iceberg.history;\").show()\n",
    "```\n",
    "\n",
    "**Aufgabe:** Untersuche und verstehe die Metadaten folgender Methoden:  \n",
    "`history`, `files`, `snapshots`, `manifests`, `partitions`\n",
    "\n",
    "Weitere Informationen zu den Metadaten und dem Zugrif darauf finden sich in der Dokumentation\n",
    "https://iceberg.apache.org/docs/latest/spark-queries/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "74d837af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-------------------+------------------+-------------------+\n",
      "|made_current_at        |snapshot_id        |parent_id         |is_current_ancestor|\n",
      "+-----------------------+-------------------+------------------+-------------------+\n",
      "|2023-07-21 00:04:45.452|868766759905674702 |null              |true               |\n",
      "|2023-07-21 00:08:50.595|2832191032484601030|868766759905674702|true               |\n",
      "+-----------------------+-------------------+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM ice.iceberg.iceberg.history;\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "60d0a9af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-------------------+---------+----------------+-------------+-------------+----------------+\n",
      "|committed_at           |snapshot_id        |operation|added-data-files|added-records|total-records|added-data-files|\n",
      "+-----------------------+-------------------+---------+----------------+-------------+-------------+----------------+\n",
      "|2023-07-21 00:04:45.452|868766759905674702 |append   |3               |4            |4            |3               |\n",
      "|2023-07-21 00:08:50.595|2832191032484601030|append   |1               |1            |5            |1               |\n",
      "+-----------------------+-------------------+---------+----------------+-------------+-------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM ice.iceberg.iceberg.snapshots;\").select(\"committed_at\",\"snapshot_id\",\"operation\",\"summary.added-data-files\",\"summary.added-records\",\"summary.total-records\",\"summary.added-data-files\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8c7b618e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- content: integer (nullable = true)\n",
      " |-- file_path: string (nullable = false)\n",
      " |-- file_format: string (nullable = false)\n",
      " |-- spec_id: integer (nullable = true)\n",
      " |-- record_count: long (nullable = false)\n",
      " |-- file_size_in_bytes: long (nullable = false)\n",
      " |-- column_sizes: map (nullable = true)\n",
      " |    |-- key: integer\n",
      " |    |-- value: long (valueContainsNull = false)\n",
      " |-- value_counts: map (nullable = true)\n",
      " |    |-- key: integer\n",
      " |    |-- value: long (valueContainsNull = false)\n",
      " |-- null_value_counts: map (nullable = true)\n",
      " |    |-- key: integer\n",
      " |    |-- value: long (valueContainsNull = false)\n",
      " |-- nan_value_counts: map (nullable = true)\n",
      " |    |-- key: integer\n",
      " |    |-- value: long (valueContainsNull = false)\n",
      " |-- lower_bounds: map (nullable = true)\n",
      " |    |-- key: integer\n",
      " |    |-- value: binary (valueContainsNull = false)\n",
      " |-- upper_bounds: map (nullable = true)\n",
      " |    |-- key: integer\n",
      " |    |-- value: binary (valueContainsNull = false)\n",
      " |-- key_metadata: binary (nullable = true)\n",
      " |-- split_offsets: array (nullable = true)\n",
      " |    |-- element: long (containsNull = false)\n",
      " |-- equality_ids: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = false)\n",
      " |-- sort_order_id: integer (nullable = true)\n",
      "\n",
      "+--------------------------------------------------------------------------------------------+------------+----------------------------------------+------------------------------------------------------------------------+----------------------------------------+\n",
      "|file_path                                                                                   |record_count|value_counts                            |lower_bounds                                                            |null_value_counts                       |\n",
      "+--------------------------------------------------------------------------------------------+------------+----------------------------------------+------------------------------------------------------------------------+----------------------------------------+\n",
      "|s3a://fileformats/iceberg/data/00000-7920-9dd85db3-402a-4657-856c-f0e8526d3256-00001.parquet|1           |{1 -> 1, 2 -> 1, 3 -> 1, 4 -> 1, 5 -> 1}|{1 -> \u0001\u0000\u0000\u0000\u0000\u0000\u0000\u0000, 2 -> otto, 3 -> �F\u0000\u0000, 4 -> \\\u0011\u0000\u0000\u0000\u0000\u0000\u0000, 5 -> neue Spalte 1}|{1 -> 0, 2 -> 0, 3 -> 0, 4 -> 0, 5 -> 0}|\n",
      "|s3a://fileformats/iceberg/data/00000-7912-ace0e8c1-fb4a-4812-bdf3-6d4c0ad58a06-00001.parquet|1           |{1 -> 1, 2 -> 1, 3 -> 1, 4 -> 1}        |{1 -> \u0001\u0000\u0000\u0000\u0000\u0000\u0000\u0000, 2 -> alex, 3 -> �E\u0000\u0000, 4 -> �\u0003\u0000\u0000\u0000\u0000\u0000\u0000}                    |{1 -> 0, 2 -> 0, 3 -> 0, 4 -> 0}        |\n",
      "|s3a://fileformats/iceberg/data/00001-7913-df5b342a-250c-4439-a721-bf25810804f3-00001.parquet|2           |{1 -> 2, 2 -> 2, 3 -> 2, 4 -> 2}        |{1 -> \u0002\u0000\u0000\u0000\u0000\u0000\u0000\u0000, 2 -> alex, 3 -> \\bF\u0000\u0000, 4 -> �\u0005\u0000\u0000\u0000\u0000\u0000\u0000}                   |{1 -> 0, 2 -> 0, 3 -> 0, 4 -> 0}        |\n",
      "|s3a://fileformats/iceberg/data/00002-7914-112c31b7-0460-4fed-b4c9-8b9a5f1dbfd5-00001.parquet|1           |{1 -> 1, 2 -> 1, 3 -> 1, 4 -> 1}        |{1 -> \u0003\u0000\u0000\u0000\u0000\u0000\u0000\u0000, 2 -> alex, 3 -> $F\u0000\u0000, 4 -> �\u0006\u0000\u0000\u0000\u0000\u0000\u0000}                    |{1 -> 0, 2 -> 0, 3 -> 0, 4 -> 0}        |\n",
      "+--------------------------------------------------------------------------------------------+------------+----------------------------------------+------------------------------------------------------------------------+----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iceber_files=spark.sql(\"SELECT * FROM ice.iceberg.iceberg.files;\")\n",
    "\n",
    "iceber_files.printSchema()\n",
    "iceber_files.select(\"file_path\",\"record_count\",\"value_counts\",\"lower_bounds\",\"null_value_counts\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c11d7be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-------------------+----------------+-------------+----------------+------------------+\n",
      "|committed_at           |snapshot_id        |added-data-files|added-records|total-data-files|total-delete-files|\n",
      "+-----------------------+-------------------+----------------+-------------+----------------+------------------+\n",
      "|2023-07-21 00:04:45.452|868766759905674702 |3               |4            |3               |0                 |\n",
      "|2023-07-21 00:08:50.595|2832191032484601030|1               |1            |4               |0                 |\n",
      "+-----------------------+-------------------+----------------+-------------+----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM ice.iceberg.iceberg.snapshots;\").select(\"committed_at\",\"snapshot_id\",\"summary.added-data-files\",\"summary.added-records\",\"summary.total-data-files\",\"summary.total-delete-files\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dca46af",
   "metadata": {},
   "source": [
    "<details style=\"border: 1px solid #aaa; border-radius: 4px; padding: 0.5em 0.5em 0; background-color:#F5F5F5\" class=\"solution\" >\n",
    "<summary style=\"margin: -0.5em -0.5em 0; padding: 0.5em;\"></summary>\n",
    "<code>\n",
    "spark.sql(\"SELECT * FROM iceberg.iceberg.history;\").show()\n",
    "spark.sql(\"SELECT * FROM iceberg.iceberg.files;\").show()\n",
    "spark.sql(\"SELECT * FROM iceberg.iceberg.snapshots;\").show()\n",
    "\n",
    "## alternative syntax example:\n",
    "spark.read.format(\"iceberg\").load(\"iceberg.iceberg.files\").show()\n",
    "</code>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6db241",
   "metadata": {},
   "source": [
    "### Iceberg: Iceberg: Time Travel\n",
    "Bei Iceberg gibt es statt Versionen Snapshots und einige fein granularere Möglichkeiten in der Daten Historie zurück zu gehen.\n",
    "z.B. können alle Time Travel Operationne auch auf die Metadaten angewendet werden. Es kann also nachgeschaut werden wie die Metadaten zu einem bestimmten Zeitpunkt/Snapshot ausgesehen haben.\n",
    "\n",
    "- ```snapshot-id``` selects a specific table snapshot\n",
    "- ```as-of-timestamp``` selects the current snapshot at a timestamp, in milliseconds\n",
    "- ```branch``` selects the head snapshot of the specified branch. Note that currently branch cannot be combined with as-of-timestamp.\n",
    "- ```tag``` selects the snapshot associated with the specified tag. Tags cannot be combined with as-of-timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6c2d137e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------+-------+-------------+\n",
      "| id|account|dt_transaction|balance|          new|\n",
      "+---+-------+--------------+-------+-------------+\n",
      "|  1|   alex|    2019-01-01|   1000|         null|\n",
      "|  2|   alex|    2019-02-01|   1500|         null|\n",
      "|  4|  maria|    2020-01-01|   5000|         null|\n",
      "|  3|   alex|    2019-03-01|   1700|         null|\n",
      "|  1|   otto|    2019-10-01|   4444|neue Spalte 1|\n",
      "+---+-------+--------------+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from the results of iceberg_table.snapshots get the snapshots IDs\n",
    "snapshot1 = spark.read \\\n",
    "                 .option(\"snapshot-id\", \"2832191032484601030\") \\\n",
    "                 .format(\"iceberg\") \\\n",
    "                 .load(\"ice.iceberg.iceberg\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0654a9e",
   "metadata": {},
   "source": [
    "<hr style=\"height: 3px; background: gray;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fcb4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
