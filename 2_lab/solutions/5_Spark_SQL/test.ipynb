{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"5765a06e\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Spark Aufgaben\\n\",\n",
    "    \"1. Importe laden\\n\",\n",
    "    \"2. Jupyter Spark starten und Twitter-Streams von Avro lesen\\n\",\n",
    "    \"3. ETL Strecke: Avro Daten einlesen und als Delta Datei wieder raus schreiben\\n\",\n",
    "    \"4. Analyse-Aufgaben erledigen \\n\",\n",
    "    \"5. Verlaufsanalyse durchführen\\n\",\n",
    "    \"6. **Ausschalten der Spark-App**\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Wichtige Hinweise\\n\",\n",
    "    \"1. Führe alle Anweisungen in der vorgegebenen Reihenfolge aus. Die einzelnen Programmierzellen bauen aufeinander auf.\\n\",\n",
    "    \"2. **Beende unbedingt am Ende die Spark-Anwendung mit dem untersten Befehl \\\"spark.stop()\\\" , wenn du aufhörst an den Daten zu arbeiten.**\\n\",\n",
    "    \"3. Du kannst jederzeit das Notebook wieder hochfahren, wenn du Schritt 1 & 2 (Laden der Imports & Jupyter Spark und seine Konfigurationen hochfahren) ausführen.\\n\",\n",
    "    \"4. Mit **\\\"Strg\\\" + \\\"Enter\\\"** führst du einzelne Zellen direkt aus.\\n\",\n",
    "    \"5. In der oberen Leiste kannst du über **\\\"Insert\\\"** weitere Zellen hinzufügen, um weitere Test-Funktionen zu schreiben. \"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"03c9d1f3\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 1. Laden der Imports\\n\",\n",
    "    \"Hier werden alle benötigten Libraries für dieses Lab heruntergeladen.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"eebe3ff2\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"from pyspark import SparkContext, SparkConf\\n\",\n",
    "    \"from pyspark.sql import SparkSession\\n\",\n",
    "    \"from pyspark.sql import SQLContext\\n\",\n",
    "    \"from pyspark.sql.types import *\\n\",\n",
    "    \"from pyspark.sql.window import Window\\n\",\n",
    "    \"from pyspark.sql import Row\\n\",\n",
    "    \"from pyspark.sql.functions import explode\\n\",\n",
    "    \"from pyspark.sql.functions import lower, col\\n\",\n",
    "    \"import pyspark.sql.functions as f\\n\",\n",
    "    \"\\n\",\n",
    "    \"from delta import *\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"import datetime\\n\",\n",
    "    \"from datetime import datetime\\n\",\n",
    "    \"import json\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"# use 95% of the screen for jupyter cell\\n\",\n",
    "    \"from IPython.core.display import display, HTML\\n\",\n",
    "    \"display(HTML(\\\"<style>.container {width:100% !important; }<style>\\\"))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"2579b387\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 2. Jupyter Spark & Konfigurationen hochfahren\\n\",\n",
    "    \"Hier wird die App jupyter-spark konfiguriert und hochgefahren, welche unsere weiteren Schritte ausführt.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"a6d15c94\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"appName=\\\"jupyter-spark\\\"\\n\",\n",
    "    \"\\n\",\n",
    "    \"conf = SparkConf()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# CLUSTER MANAGER\\n\",\n",
    "    \"################################################################################\\n\",\n",
    "    \"# set Kubernetes Master as Cluster Manager(“k8s://https://” is NOT a typo, this is how Spark knows the “provider” type).\\n\",\n",
    "    \"conf.setMaster(\\\"k8s://https://kubernetes.default.svc.cluster.local:443\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# CONFIGURE KUBERNETES\\n\",\n",
    "    \"################################################################################\\n\",\n",
    "    \"# set the namespace that will be used for running the driver and executor pods.\\n\",\n",
    "    \"conf.set(\\\"spark.kubernetes.namespace\\\",\\\"frontend\\\")\\n\",\n",
    "    \"# set the docker image from which the Worker pods are created\\n\",\n",
    "    \"conf.set(\\\"spark.kubernetes.container.image\\\", \\\"thinkportgmbh/workshops:spark-3.3.1\\\")\\n\",\n",
    "    \"conf.set(\\\"spark.kubernetes.container.image.pullPolicy\\\", \\\"Always\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# set service account to be used\\n\",\n",
    "    \"conf.set(\\\"spark.kubernetes.authenticate.driver.serviceAccountName\\\", \\\"spark\\\")\\n\",\n",
    "    \"# authentication for service account(required to create worker pods):\\n\",\n",
    "    \"conf.set(\\\"spark.kubernetes.authenticate.caCertFile\\\", \\\"/var/run/secrets/kubernetes.io/serviceaccount/ca.crt\\\")\\n\",\n",
    "    \"conf.set(\\\"spark.kubernetes.authenticate.oauthTokenFile\\\", \\\"/var/run/secrets/kubernetes.io/serviceaccount/token\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"# CONFIGURE SPARK\\n\",\n",
    "    \"################################################################################\\n\",\n",
    "    \"conf.set(\\\"spark.sql.session.timeZone\\\", \\\"Europe/Berlin\\\")\\n\",\n",
    "    \"# set driver host. In this case the ingres service for the spark driver\\n\",\n",
    "    \"# find name of the driver service with 'kubectl get services' or in the helm chart configuration\\n\",\n",
    "    \"conf.set(\\\"spark.driver.host\\\", \\\"jupyter-spark-driver.frontend.svc.cluster.local\\\")\\n\",\n",
    "    \"# set the port, If this port is busy, spark-shell tries to bind to another port.\\n\",\n",
    "    \"conf.set(\\\"spark.driver.port\\\", \\\"29413\\\")\\n\",\n",
    "    \"# add the postgres driver jars into session\\n\",\n",
    "    \"conf.set(\\\"spark.jars\\\", \\\"/opt/spark/jars/spark-sql-kafka-0-10_2.12-3.3.1.jar, /opt/spark/jars/kafka-clients-3.3.1.jar, /opt/spark/jars/spark-avro_2.12-3.3.1.jar\\\")\\n\",\n",
    "    \"#conf.set(\\\"spark.driver.extraClassPath\\\",\\\"/opt/spark/jars/spark-sql-kafka-0-10_2.12-3.3.1.jar, /opt/spark/jars/kafka-clients-3.3.1.jar, /opt/spark/jars/spark-avro_2.12-3.3.1.jar\\\")\\n\",\n",
    "    \"conf.set(\\\"spark.executor.extraClassPath\\\",\\\"/opt/spark/jars/spark-sql-kafka-0-10_2.12-3.3.1.jar, /opt/spark/jars/kafka-clients-3.3.1.jar, /opt/spark/jars/spark-avro_2.12-3.3.1.jar\\\")\\n\",\n",
    "    \"#conf.set(\\\"spark.executor.extraLibrary\\\",\\\"/opt/spark/jars/spark-sql-kafka-0-10_2.12-3.3.1.jar, /opt/spark/jars/kafka-clients-3.3.1.jar\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# CONFIGURE S3 CONNECTOR\\n\",\n",
    "    \"conf.set(\\\"spark.hadoop.fs.s3a.endpoint\\\", \\\"minio.minio.svc.cluster.local:9000\\\")\\n\",\n",
    "    \"conf.set(\\\"spark.hadoop.fs.s3a.access.key\\\", \\\"trainadm\\\")\\n\",\n",
    "    \"conf.set(\\\"spark.hadoop.fs.s3a.secret.key\\\", \\\"train@thinkport\\\")\\n\",\n",
    "    \"conf.set(\\\"spark.hadoop.fs.s3a.path.style.access\\\", \\\"true\\\")\\n\",\n",
    "    \"conf.set(\\\"spark.hadoop.fs.s3.impl\\\", \\\"org.apache.hadoop.fs.s3a.S3AFileSystem\\\")\\n\",\n",
    "    \"conf.set(\\\"spark.hadoop.fs.s3a.aws.credentials.provider\\\", \\\"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\\\")\\n\",\n",
    "    \"conf.set(\\\"spark.hadoop.fs.s3a.connection.ssl.enabled\\\", \\\"false\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# conf.set(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.1\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# CONFIGURE WORKER (Customize based on workload)\\n\",\n",
    "    \"################################################################################\\n\",\n",
    "    \"# set number of worker pods\\n\",\n",
    "    \"conf.set(\\\"spark.executor.instances\\\", \\\"1\\\")\\n\",\n",
    "    \"# set memory of each worker pod\\n\",\n",
    "    \"conf.set(\\\"spark.executor.memory\\\", \\\"1G\\\")\\n\",\n",
    "    \"# set cpu of each worker pod\\n\",\n",
    "    \"conf.set(\\\"spark.executor.cores\\\", \\\"2\\\")\\n\",\n",
    "    \"# Number of possible tasks = cores * executores\\n\",\n",
    "    \"\\n\",\n",
    "    \"# SPARK SESSION\\n\",\n",
    "    \"################################################################################\\n\",\n",
    "    \"# and last, create the spark session and pass it the config object\\n\",\n",
    "    \"\\n\",\n",
    "    \"spark = SparkSession\\\\\\n\",\n",
    "    \"    .builder\\\\\\n\",\n",
    "    \"    .config(conf=conf) \\\\\\n\",\n",
    "    \"    .config('spark.sql.session.timeZone', 'Europe/Berlin') \\\\\\n\",\n",
    "    \"    .appName(appName)\\\\\\n\",\n",
    "    \"    .getOrCreate()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# also get the spark context\\n\",\n",
    "    \"sc=spark.sparkContext\\n\",\n",
    "    \"# change the log level to warning, to see less output\\n\",\n",
    "    \"sc.setLogLevel('WARN')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# get the configuration object to check all the configurations the session was startet with\\n\",\n",
    "    \"for entry in sc.getConf().getAll():\\n\",\n",
    "    \"        if entry[0] in [\\\"spark.app.name\\\",\\\"spark.kubernetes.namespace\\\",\\\"spark.executor.memory\\\",\\\"spark.executor.cores\\\",\\\"spark.driver.host\\\",\\\"spark.master\\\"]:\\n\",\n",
    "    \"            print(entry[0],\\\"=\\\",entry[1])\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"9f42ed16\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 3. Einlesen und Schreiben von Daten\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"93cbac52\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### 3.1 Einlesen der Daten aus unserem S3 Speicher-Bucket \\n\",\n",
    "    \"Laden der Daten aus unserem Bucket in \\\"s3a://twitter/avro\\\" in einen DataFrame, um auf den Daten zu arbeiten. \"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"90b65543\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"df=(spark\\n\",\n",
    "    \"    .read.format(\\\"avro\\\")\\n\",\n",
    "    \"    # Pfad zu Bucket\\n\",\n",
    "    \"    .load(\\\"s3a://twitter/avro\\\")\\n\",\n",
    "    \"    # filter retweets raus bei denen das Hashtag truncated wurde\\n\",\n",
    "    \"    .filter(f.array_contains(f.col(\\\"hashtags\\\"),\\\"BigData\\\")==True)\\n\",\n",
    "    \"    # repartition auf 20 um optimierter mit den wenigen cpu zu arbeiten\\n\",\n",
    "    \"    .repartition(20)\\n\",\n",
    "    \"   ).cache()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"0f31f3ee\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"Erste Ausgabe der Daten in Form eines DataFrames\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"5dec3929\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"df.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"7da71da4\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### 3.2 Schreiben der Daten ins Delta-Format\\n\",\n",
    "    \"Hier werden die Daten direkt im Delta-Format umgewandelt und in den S3-Bucket \\\"s3a://twitter/delta\\\" geschrieben. Dieser Schritt ist wichtig, um die Daten passend für Trino zu abzulegen.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"d6249e07\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"writer_delta=(df\\n\",\n",
    "    \"                .write.partitionBy(\\\"language\\\")\\n\",\n",
    "    \"                .mode(\\\"overwrite\\\")\\n\",\n",
    "    \"                .format(\\\"delta\\\")\\n\",\n",
    "    \"                .option(\\\"overwriteSchema\\\", \\\"true\\\")\\n\",\n",
    "    \"                .option(\\\"userMetadata\\\", \\\"Initial Ladung\\\")\\n\",\n",
    "    \"                .save(\\\"s3a://twitter/delta\\\")\\n\",\n",
    "    \"             )\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"518be64b\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 4. Analyse-Aufgaben\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"f9b79d70\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### 4.1 Tweets anschauen und den Aufbau des Dataframes\\n\",\n",
    "    \"Schau dir den Datensatz einmal genau an. Welche Spalten gibt es? Welche Datentypen sind vorhanden?\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"406525a9\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"df.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"9b4223d1\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### 4.2  Das Schema des Datensatzes anzeigen \\n\",\n",
    "    \"<br>\\n\",\n",
    "    \"<code> df.printSchema()</code> gibt das Schema des Datensatzes aus.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"196c1696\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"df.printSchema()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"attachments\": {},\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"edc238b9\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### 4.3 Zählen der Tweets pro Stunde\\n\",\n",
    "    \"Schreibe eine Abfrage, die **die Anzahl an Tweets pro Stunde** zählt.\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"1185327b\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"df_hourly=(df  \\n\",\n",
    "    \"            ...\\n\",\n",
    "    \"          )\\n\",\n",
    "    \"\\n\",\n",
    "    \"df_hourly.show(20)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"attachments\": {},\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"2f0d5a8f\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### 4.4 Top 10 User nach Tweet-Anzahl\\n\",\n",
    "    \"Schreibe eine Abfrage, die die **Top User** nach ihrer **Anzahl an Tweets** ausgibt. Bedenke dabei, deine Ausgabe auf **10** Einträge zu limitieren.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"c2a6ca1f\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"df_top_user=(df\\n\",\n",
    "    \"                ...\\n\",\n",
    "    \"                )\\n\",\n",
    "    \"df_top_user.show()\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"attachments\": {},\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"75828392\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### 4.5 Umgang mit  Arrays\\n\",\n",
    "    \"Für die folgenden Aufgabe wird die <code>explode</code>-Funktion benötigt. Schreibe eine Abfrage die das Hashtag-array mit <code>explode</code> teilt. Gebe dabei die Spalten \\\"user_name\\\", \\\"tweet_id\\\"und die explodierte\\\"hashtags\\\"- Spalte mit einem Limit von 20 Zeilen aus. \\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"bd44fd32\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"df_hash=(df\\n\",\n",
    "    \"         ...\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"df_hash.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"attachments\": {},\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"58765db1\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### 4.6 Top 5 Hashtags der Top 10 User\\n\",\n",
    "    \"Schreibe eine Abfrage, die die **Top 5 der Hashtags** der **10 User** mit den **meisten Tweets** ausgibt.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"f0bd4d64\",\n",
    "   \"metadata\": {\n",
    "    \"scrolled\": true\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"df_top5_per_user=(df_top_user\\n\",\n",
    "    \"                ...\\n\",\n",
    "    \"                 )\\n\",\n",
    "    \"df_top5_per_user.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"attachments\": {},\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"cbb68e6c\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \" ### 4.7 Top 10 Influencer (User mit #BigData-tweets mit den meisten Followern) \\n\",\n",
    "    \" Schreibe eine Abfrage, die die **Top 10 Influencer** mit den **meisten Follower** zählt und sortiert anzeigt.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"79b5113b\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"df_top_influencer=(df\\n\",\n",
    "    \"                ...\\n\",\n",
    "    \"                )\\n\",\n",
    "    \"df_top_influencer.show(10)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"attachments\": {},\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"935e6148\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### 4.8 Top 10 Influencer und ihre Anzahl an tweets\\n\",\n",
    "    \"Schreibe eine Abfrage, die die **Top 10 Influencer**, ihre Follower und die **Anzahl ihrer Tweets** ausgibt. außeredem soll es sortiert nach den Anzahl ihrer Follower sein. \"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"e47a7c26\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"df_withRetweets=(df_top_user\\n\",\n",
    "    \"            ....\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"\\n\",\n",
    "    \"df_withRetweets.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"attachments\": {},\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"13676250\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### Bonusaufgabe: Filter nach den Top 10 Locations und ihrem Top Hashtag\\n\",\n",
    "    \"Schreibe eine Abfrage, die die **Top 10 häufigsten Locations** ausgibt und das am **zweitmeisten verwendete Hashtag** dort. Da alle unsere Daten das Hashtag #BigData beinhalten. \"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"2f14e0c2\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": []\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"eb05cf17\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": []\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"132149f4\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": []\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"a081f82e\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 5. Delta History and Time Travel\\n\",\n",
    "    \"Führe den folgenden Code aus um die aktuelle Delta-Daten-Version upzudaten. Wenn du mehrere Versionen sehen willst schreibe öfter raus mit <code>writer_delta()</code> mit einigen Minuten Abstand.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"287ada1c\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"writer_delta=(df\\n\",\n",
    "    \"                .write.partitionBy(\\\"language\\\")\\n\",\n",
    "    \"                .mode(\\\"overwrite\\\")\\n\",\n",
    "    \"                .format(\\\"delta\\\")\\n\",\n",
    "    \"                .option(\\\"overwriteSchema\\\", \\\"true\\\")\\n\",\n",
    "    \"                .option(\\\"userMetadata\\\", \\\"Update Ladung\\\")\\n\",\n",
    "    \"                .save(\\\"s3a://twitter/delta\\\")\\n\",\n",
    "    \"             )\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"attachments\": {},\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"52d10e78\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### 5.1 Delta Tabelle ausgeben\\n\",\n",
    "    \"Lade die Delta-Tabelle und lasse dir die ersten 2 Einträge ausgeben.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"34f34013\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Load Delta file in s3 into Delta Table Object\\n\",\n",
    "    \"dt = DeltaTable.forPath(spark, \\\"s3a://twitter/delta\\\")\\n\",\n",
    "    \"dt.toDF().show(2)\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"c7862d91\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### 5.2  Erzeugen einer Historie\\n\",\n",
    "    \"1. Führe mehrmals Write to Delta aus und prüfe, wie die Historie neue Einträge hinzufügt  \"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"5127f540\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# get the metadata for the full history of the table\\n\",\n",
    "    \"fullHistoryDF = dt.history()    \\n\",\n",
    "    \"\\n\",\n",
    "    \"# get the metadata for the last operation\\n\",\n",
    "    \"lastOperationDF = dt.history(1) \\n\",\n",
    "    \"\\n\",\n",
    "    \"fullHistoryDF.select(\\\"version\\\",\\\"readVersion\\\",\\\"timestamp\\\",\\\"userId\\\",\\\"operation\\\",\\\"operationParameters\\\",\\\"operationMetrics\\\").show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"attachments\": {},\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"39f0a855\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### 5.3 Laden der aktuellen  Versionen \\n\",\n",
    "    \"Lade eine der Versionen und lasse dir alle `languages` anzeigen (via distinct().show())\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"f92161ce\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# load latest delta version\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"attachments\": {},\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"54cee9d7\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### 5.4. Laden einer ältere Versionen \\n\",\n",
    "    \"Lade eine ältere Version und bestätige, dass noch alle Daten vorhanden sind.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"85e31aad\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"#load specific historic version\\n\",\n",
    "    \"df_timetravel_old = spark.read.format(\\\"delta\\\").option(\\\"versionAsOf\\\", 2).load(\\\"s3a://twitter/delta\\\")\\n\",\n",
    "    \"df_timetravel_old.select(\\\"language\\\").distinct().show()\\n\",\n",
    "    \"df_timetravel_old.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"attachments\": {},\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"9144a895\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### 5.5 Überschreiben von neueren Version\\n\",\n",
    "    \"Überschreibe nun mit der älteren Version die Aktuellste. \"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"49e561fa\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# write old version back as latest\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"attachments\": {},\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"7cde3106\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### 5.6 Zurück in die Zukunft\\n\",\n",
    "    \"Kehre zurück zum aktuellsten Timestamp, indem `timestampAsOf`anstelle von `versionAsOf`verwenden und einem aktuellen timestamp, anstelle der Versionsnummer.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"8be4272d\",\n",
    "   \"metadata\": {\n",
    "    \"scrolled\": true\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"f_b2future = (spark\\n\",\n",
    "    \"                .read.format(\\\"delta\\\").option(\\\"timestampAsOf\\\", \\\"2022-12-08 16:37:54\\\").load(\\\"s3a://twitter/delta\\\")\\n\",\n",
    "    \"               )\\n\",\n",
    "    \"f_b2future.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"5d5df865\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# 6. Ausschalten der Spark-App\\n\",\n",
    "    \"**Bitte schließe am Ende die Spark-App wieder mit dem folgenden Befehl `spark.stop()`, wenn du fertig mit der Bearbeitung der Aufgaben bist.** \"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"bfdb31e3\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"spark.stop()\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"bb598b6c\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": []\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3 (ipykernel)\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.9.2\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 5\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
