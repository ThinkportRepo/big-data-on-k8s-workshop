{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "610d267c",
   "metadata": {},
   "source": [
    "# Demo and Comparission of Big Data File Formats\n",
    "This Notebook runs only on a local Spark Environment, not on Kubernetes\n",
    "\n",
    "## 1. CSV and JSON\n",
    "Old dat formats that are not designed for big data and scaling  \n",
    "**Typical feature:** humand readable\n",
    "\n",
    "## 2. Avro, OCR, Parquet\n",
    "First generation of special big data formats that allow fast writes, fast reads or both  \n",
    "**Typical features:** splittable, compressible, data skipping and predicat pushdown, data schema inclueded\n",
    "\n",
    "\n",
    "\n",
    "## 3. Delta, Iceberg, Hudi\n",
    "Latest generation of big data format that support ACID transaction, audit save transaction logs and time travel  \n",
    "**Typical features:** enhancing first generation format with additonal meta data and read/write procedures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72a3b58b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container {width:100% !important; }<style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#################################################################################\n",
    "# Load all relevant Python Modules\n",
    "#################################################################################\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "from delta import *\n",
    "\n",
    "import datetime\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import json\n",
    "import csv\n",
    "\n",
    "# use 95% of the screen for jupyter cell\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container {width:100% !important; }<style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3e1e98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first for local usage pip install delta-spark\n",
    "\n",
    "builder = pyspark.sql.SparkSession.builder.appName(\"MyApp\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.jars\", \"/Users/alor/opt/spark/jars/spark-sql-kafka-0-10_2.12-3.3.1.jar, /Users/alor/opt/spark/jars/kafka-clients-3.3.1.jar, /Users/alor/opt/spark/jars/spark-avro_2.12-3.3.1.jar\") \\\n",
    "    .config(\"spark.driver.extraClassPath\",\"/Users/alor/opt/spark/jars/spark-sql-kafka-0-10_2.12-3.3.1.jar, /Users/alor/opt/spark/jars/kafka-clients-3.3.1.jar, /Users/alor/opt/spark/jars/spark-avro_2.12-3.3.1.jar\") \\\n",
    "    .config(\"spark.executor.extraClassPath\",\"/Users/alor/opt/spark/jars/spark-sql-kafka-0-10_2.12-3.3.1.jar, /Users/alor/opt/spark/jars/kafka-clients-3.3.1.jar, /Users/alor/opt/spark/jars/spark-avro_2.12-3.3.1.jar\")\n",
    "\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28f7c8e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://macbook-thinkport.fritz.box:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7ff2d8e4be20>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27495f26",
   "metadata": {},
   "source": [
    "## Create sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1b8b7c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++ create new dataframe and show schema and data\n",
      "################################################\n",
      "++ start data\n",
      "+---+-------+--------------+-------+\n",
      "|id |account|dt_transaction|balance|\n",
      "+---+-------+--------------+-------+\n",
      "|2  |alex   |2019-02-01    |1500   |\n",
      "|3  |alex   |2019-03-01    |1700   |\n",
      "|1  |alex   |2019-01-01    |1000   |\n",
      "|4  |maria  |2020-01-01    |5000   |\n",
      "+---+-------+--------------+-------+\n",
      "\n",
      "++ update row and add row\n",
      "+---+-------+--------------+-------+\n",
      "|id |account|dt_transaction|balance|\n",
      "+---+-------+--------------+-------+\n",
      "|1  |alex   |2019-03-01    |3300   |\n",
      "|2  |peter  |2021-01-01    |100    |\n",
      "+---+-------+--------------+-------+\n",
      "\n",
      "++ add new column\n",
      "+---+-------+--------------+-------+-------------+\n",
      "|id |account|dt_transaction|balance|new          |\n",
      "+---+-------+--------------+-------+-------------+\n",
      "|1  |otto   |2019-10-01    |4444   |neue Spalte 1|\n",
      "+---+-------+--------------+-------+-------------+\n",
      "\n",
      "++ add new row with wrong schema (id)\n",
      "+---+-------+--------------+-------+\n",
      "|id |account|dt_transaction|balance|\n",
      "+---+-------+--------------+-------+\n",
      "|5  |markus |2019-09-01    |555    |\n",
      "+---+-------+--------------+-------+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- account: string (nullable = true)\n",
      " |-- dt_transaction: date (nullable = true)\n",
      " |-- balance: long (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- account: string (nullable = true)\n",
      " |-- dt_transaction: date (nullable = true)\n",
      " |-- balance: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# initial Daten\n",
    "account_data1 = [\n",
    "    (1,\"alex\",\"2019-01-01\",1000),\n",
    "    (2,\"alex\",\"2019-02-01\",1500),\n",
    "    (3,\"alex\",\"2019-03-01\",1700),\n",
    "    (4,\"maria\",\"2020-01-01\",5000)\n",
    "    ]\n",
    "\n",
    "# Datensatz mit einem Update und einer neuen Zeile\n",
    "account_data2 = [\n",
    "    (1,\"alex\",\"2019-03-01\",3300),\n",
    "    (2,\"peter\",\"2021-01-01\",100)\n",
    "    ]\n",
    "\n",
    "# Datensatz mit neuer Zeile und neuer Spalte\n",
    "account_data3 = [\n",
    "    (1,\"otto\",\"2019-10-01\",4444,\"neue Spalte 1\")\n",
    "]\n",
    "\n",
    "# Datensatz mit neuer Zeile und neuer Spalte\n",
    "account_data4 = [\n",
    "    (5,\"markus\",\"2019-09-01\",555)\n",
    "]\n",
    "\n",
    "schema = [\"id\",\"account\",\"dt_transaction\",\"balance\"]\n",
    "schema3 = [\"id\",\"account\",\"dt_transaction\",\"balance\",\"new\"]\n",
    "\n",
    "df1 = spark.createDataFrame(data=account_data1, schema = schema).withColumn(\"dt_transaction\",f.col(\"dt_transaction\").cast(\"date\")).repartition(3)\n",
    "df2 = spark.createDataFrame(data=account_data2, schema = schema).withColumn(\"dt_transaction\",f.col(\"dt_transaction\").cast(\"date\")).repartition(2)\n",
    "df3 = spark.createDataFrame(data=account_data3, schema = schema3).withColumn(\"dt_transaction\",f.col(\"dt_transaction\").cast(\"date\")).repartition(1)\n",
    "df4 = spark.createDataFrame(data=account_data4, schema = schema).withColumn(\"dt_transaction\",f.col(\"dt_transaction\").cast(\"date\")).withColumn(\"id\",f.col(\"id\").cast(\"string\")).repartition(1)\n",
    "\n",
    "\n",
    "print(\"++ create new dataframe and show schema and data\")\n",
    "print(\"################################################\")\n",
    "\n",
    "# df1.printSchema()\n",
    "print(\"++ start data\")\n",
    "df1.show(truncate=False)\n",
    "print(\"++ update row and add row\")\n",
    "df2.show(truncate=False)\n",
    "print(\"++ add new column\")\n",
    "df3.show(truncate=False)\n",
    "print(\"++ add new row with wrong schema (id)\")\n",
    "df4.show(truncate=False)\n",
    "df1.printSchema()\n",
    "df4.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6fe705",
   "metadata": {},
   "source": [
    "## CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "916ad0c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Partitions: 3\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Partitions:\", df1.rdd.getNumPartitions())\n",
    "\n",
    "# Schreibe Datenset 1 als CSV Datei\n",
    "write_csv=(df1\n",
    "           .write\n",
    "           .format(\"csv\")\n",
    "           .mode(\"overwrite\") # append\n",
    "           .save(\"output/csv\")\n",
    "          )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac9d2c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_SUCCESS\r\n",
      "part-00000-3bb47c50-3e8a-4447-bcc4-bc92f1b50767-c000.csv\r\n",
      "part-00001-3bb47c50-3e8a-4447-bcc4-bc92f1b50767-c000.csv\r\n",
      "part-00002-3bb47c50-3e8a-4447-bcc4-bc92f1b50767-c000.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls output/csv/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d15fb71a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4,maria,2020-01-01,5000\r\n"
     ]
    }
   ],
   "source": [
    "! cat output/csv/part-00002-1969e3ad-db5e-4e0a-8214-4a9869c24d11-c000.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f31d31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      "\n",
      "+---+-----+----------+----+\n",
      "|_c0|  _c1|       _c2| _c3|\n",
      "+---+-----+----------+----+\n",
      "|  2| alex|2019-02-01|1500|\n",
      "|  3| alex|2019-03-01|1700|\n",
      "|  4|maria|2020-01-01|5000|\n",
      "|  1| alex|2019-01-01|1000|\n",
      "+---+-----+----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "read_csv=spark.read.format(\"csv\").load(\"output/csv\")\n",
    "\n",
    "read_csv.printSchema()\n",
    "read_csv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c5a5979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# schreibe Datenset 3 (neue Spalte) in die gleiche Tabelle dazu\n",
    "write_csv=(df3\n",
    "           .write\n",
    "           .format(\"csv\")\n",
    "           .mode(\"append\") # append\n",
    "           .save(\"output/csv\")\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a771362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_SUCCESS\r\n",
      "part-00000-1969e3ad-db5e-4e0a-8214-4a9869c24d11-c000.csv\r\n",
      "part-00000-411ddc45-144a-4a3d-900f-90a6bf3c46bf-c000.csv\r\n",
      "part-00001-1969e3ad-db5e-4e0a-8214-4a9869c24d11-c000.csv\r\n",
      "part-00002-1969e3ad-db5e-4e0a-8214-4a9869c24d11-c000.csv\r\n",
      "part-00002-411ddc45-144a-4a3d-900f-90a6bf3c46bf-c000.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls output/csv/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "349331cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat output/csv/part-00000-411ddc45-144a-4a3d-900f-90a6bf3c46bf-c000.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31fd06f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      "\n",
      "+---+-----+----------+----+\n",
      "|_c0|  _c1|       _c2| _c3|\n",
      "+---+-----+----------+----+\n",
      "|  2| alex|2019-02-01|1500|\n",
      "|  3| alex|2019-03-01|1700|\n",
      "|  1| otto|2019-10-01|4444|\n",
      "|  4|maria|2020-01-01|5000|\n",
      "|  1| alex|2019-01-01|1000|\n",
      "+---+-----+----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# und lese alles nochmal ein um zu schauen ob die neue Spalte richtig erkannt wurde\n",
    "read_csv=spark.read.format(\"csv\").load(\"output/csv\")\n",
    "\n",
    "read_csv.printSchema()\n",
    "read_csv.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1b3038",
   "metadata": {},
   "source": [
    "#### Erkenntnisse CSV\n",
    "* In wieviele Dateien wird das Datenset aufgeteilt und warum?\n",
    "* Bleibt das Schema erhalten (Selbsterklärend)\n",
    "* Können neue Spalten angefügt werden (Schema Evolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55bcbfa",
   "metadata": {},
   "source": [
    "## JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e368413b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Partitions: 3\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Partitions:\", df1.rdd.getNumPartitions())\n",
    "\n",
    "write_json=(df1\n",
    "           .write\n",
    "           .format(\"json\")\n",
    "           .mode(\"overwrite\") # append\n",
    "           .save(\"output/json\")\n",
    "          )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da9898f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_SUCCESS\r\n",
      "part-00000-e4a7d280-0fa8-484b-abee-0feabe89857a-c000.json\r\n",
      "part-00001-e4a7d280-0fa8-484b-abee-0feabe89857a-c000.json\r\n",
      "part-00002-e4a7d280-0fa8-484b-abee-0feabe89857a-c000.json\r\n"
     ]
    }
   ],
   "source": [
    "!ls output/json/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3ba7f5c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\":2,\"account\":\"alex\",\"dt_transaction\":\"2019-02-01\",\"balance\":1500}\r\n",
      "{\"id\":3,\"account\":\"alex\",\"dt_transaction\":\"2019-03-01\",\"balance\":1700}\r\n"
     ]
    }
   ],
   "source": [
    "! cat output/json/part-00000-8f1197ca-4955-4b8a-a050-010b3eba7e2e-c000.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7506f71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_csv=(df3\n",
    "           .write\n",
    "           .format(\"json\")\n",
    "           .mode(\"append\") # append\n",
    "           .save(\"output/json\")\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ba940d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- account: string (nullable = true)\n",
      " |-- balance: string (nullable = true)\n",
      " |-- dt_transaction: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- new: string (nullable = true)\n",
      "\n",
      "+-------+-------+--------------+---+-------------+\n",
      "|account|balance|dt_transaction| id|          new|\n",
      "+-------+-------+--------------+---+-------------+\n",
      "|   alex|   1500|    2019-02-01|  2|         null|\n",
      "|   alex|   1700|    2019-03-01|  3|         null|\n",
      "|   otto|   4444|    2019-10-01|  1|neue Spalte 1|\n",
      "|   otto|   4444|    2019-10-01|  1|neue Spalte 1|\n",
      "|  maria|   5000|    2020-01-01|  4|         null|\n",
      "|   alex|   1000|    2019-01-01|  1|         null|\n",
      "+-------+-------+--------------+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "read_json=spark.read.format(\"json\").load(\"output/json\")\n",
    "\n",
    "read_json.printSchema()\n",
    "read_json.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad580792",
   "metadata": {},
   "source": [
    "#### Erkenntnisse JSON\n",
    "* Bleibt das Schema erhalten (Selbsterklärend)?\n",
    "* Können neue Spalten angefügt werden (Schema Evolution)?\n",
    "* Gibt es Schema Enforcement?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094e8bb0",
   "metadata": {},
   "source": [
    "## Avro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44777375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Partitions: 3\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Partitions:\", df1.rdd.getNumPartitions())\n",
    "# Schreibe Datenset 1 als AVRO Datei\n",
    "write_avro=(df1\n",
    "           .write\n",
    "           .format(\"avro\")\n",
    "           .mode(\"overwrite\") # append\n",
    "           .save(\"output/avro\")\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d82f2e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_SUCCESS\r\n",
      "part-00000-1fcb830b-c287-4fea-b4f0-b23504fb105a-c000.avro\r\n",
      "part-00001-1fcb830b-c287-4fea-b4f0-b23504fb105a-c000.avro\r\n",
      "part-00002-1fcb830b-c287-4fea-b4f0-b23504fb105a-c000.avro\r\n"
     ]
    }
   ],
   "source": [
    "!ls output/avro/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fc0cf679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obj\u0001\u0006\u0016avro.schema�\u0003{\"type\":\"record\",\"name\":\"topLevelRecord\",\"fields\":[{\"name\":\"id\",\"type\":[\"long\",\"null\"]},{\"name\":\"account\",\"type\":[\"string\",\"null\"]},{\"name\":\"dt_transaction\",\"type\":[{\"type\":\"int\",\"logicalType\":\"date\"},\"null\"]},{\"name\":\"balance\",\"type\":[\"long\",\"null\"]}]}0org.apache.spark.version\r\n",
      "3.1.2\u0014avro.codec\f",
      "snappy\u0000��\u0001��!ɸ \u000b",
      "\u0015\u0014�\u00119\r\n",
      "\u0004H\u001e",
      "t\u0000\u0004\u0000\balex\u0000��\u0002\u0000�\u0017\u0000\u0006\u0000\balex\u0000Ș\u0002\u0000�\u001a3�����\u0001��!ɸ \u000b",
      "\u0015\u0014�\u00119\r\n"
     ]
    }
   ],
   "source": [
    "! cat output/avro/part-00000-f47ea23a-bd11-4456-9c6e-dd455ffeecb3-c000.avro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ac72f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- account: string (nullable = true)\n",
      " |-- dt_transaction: date (nullable = true)\n",
      " |-- balance: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "read_json=spark.read.format(\"avro\").load(\"output/avro\")\n",
    "read_json.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b1e0ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# schreibe Datenset 3 (neue Spalte) in die gleiche Tabelle dazu (!! append NOT overwrite)\n",
    "write_avro=(df3\n",
    "           .write\n",
    "           .format(\"avro\")\n",
    "           .mode(\"append\") # append\n",
    "           .save(\"output/avro\")\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "70570799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- account: string (nullable = true)\n",
      " |-- dt_transaction: date (nullable = true)\n",
      " |-- balance: long (nullable = true)\n",
      "\n",
      "+---+-------+--------------+-------+\n",
      "| id|account|dt_transaction|balance|\n",
      "+---+-------+--------------+-------+\n",
      "|  1|   otto|    2019-10-01|   4444|\n",
      "|  2|   alex|    2019-02-01|   1500|\n",
      "|  3|   alex|    2019-03-01|   1700|\n",
      "|  4|  maria|    2020-01-01|   5000|\n",
      "|  1|   alex|    2019-01-01|   1000|\n",
      "+---+-------+--------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "read_json=spark.read.format(\"avro\").load(\"output/avro\")\n",
    "read_json.printSchema()\n",
    "read_json.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b1054d",
   "metadata": {},
   "source": [
    "* Schema erhalten\n",
    "* Schema evolution "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45de295",
   "metadata": {},
   "source": [
    "## Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb546000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Partitions: 3\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Partitions:\", df1.rdd.getNumPartitions())\n",
    "\n",
    "write_parquet=(df1\n",
    "           .write\n",
    "           # Fachliche Partitionierung beim Schreiben\n",
    "           .partitionBy(\"account\")\n",
    "           .format(\"parquet\")\n",
    "           .mode(\"overwrite\") # append\n",
    "           .save(\"output/parquet\")\n",
    "          )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7bbba040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------+-------+\n",
      "| id|account|dt_transaction|balance|\n",
      "+---+-------+--------------+-------+\n",
      "|  2|   alex|    2019-02-01|   1500|\n",
      "|  3|   alex|    2019-03-01|   1700|\n",
      "|  1|   alex|    2019-01-01|   1000|\n",
      "|  4|  maria|    2020-01-01|   5000|\n",
      "+---+-------+--------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "49b6d692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_SUCCESS      \u001b[34maccount=alex\u001b[m\u001b[m  \u001b[34maccount=maria\u001b[m\u001b[m\n",
      "----------------------------\n",
      "part-00000-aacb8d79-4737-47c1-97a9-54c1dec538c3.c000.snappy.parquet\n",
      "part-00001-aacb8d79-4737-47c1-97a9-54c1dec538c3.c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "!ls  output/parquet/\n",
    "!echo \"----------------------------\"\n",
    "!ls output/parquet/account=alex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2e99e854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PAR1\u0015\u0000\u0015,\u0015.,\u0015\u0004\u0015\u0000\u0015\u0006\u0015\b\u001c",
      "\u0018\b\u0003\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0018\b\u0002\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0016\u0000(\b\u0003\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0018\b\u0002\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0016\u0014\u0002\u0000\u0000\u0000\u0003\u0003\u0001\u0006,\u0000\u0000\u0000\u0000\u0003\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0015\u0000\u0015\u001c",
      "\u0015 ,\u0015\u0004\u0015\u0000\u0015\u0006\u0015\b\u001c",
      "\u0018\u0004$F\u0000\u0000\u0018\u0004\bF\u0000\u0000\u0016\u0000(\u0004$F\u0000\u0000\u0018\u0004\bF\u0000\u0000\u0000\u0000\u0000\u000e4\u0002\u0000\u0000\u0000\u0003\u0003\bF\u0000\u0000$F\u0000\u0000\u0015\u0000\u0015,\u00150,\u0015\u0004\u0015\u0000\u0015\u0006\u0015\b\u001c",
      "\u0018\b�\u0006\u0000\u0000\u0000\u0000\u0000\u0000\u0018\b�\u0005\u0000\u0000\u0000\u0000\u0000\u0000\u0016\u0000(\b�\u0006\u0000\u0000\u0000\u0000\u0000\u0000\u0018\b�\u0005\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0016T\u0002\u0000\u0000\u0000\u0003\u0003�\u0005\u0000\u0000\u0000\u0000\u0000\u0000�\u0006\u0000\u0000\u0000\u0000\u0000\u0000\u0015\u0002\u0019LH\f",
      "spark_schema\u0015\u0006\u0000\u0015\u0004%\u0002\u0018\u0002id\u0000\u0015\u0002%\u0002\u0018\u000edt_transaction%\f",
      "\u0000\u0015\u0004%\u0002\u0018\u0007balance\u0000\u0016\u0004\u0019\u001c",
      "\u0019<&\b\u001c",
      "\u0015\u0004\u00195\u0000\b\u0006\u0019\u0018\u0002id\u0015\u0002\u0016\u0004\u0016�\u0001\u0016�\u0001&\b<\u0018\b\u0003\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0018\b\u0002\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0016\u0000(\b\u0003\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0018\b\u0002\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0019\u001c",
      "\u0015\u0000\u0015\u0000\u0015\u0002\u0000\u0000\u0000&�\u0001\u001c",
      "\u0015\u0002\u00195\u0000\b\u0006\u0019\u0018\u000edt_transaction\u0015\u0002\u0016\u0004\u0016v\u0016z&�\u0001<\u0018\u0004$F\u0000\u0000\u0018\u0004\bF\u0000\u0000\u0016\u0000(\u0004$F\u0000\u0000\u0018\u0004\bF\u0000\u0000\u0000\u0019\u001c",
      "\u0015\u0000\u0015\u0000\u0015\u0002\u0000\u0000\u0000&�\u0002\u001c",
      "\u0015\u0004\u00195\u0000\b\u0006\u0019\u0018\u0007balance\u0015\u0002\u0016\u0004\u0016�\u0001\u0016�\u0001&�\u0002<\u0018\b�\u0006\u0000\u0000\u0000\u0000\u0000\u0000\u0018\b�\u0005\u0000\u0000\u0000\u0000\u0000\u0000\u0016\u0000(\b�\u0006\u0000\u0000\u0000\u0000\u0000\u0000\u0018\b�\u0005\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0019\u001c",
      "\u0015\u0000\u0015\u0000\u0015\u0002\u0000\u0000\u0000\u0016�\u0003\u0016\u0004\u0000\u0019,\u0018\u0018org.apache.spark.version\u0018\u00053.1.2\u0000\u0018)org.apache.spark.sql.parquet.row.metadata\u0018�\u0001{\"type\":\"struct\",\"fields\":[{\"name\":\"id\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"dt_transaction\",\"type\":\"date\",\"nullable\":true,\"metadata\":{}},{\"name\":\"balance\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}}]}\u0000\u0018Jparquet-mr version 1.10.1 (build a89df8f9932b6ef6633d06069e50c9b7970bebd1)\u0019<\u001c",
      "\u0000\u0000\u001c",
      "\u0000\u0000\u001c",
      "\u0000\u0000\u0000�\u0002\u0000\u0000PAR1"
     ]
    }
   ],
   "source": [
    "! cat output/parquet/account=alex/part-00000-aacb8d79-4737-47c1-97a9-54c1dec538c3.c000.snappy.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e00a1e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+-------+-------+\n",
      "| id|dt_transaction|balance|account|\n",
      "+---+--------------+-------+-------+\n",
      "|  2|    2019-02-01|   1500|   alex|\n",
      "|  3|    2019-03-01|   1700|   alex|\n",
      "|  1|    2019-01-01|   1000|   alex|\n",
      "+---+--------------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "read_parquet=spark.read.format(\"parquet\").load(\"output/parquet/\").filter(col(\"account\")==\"alex\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2e4710",
   "metadata": {},
   "source": [
    "### Parquet: Filter Pushdown\n",
    "Da das Parquet Format spalten basiert ist und für jede Spalte Metadaten vorhällt, können Programme die diese Dateien einlesen vor der Serialisierung (dem kompletten Einlesen in den Arbeitsspeicher) erst die Header scannen und entscheiden welche Dateien tatsächlich benötigt werden.  \n",
    "Dies nennt man Attribut oder Filter Pushdown.  \n",
    "Spark kann außerdem, wenn die Daten in Partition im Format `PartitionKey=value` diese automatisch erkennen und wenn ein Filter auf die Partition gelegt ist nur diesen Unterordner einlesen.  \n",
    "\n",
    "**Aufgabe:** Untersuche den Execution Plan der Filter Operation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8abafc7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition Filter\n",
      "== Physical Plan ==\n",
      "*(1) ColumnarToRow\n",
      "+- FileScan parquet [id#787L,dt_transaction#788,balance#789L,account#790] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex[file:/Users/alor/All/git/big-data-on-k8s-workshop/4_demos/output/parquet], PartitionFilters: [isnotnull(account#790), (account#790 = alex)], PushedFilters: [], ReadSchema: struct<id:bigint,dt_transaction:date,balance:bigint>\n",
      "\n",
      "\n",
      "Pushdow Filter\n",
      "== Physical Plan ==\n",
      "*(1) Filter (isnotnull(balance#797L) AND (balance#797L > 1500))\n",
      "+- *(1) ColumnarToRow\n",
      "   +- FileScan parquet [id#795L,dt_transaction#796,balance#797L,account#798] Batched: true, DataFilters: [isnotnull(balance#797L), (balance#797L > 1500)], Format: Parquet, Location: InMemoryFileIndex[file:/Users/alor/All/git/big-data-on-k8s-workshop/4_demos/output/parquet], PartitionFilters: [], PushedFilters: [IsNotNull(balance), GreaterThan(balance,1500)], ReadSchema: struct<id:bigint,dt_transaction:date,balance:bigint>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parquet Datei mit PartitionFilter laden\n",
    "read_parquet=(spark\n",
    "              .read.format(\"parquet\")\n",
    "              .load(f\"output/parquet\")\n",
    "              # Filter auf die Spalte über die partitioniert wurde\n",
    "              .filter(f.col(\"account\")==\"alex\")\n",
    "             )\n",
    "\n",
    "# Parquet mit normalem Filter laden\n",
    "read_parquet2=(spark\n",
    "              .read.format(\"parquet\")\n",
    "              .load(f\"output//parquet\")\n",
    "              # Filter auf die Spalte über eine normale Spalte\n",
    "              .filter(f.col(\"balance\")>1500)\n",
    "             )\n",
    "\n",
    "# Anzeigen des physischen Execution Plans um zu sehen welche Filter ins Dateisystem bzw. in die Parquet Datei gepusht werden\n",
    "print(\"Partition Filter\")\n",
    "read_parquet.explain(\"simple\")\n",
    "print(\"Pushdow Filter\")\n",
    "read_parquet2.explain(\"simple\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66aa979e",
   "metadata": {},
   "source": [
    "### Parquet: Schema Evolution\n",
    "Schema Evolution ermöglicht es das Schema der Tabelle zu erweitern.  \n",
    "Der Spark Parquet reader bietet verschiedenen Möglichkeiten mit Schemaerweiterungen umzugehen  \n",
    "https://spark.apache.org/docs/latest/sql-data-sources-parquet.html#schema-merging\n",
    "\n",
    "**Aufgabe:** Lese die Daten so ein, dass das Schema korrekt erweitert wird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e2ad4a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zeile mit neuer Spalte anfügen\n",
    "write_parquet=(df3\n",
    "           .write\n",
    "           .format(\"parquet\")\n",
    "           .mode(\"append\") # append\n",
    "           # schreibe ohne zu Partitionieren direkt in ein neues Unterverzeichnis\n",
    "           .save(f\"output/parquet/account=otto\")\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4e0dbf0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- dt_transaction: date (nullable = true)\n",
      " |-- balance: long (nullable = true)\n",
      " |-- account: string (nullable = true)\n",
      "\n",
      "+---+--------------+-------+-------+\n",
      "| id|dt_transaction|balance|account|\n",
      "+---+--------------+-------+-------+\n",
      "|  1|    2019-10-01|   4444|   otto|\n",
      "|  2|    2019-02-01|   1500|   alex|\n",
      "|  3|    2019-03-01|   1700|   alex|\n",
      "|  4|    2020-01-01|   5000|  maria|\n",
      "|  1|    2019-01-01|   1000|   alex|\n",
      "+---+--------------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# einlesen mit der mergeSchema Option\n",
    "read_parquet=(spark\n",
    "              .read.format(\"parquet\")\n",
    "              # setzte die mergeSchema auf true/false um den Unterschied beim Einlesen zu sehen\n",
    "              # Vegleiche: https://spark.apache.org/docs/latest/sql-data-sources-parquet.html#schema-merging\n",
    "              #.option(\"mergeSchema\", \"false\")\n",
    "              .load(f\"output/parquet\")\n",
    "             )\n",
    "\n",
    "read_parquet.printSchema()\n",
    "read_parquet.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c394e39e",
   "metadata": {},
   "source": [
    "### Parquet: Schema Enforcement\n",
    "Schema Enforcement sorgt dafür, dass in ein bestehendes Schema keine Daten mit falschen Typen gelesen oder geschrieben werden können"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5023dc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datensatz mit falschem Datentyp anfügen\n",
    "df2a=(df2.where(f.col(\"account\")==\"peter\").withColumn(\"id\", f.col(\"id\").cast(\"string\")))\n",
    "\n",
    "\n",
    "# Zeile mit falschem Typ anfügen\n",
    "write_parquet=(df2a\n",
    "           .write\n",
    "           .partitionBy(\"account\")\n",
    "           .format(\"parquet\")\n",
    "           .mode(\"append\") # append\n",
    "           .save(f\"output/parquet\")\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d14b2b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- dt_transaction: date (nullable = true)\n",
      " |-- balance: long (nullable = true)\n",
      " |-- account: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o237.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 48.0 failed 1 times, most recent failure: Lost task 3.0 in stage 48.0 (TID 217) (macbook-thinkport.fritz.box executor driver): org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file file:///Users/alor/All/git/big-data-on-k8s-workshop/4_demos/output/parquet/account=peter/part-00000-a447ca37-2532-4021-8346-4040597cbc23.c000.snappy.parquet. Column: [id], Expected: bigint, Found: BINARY\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:179)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:503)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.constructConvertNotSupportedException(VectorizedColumnReader.java:339)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBinaryBatch(VectorizedColumnReader.java:696)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:309)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:283)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:181)\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:37)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:173)\n\t... 20 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2929)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:338)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file file:///Users/alor/All/git/big-data-on-k8s-workshop/4_demos/output/parquet/account=peter/part-00000-a447ca37-2532-4021-8346-4040597cbc23.c000.snappy.parquet. Column: [id], Expected: bigint, Found: BINARY\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:179)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:503)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\nCaused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.constructConvertNotSupportedException(VectorizedColumnReader.java:339)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBinaryBatch(VectorizedColumnReader.java:696)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:309)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:283)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:181)\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:37)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:173)\n\t... 20 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-9fce6697a4e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mread_parquet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mread_parquet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \"\"\"\n\u001b[1;32m    483\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/pyspark/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/pyspark/lib/python3.8/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o237.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 48.0 failed 1 times, most recent failure: Lost task 3.0 in stage 48.0 (TID 217) (macbook-thinkport.fritz.box executor driver): org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file file:///Users/alor/All/git/big-data-on-k8s-workshop/4_demos/output/parquet/account=peter/part-00000-a447ca37-2532-4021-8346-4040597cbc23.c000.snappy.parquet. Column: [id], Expected: bigint, Found: BINARY\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:179)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:503)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.constructConvertNotSupportedException(VectorizedColumnReader.java:339)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBinaryBatch(VectorizedColumnReader.java:696)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:309)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:283)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:181)\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:37)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:173)\n\t... 20 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2929)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:338)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file file:///Users/alor/All/git/big-data-on-k8s-workshop/4_demos/output/parquet/account=peter/part-00000-a447ca37-2532-4021-8346-4040597cbc23.c000.snappy.parquet. Column: [id], Expected: bigint, Found: BINARY\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:179)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:503)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\nCaused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.constructConvertNotSupportedException(VectorizedColumnReader.java:339)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBinaryBatch(VectorizedColumnReader.java:696)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:309)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:283)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:181)\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:37)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:173)\n\t... 20 more\n"
     ]
    }
   ],
   "source": [
    "# einlesen mit der mergeSchema Option\n",
    "read_parquet=(spark\n",
    "              .read.format(\"parquet\")\n",
    "              # setzte die mergeSchema auf true/false um den Unterschied beim Einlesen zu sehen\n",
    "              .option(\"mergeSchema\", \"false\")\n",
    "              .load(f\"output/parquet\")\n",
    "             )\n",
    "\n",
    "read_parquet.printSchema()\n",
    "read_parquet.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2114e5df",
   "metadata": {},
   "source": [
    "#### Erkenntnisse Parquet\n",
    "* Sind Parquet Dateien selbsterklärend (haben ein Spalten und Typenschema )\n",
    "* Partitioning and Partion Discovery: werden die Daten in Verzeichnisse geschriebe und wieder als Partitionen erkannt?\n",
    "* Schema Evolution: Kann das Schema erweitert werden, also eine neue Spalte angefügt werden?\n",
    "* Schema Enforcement on write: Kann eine Spalte mit falschem Datetyp einfach beim schreiben hinzugefügt werden? \n",
    "* Schema Enforcement on read: Kann ein Verzeichnis mit mehreren Parquet Dateien bei der eine Spalte ein anderes Schema hat gelesen werden?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eadd53c",
   "metadata": {},
   "source": [
    "# Delta\n",
    "Das Delta Format fügt Parquet Dateien einen zusätzlichen Layer an Metadatan hinzu und erfüllt mit dem entsprechenden Treiber alle ACID Eigenschaften einer Datenbank und mehr. \n",
    "\n",
    "A = **Atomic** heißt alle Datenänderungen werden wie eine einzige Operation verarbeitet. Dies bedeutet, dass entweder alle Änderungen durchgeführt werden oder keine. Wenn das schreiben also mitten drin Fehlschlägt werden alle Daten dieser Schreiboperation die bereits geschrieben wurden wieder entfernt, bzw. nicht als erfolgreich geschrieben markiert.  \n",
    "T = **Consistency** bedeutet, wenn eine Transaktion beginnt und wenn eine Transaktion endet, befinden sich die Daten in einem konsistenten Zustand.  \n",
    "I = **Isolation** und bedeutet, dass der Übergangszustand einer Transaktion für andere Transaktionen nicht sichtbar ist. Dies führt dazu, dass Transaktionen, die gleichzeitig ablaufen, sich nicht gegenseitig beeinflussen oder blockieren.  \n",
    "D =**Durability** heißt, das die Datenänderungen nach erfolgreich abgeschlossener Transaktion erhalten bleiben und werden nicht rückgängig gemacht werden, selbst wenn ein Systemausfall auftritt.  \n",
    "\n",
    "**Time Travel** bei Delta bedeutet, dass jede Datenänderung als eigene Version aufgezeichnet wird und jederzeit zu einer alten Version zurück gegegangen werden kann.\n",
    "\n",
    "### Aufgabe:\n",
    "Wiederhole die gleichen Schritte mit dem DELTA Format und schaue wie sich hier Schema und neue Spalten verhalten\n",
    "\n",
    "1. Datenset 1 als DELTA schreiben (.format(\"delta\") und Pfad= .save(f\"s3://{bucket}/delta\"))\n",
    "2. Dateien und Inhalt anzeigen, vestehen was da passiert ist\n",
    "3. Metadaten und Deltalog in Datei verstehen\n",
    "3. Daten wieder einlese und checken ob es ein Schema und Spaltennamen gibt\n",
    "4. Schema Evolutiuon: Datenset 3 anfügen mit neuer Spalte anfügen\n",
    "5. Daten wieder einlesen und checken was mit der neuen Spalte passiert\n",
    "6. Partion & Pushdown Filter: Execution Plan für verschiedene Filter anzeigen\n",
    "6. Schema Enforcement: Datentyp in bestehender Spalte ändern und schauen ob und wie dies gehandhabt wird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "513d213c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schreibe die Daten df1 als Delta Datei in den Pfad bucket/delta\n",
    "write_delta=(df1\n",
    "           .write\n",
    "           .format(\"delta\")\n",
    "           #.option(\"mergeSchema\", \"false\")\n",
    "           .mode(\"overwrite\") \n",
    "           .save(f\"output/delta\")\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "70cd6f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m_delta_log\u001b[m\u001b[m\n",
      "part-00000-c5db6f76-a542-45fe-bcf4-e3e7ef46e9d3-c000.snappy.parquet\n",
      "part-00000-dada16c9-a473-4ded-a0c1-deb66d743bc0-c000.snappy.parquet\n",
      "part-00001-4e386600-5d7f-45a6-9152-5c8ef563d551-c000.snappy.parquet\n",
      "part-00001-96342876-21c2-450c-9a4e-f8f4223b4f72-c000.snappy.parquet\n",
      "part-00002-b32284bf-bd8b-4c80-85b7-037aa0802901-c000.snappy.parquet\n",
      "part-00002-cba791ad-9a06-493d-bc03-a565dccc6c0b-c000.snappy.parquet\n",
      "-------------------------------------------------------------------\n",
      "00000000000000000000.json 00000000000000000001.json\n",
      "{\"commitInfo\":{\"timestamp\":1689839781730,\"operation\":\"WRITE\",\"operationParameters\":{\"mode\":\"Overwrite\",\"partitionBy\":\"[]\"},\"isBlindAppend\":false,\"operationMetrics\":{\"numFiles\":\"3\",\"numOutputBytes\":\"3527\",\"numOutputRows\":\"4\"}}}\n",
      "{\"protocol\":{\"minReaderVersion\":1,\"minWriterVersion\":2}}\n",
      "{\"metaData\":{\"id\":\"1da24135-4733-4b1e-b5b3-9ebbc720d55d\",\"format\":{\"provider\":\"parquet\",\"options\":{}},\"schemaString\":\"{\\\"type\\\":\\\"struct\\\",\\\"fields\\\":[{\\\"name\\\":\\\"id\\\",\\\"type\\\":\\\"long\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"account\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"dt_transaction\\\",\\\"type\\\":\\\"date\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"balance\\\",\\\"type\\\":\\\"long\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}}]}\",\"partitionColumns\":[],\"configuration\":{},\"createdTime\":1689839777969}}\n",
      "{\"add\":{\"path\":\"part-00000-dada16c9-a473-4ded-a0c1-deb66d743bc0-c000.snappy.parquet\",\"partitionValues\":{},\"size\":1202,\"modificationTime\":1689839778000,\"dataChange\":true}}\n",
      "{\"add\":{\"path\":\"part-00001-4e386600-5d7f-45a6-9152-5c8ef563d551-c000.snappy.parquet\",\"partitionValues\":{},\"size\":1157,\"modificationTime\":1689839778000,\"dataChange\":true}}\n",
      "{\"add\":{\"path\":\"part-00002-b32284bf-bd8b-4c80-85b7-037aa0802901-c000.snappy.parquet\",\"partitionValues\":{},\"size\":1168,\"modificationTime\":1689839778000,\"dataChange\":true}}\n"
     ]
    }
   ],
   "source": [
    "! ls output/delta\n",
    "! echo \"-------------------------------------------------------------------\"\n",
    "! ls output/delta/_delta_log\n",
    "! cat output/delta/_delta_log/00000000000000000000.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8620df19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- account: string (nullable = true)\n",
      " |-- dt_transaction: date (nullable = true)\n",
      " |-- balance: long (nullable = true)\n",
      "\n",
      "+---+-------+--------------+-------+\n",
      "| id|account|dt_transaction|balance|\n",
      "+---+-------+--------------+-------+\n",
      "|  2|   alex|    2019-02-01|   1500|\n",
      "|  3|   alex|    2019-03-01|   1700|\n",
      "|  4|  maria|    2020-01-01|   5000|\n",
      "|  1|   alex|    2019-01-01|   1000|\n",
      "+---+-------+--------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lese die gerade geschriebenen Delta Tabelle wieder ein und zeige sie an, überprüfe das Schema\n",
    "read_delta=(spark\n",
    "              .read.format(\"delta\")\n",
    "              .load(f\"output/delta\")\n",
    "             )\n",
    "read_delta.printSchema()\n",
    "read_delta.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9942387e",
   "metadata": {},
   "source": [
    "### Delta: Schema Evolution\n",
    "**Aufgabe:** Verstehe die Option *mergeSchema* on write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a714c074",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "A schema mismatch detected when writing to the Delta table (Table ID: 1da24135-4733-4b1e-b5b3-9ebbc720d55d).\nTo enable schema migration using DataFrameWriter or DataStreamWriter, please set:\n'.option(\"mergeSchema\", \"true\")'.\nFor other operations, set the session configuration\nspark.databricks.delta.schema.autoMerge.enabled to \"true\". See the documentation\nspecific to the operation for details.\n\nTable schema:\nroot\n-- id: long (nullable = true)\n-- account: string (nullable = true)\n-- dt_transaction: date (nullable = true)\n-- balance: long (nullable = true)\n\n\nData schema:\nroot\n-- id: long (nullable = true)\n-- account: string (nullable = true)\n-- dt_transaction: date (nullable = true)\n-- balance: long (nullable = true)\n-- new: string (nullable = true)\n\n         ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-03a2da81e3c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Zeile mit zusätzlicher Spalte anfügen (df3)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m write_delta=(df3\n\u001b[0m\u001b[1;32m      3\u001b[0m            \u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m            \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"delta\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m            \u001b[0;31m# Bei Delta kann bein Schreiben gesetzt werden ob die Tabelle erweitert werden soll oder nicht, Default ist false.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1107\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1109\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/pyspark/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: A schema mismatch detected when writing to the Delta table (Table ID: 1da24135-4733-4b1e-b5b3-9ebbc720d55d).\nTo enable schema migration using DataFrameWriter or DataStreamWriter, please set:\n'.option(\"mergeSchema\", \"true\")'.\nFor other operations, set the session configuration\nspark.databricks.delta.schema.autoMerge.enabled to \"true\". See the documentation\nspecific to the operation for details.\n\nTable schema:\nroot\n-- id: long (nullable = true)\n-- account: string (nullable = true)\n-- dt_transaction: date (nullable = true)\n-- balance: long (nullable = true)\n\n\nData schema:\nroot\n-- id: long (nullable = true)\n-- account: string (nullable = true)\n-- dt_transaction: date (nullable = true)\n-- balance: long (nullable = true)\n-- new: string (nullable = true)\n\n         "
     ]
    }
   ],
   "source": [
    "# Zeile mit zusätzlicher Spalte anfügen (df3)\n",
    "write_delta=(df3\n",
    "           .write\n",
    "           .format(\"delta\")\n",
    "           # Bei Delta kann bein Schreiben gesetzt werden ob die Tabelle erweitert werden soll oder nicht, Default ist false. \n",
    "           # Führe den Code zuerst ohne diese Option aus und schaue das Ergebnis, an \n",
    "           #.option(\"mergeSchema\", \"false\")\n",
    "           .mode(\"append\") # append\n",
    "           .save(f\"output/delta\")\n",
    "          )\n",
    "\n",
    "\n",
    "# überprüfe ob die neue Spalte korrekt angefügt wurde\n",
    "read_delta=spark.read.format(\"delta\").load(f\"output/delta\")\n",
    "read_delta.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a5bf92d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m_delta_log\u001b[m\u001b[m\n",
      "part-00000-c5db6f76-a542-45fe-bcf4-e3e7ef46e9d3-c000.snappy.parquet\n",
      "part-00000-dada16c9-a473-4ded-a0c1-deb66d743bc0-c000.snappy.parquet\n",
      "part-00001-4e386600-5d7f-45a6-9152-5c8ef563d551-c000.snappy.parquet\n",
      "part-00001-96342876-21c2-450c-9a4e-f8f4223b4f72-c000.snappy.parquet\n",
      "part-00002-b32284bf-bd8b-4c80-85b7-037aa0802901-c000.snappy.parquet\n",
      "part-00002-cba791ad-9a06-493d-bc03-a565dccc6c0b-c000.snappy.parquet\n",
      "-------------------------------------------------------------------\n",
      "00000000000000000000.json 00000000000000000001.json\n"
     ]
    }
   ],
   "source": [
    "! ls output/delta\n",
    "! echo \"-------------------------------------------------------------------\"\n",
    "! ls output/delta/_delta_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1be610",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat output/delta/_delta_log/00000000000000000002.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3729d8da",
   "metadata": {},
   "source": [
    "### Delta: Schema Enforcement\n",
    "Schema Enforcement bedeutet soll garantieren, dass keine Daten mit falschen Datentyp der Tabelle abgefügt werden  \n",
    "\n",
    "**Aufgabe:** verstehe die Option *mergeSchema* im Kontext von Datentyp Änderungen bei bestehenden Spalten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb41a734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# füge eine Zeile mit falschen Datetyp für eine bestehenden Spalte an\n",
    "write_delta=(df2\n",
    "           # Eine Zeile aus dem df2 filtern\n",
    "           .where(f.col(\"account\")==\"peter\")\n",
    "           # Bestehenden Spaltentyp ändern\n",
    "           .withColumn(\"id\", f.col(\"id\").cast(\"string\"))\n",
    "           .write\n",
    "           .format(\"delta\")\n",
    "           # Bei Delta kann bein Schreiben gesetzt werden ob die Tabelle erweitert werden kann oder nicht, Default ist false\n",
    "           #.option(\"mergeSchema\", \"false\")\n",
    "           .mode(\"overwrite\") # append\n",
    "           .save(f\"s3://{bucket}/delta\")\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79abaf75",
   "metadata": {},
   "source": [
    "### Delta: Schema Replacement\n",
    "Delta bieten die Möglichkeit das Schema einer Tabelle zu ändern, also z.B. eine bestehende Spalte umzubenennen und deren Datentyp zu ändern.   \n",
    "Dokumentation mit Beispielen: https://docs.delta.io/latest/delta-batch.html#replace-table-schema  \n",
    "\n",
    "**Aufgabe:** Ändere in der bestehenden Tabelle den Datentyp der Spalte `id` auf `string` und den Spaltennamen `new` zu `comment`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28dee6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_delta=(spark\n",
    "              .read.format(\"delta\")\n",
    "              .load(f\"output/delta\")\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3731648e",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_delta=()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b365bf",
   "metadata": {},
   "source": [
    "### Delta: History und Metadaten\n",
    "Im Delta Log werde alle Transaktionen mit zahlreichen Metadaten gespeichert. Das Spark Modul, der Treiber, um diese Daten auszulesen bietet zahlreiche Möglichkeiten diese Daten zu analysieren\n",
    "\n",
    "**Aufgabe:** Lese den History Log ein und verstehe was in den einzelnen Attributen steht. Treffe eine Auswahl der interessanten Informationen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7cb3480c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- version: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      " |-- userName: string (nullable = true)\n",
      " |-- operation: string (nullable = true)\n",
      " |-- operationParameters: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      " |-- job: struct (nullable = true)\n",
      " |    |-- jobId: string (nullable = true)\n",
      " |    |-- jobName: string (nullable = true)\n",
      " |    |-- runId: string (nullable = true)\n",
      " |    |-- jobOwnerId: string (nullable = true)\n",
      " |    |-- triggerType: string (nullable = true)\n",
      " |-- notebook: struct (nullable = true)\n",
      " |    |-- notebookId: string (nullable = true)\n",
      " |-- clusterId: string (nullable = true)\n",
      " |-- readVersion: long (nullable = true)\n",
      " |-- isolationLevel: string (nullable = true)\n",
      " |-- isBlindAppend: boolean (nullable = true)\n",
      " |-- operationMetrics: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      " |-- userMetadata: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Erzeuge ein DeltaTable Objekt was alle Zusatzeigenschaften von Delta bereitstellt\n",
    "deltaTable = DeltaTable.forPath(spark, f\"output/delta\")\n",
    "\n",
    "# Historie aus den Delta Logs erzeugen\n",
    "fullHistoryDF = deltaTable.history() \n",
    "\n",
    "# Alle verfügbaren Spalten anzeigen\n",
    "fullHistoryDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ac186df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-------------------+------+---------+--------------------+--------------------+------------+\n",
      "|version|readVersion|          timestamp|userId|operation| operationParameters|    operationMetrics|userMetadata|\n",
      "+-------+-----------+-------------------+------+---------+--------------------+--------------------+------------+\n",
      "|      5|          4|2023-04-11 11:39:56|  null|    WRITE|{mode -> Overwrit...|{numFiles -> 2, n...|        null|\n",
      "|      4|          3|2023-04-11 11:39:39|  null|    WRITE|{mode -> Append, ...|{numFiles -> 2, n...|        null|\n",
      "|      3|          2|2023-04-11 11:34:00|  null|    WRITE|{mode -> Overwrit...|{numFiles -> 3, n...|        null|\n",
      "|      2|          1|2023-03-27 10:23:49|  null|    WRITE|{mode -> Overwrit...|{numFiles -> 2, n...|        null|\n",
      "|      1|          0|2023-03-27 10:23:34|  null|    WRITE|{mode -> Append, ...|{numFiles -> 2, n...|        null|\n",
      "|      0|       null|2023-03-27 10:22:22|  null|    WRITE|{mode -> Overwrit...|{numFiles -> 3, n...|        null|\n",
      "+-------+-----------+-------------------+------+---------+--------------------+--------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fullHistoryDF.select(\"version\",\"readVersion\",\"timestamp\",\"userId\",\"operation\",\"operationParameters\",\"operationMetrics\",\"userMetadata\").show(truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "49f89cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-------------------+---------+---------+--------+-------------+\n",
      "|version|readVersion|timestamp          |operation|mode     |numFiles|numOutputRows|\n",
      "+-------+-----------+-------------------+---------+---------+--------+-------------+\n",
      "|1      |0          |2023-07-20 10:06:23|WRITE    |Overwrite|3       |4            |\n",
      "|0      |null       |2023-07-20 09:56:22|WRITE    |Overwrite|3       |4            |\n",
      "+-------+-----------+-------------------+---------+---------+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fullHistoryDF.select(\"version\",\"readVersion\",\"timestamp\",\"operation\",\"operationParameters.mode\",\"operationMetrics.numFiles\",\"operationMetrics.numOutputRows\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7839472",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4dc61efc",
   "metadata": {},
   "source": [
    "## Time travel\n",
    "Die Time Travel Funktion von Delta ermöglicht es den Zustand der Datentabelle zu einem bestimmten Zeitpunkt in der Vergangenheit wiederherzustellen oder Änderungen zu verfolgen.   \n",
    "Time Travel ermöglicht es, vorherige Versionen der Daten abzufragen und historische Analysen durchzuführen, ohne auf separate Backups oder Snapshots angewiesen zu sein.\n",
    "\n",
    "**Aufgabe:** lese verschiedenen Datenstände nach Versionsnummer oder Timestap ein.  \n",
    "Weitere Informationen hierzu finden sich in https://delta.io/blog/2023-02-01-delta-lake-time-travel/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6617afc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------+-------+----+\n",
      "| id|account|dt_transaction|balance| new|\n",
      "+---+-------+--------------+-------+----+\n",
      "|  2|   alex|    2019-02-01|   1500|null|\n",
      "|  3|   alex|    2019-03-01|   1700|null|\n",
      "|  2|  peter|    2021-01-01|    100|null|\n",
      "|  4|  maria|    2020-01-01|   5000|null|\n",
      "|  1|   alex|    2019-03-01|   3300|null|\n",
      "|  1|   alex|    2019-01-01|   1000|null|\n",
      "+---+-------+--------------+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"delta\").option(\"versionAsOf\", \"4\").load(\"output/delta\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55d249d",
   "metadata": {},
   "source": [
    "## Merge\n",
    "### Delta: Merge (Upsert)\n",
    "Delta bietet die Möglichkeiten auf bestehenden Dateien ein Daten Upsert durchzuführen.   \n",
    "Ubsert oder Merge bedeutet zu prüfen ob es die neue Zeile für einen bestimmten Schlüssel schon in den Daten gibt und wenn ja diese zu updaten und wenn nein sie neu hinzuzufügen\n",
    "\n",
    "**Aufgabe:** Merge den Datensatz df2 mit dem der Änderung der `balance` für `Alex` und überprüfe ob das Update korrekt war"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2aea3fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++ Datensatz der auf bestehende Daten upserted/merged werden soll\n",
      "+---+-------+--------------+-------+----+\n",
      "| id|account|dt_transaction|balance| new|\n",
      "+---+-------+--------------+-------+----+\n",
      "|  1|   alex|    2019-03-01|   3300|test|\n",
      "|  2|  peter|    2021-01-01|    100|test|\n",
      "+---+-------+--------------+-------+----+\n",
      "\n",
      "++ Bestehender Datensatz in Delta\n",
      "+---+-------+--------------+-------+\n",
      "| id|account|dt_transaction|balance|\n",
      "+---+-------+--------------+-------+\n",
      "|  4|  maria|    2020-01-01|   5000|\n",
      "|  2|  peter|    2021-01-01|    100|\n",
      "|  1|   alex|    2019-01-01|   1000|\n",
      "|  2|   alex|    2019-02-01|   1500|\n",
      "|  1|   alex|    2019-03-01|   3300|\n",
      "+---+-------+--------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deltaTable2 = DeltaTable.forPath(spark, f\"output/delta\")\n",
    "\n",
    "# Spalte anfügen, da merge nur funktioniert wenn das Schema stimmt\n",
    "#df2a=df2.withColumn(\"new\",f.lit(\"test\"))\n",
    "\n",
    "print(\"++ Datensatz der auf bestehende Daten upserted/merged werden soll\")\n",
    "df2a.show()\n",
    "print(\"++ Bestehender Datensatz in Delta\")\n",
    "deltaTable2.toDF().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e1688108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------+-------+\n",
      "| id|account|dt_transaction|balance|\n",
      "+---+-------+--------------+-------+\n",
      "|  4|  maria|    2020-01-01|   5000|\n",
      "|  2|  peter|    2021-01-01|    100|\n",
      "|  1|   alex|    2019-01-01|   1000|\n",
      "|  1|   alex|    2019-03-01|   3300|\n",
      "|  2|   alex|    2019-02-01|   1500|\n",
      "+---+-------+--------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verwendung der merge Funktion (es gibt auch eine update() oder delete() Funktion)\n",
    "dt3=(deltaTable2.alias(\"oldData\")\n",
    "      .merge(df2a.alias(\"newData\"),\n",
    "            \"oldData.account = newData.account AND oldData.dt_transaction = newData.dt_transaction\")\n",
    "            .whenMatchedUpdateAll()\n",
    "            .whenNotMatchedInsertAll()\n",
    "      .execute()\n",
    "    )\n",
    "\n",
    "deltaTable2.toDF().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e24bd7",
   "metadata": {},
   "source": [
    "### Delta: Roleback\n",
    "Delta bietet die Möglichkeiten direkt auf den Datenstand einer bestimmten Version oder eines Zeitpunktes zurück zu gehen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e42a0894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------+-------+\n",
      "| id|account|dt_transaction|balance|\n",
      "+---+-------+--------------+-------+\n",
      "|  4|  maria|    2020-01-01|   5000|\n",
      "|  2|  peter|    2021-01-01|    100|\n",
      "|  1|   alex|    2019-01-01|   1000|\n",
      "|  1|   alex|    2019-03-01|   3300|\n",
      "|  2|   alex|    2019-02-01|   1500|\n",
      "+---+-------+--------------+-------+\n",
      "\n",
      "+-------+-----------+-------------------+---------+---------+--------+-------------+\n",
      "|version|readVersion|timestamp          |operation|mode     |numFiles|numOutputRows|\n",
      "+-------+-----------+-------------------+---------+---------+--------+-------------+\n",
      "|3      |2          |2023-07-20 10:19:18|MERGE    |null     |null    |2            |\n",
      "|2      |1          |2023-07-20 10:18:12|MERGE    |null     |null    |3            |\n",
      "|1      |0          |2023-07-20 10:06:23|WRITE    |Overwrite|3       |4            |\n",
      "|0      |null       |2023-07-20 09:56:22|WRITE    |Overwrite|3       |4            |\n",
      "+-------+-----------+-------------------+---------+---------+--------+-------------+\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DeltaTable' object has no attribute 'restoreToVersion'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-05849318bb44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdeltaTable2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdeltaTable2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"version\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"readVersion\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"timestamp\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"operation\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"operationParameters.mode\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"operationMetrics.numFiles\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"operationMetrics.numOutputRows\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdeltaTable2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestoreToVersion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#restoreToTimestamp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#isDeltaTable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DeltaTable' object has no attribute 'restoreToVersion'"
     ]
    }
   ],
   "source": [
    "deltaTable2.toDF().show()\n",
    "deltaTable2.history().select(\"version\",\"readVersion\",\"timestamp\",\"operation\",\"operationParameters.mode\",\"operationMetrics.numFiles\",\"operationMetrics.numOutputRows\").show(truncate=False)\n",
    "deltaTable2.restoreToVersion(0)\n",
    "#restoreToTimestamp\n",
    "#isDeltaTable\n",
    "deltaTable2.toDF().show()\n",
    "#spark.read.format(\"delta\").load(f\"s3://{bucket}/delta\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe344799",
   "metadata": {},
   "source": [
    "#### Erkenntnisse Delta\n",
    "* Wie funktioniert das Metadatenmanagement und was steht im Delta Log?\n",
    "* Schema Evolution: Kann das Schema erweitert werden, also eine neue Spalte angefügt werden?\n",
    "* Schema Enforcement on write: Kann eine Spalte mit falschem Datetyp einfach beim schreiben hinzugefügt werden? \n",
    "* Schema Enforcement on read: Kann ein Verzeichnis mit mehreren Parquet Dateien bei der eine Spalte ein anderes Schema hat gelesen werden? (um die Ecke Denk Frage)\n",
    "* Was ermöglichen mir die Metadaten (Historie, Audit etc)\n",
    "* Was ist der Vorteil der Merge Funktion? Wie müsste ich sonst Dateibasiert einen Merge/Update durchführen?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d11faea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6ffc289a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                       Version\n",
      "----------------------------- -------------------\n",
      "aiobotocore                   2.3.4\n",
      "aiohttp                       3.8.1\n",
      "aioitertools                  0.10.0\n",
      "aiosignal                     1.2.0\n",
      "appnope                       0.1.2\n",
      "argon2-cffi                   20.1.0\n",
      "async-generator               1.10\n",
      "async-timeout                 4.0.2\n",
      "attrs                         21.2.0\n",
      "backcall                      0.2.0\n",
      "bleach                        3.3.1\n",
      "boto3                         1.22.2\n",
      "botocore                      1.24.21\n",
      "Bottleneck                    1.3.2\n",
      "cachetools                    5.2.0\n",
      "certifi                       2021.10.8\n",
      "cffi                          1.14.6\n",
      "charset-normalizer            2.0.12\n",
      "click                         8.1.3\n",
      "cryptography                  37.0.2\n",
      "decorator                     5.0.9\n",
      "defusedxml                    0.7.1\n",
      "delta                         0.4.2\n",
      "delta-spark                   2.1.1\n",
      "entrypoints                   0.3\n",
      "frozenlist                    1.3.0\n",
      "fsspec                        2022.5.0\n",
      "google-api-core               2.8.2\n",
      "google-auth                   2.9.0\n",
      "google-cloud                  0.34.0\n",
      "google-cloud-bigquery         3.2.0\n",
      "google-cloud-bigquery-storage 2.13.2\n",
      "google-cloud-core             2.3.1\n",
      "google-cloud-storage          2.4.0\n",
      "google-crc32c                 1.3.0\n",
      "google-resumable-media        2.3.3\n",
      "googleapis-common-protos      1.56.3\n",
      "grpcio                        1.47.0\n",
      "grpcio-status                 1.47.0\n",
      "idna                          3.3\n",
      "importlib-metadata            3.10.0\n",
      "iniconfig                     1.1.1\n",
      "ipykernel                     5.3.4\n",
      "ipython                       7.22.0\n",
      "ipython-genutils              0.2.0\n",
      "ipywidgets                    7.6.3\n",
      "jedi                          0.17.0\n",
      "Jinja2                        3.0.1\n",
      "jmespath                      1.0.0\n",
      "jsonschema                    3.2.0\n",
      "jupyter                       1.0.0\n",
      "jupyter-client                6.1.12\n",
      "jupyter-console               6.4.0\n",
      "jupyter-core                  4.7.1\n",
      "jupyterlab-pygments           0.1.2\n",
      "jupyterlab-widgets            1.0.0\n",
      "logging                       0.4.9.6\n",
      "lorem-text                    2.1\n",
      "MarkupSafe                    2.0.1\n",
      "mistune                       0.8.4\n",
      "mkl-fft                       1.3.0\n",
      "mkl-random                    1.2.2\n",
      "mkl-service                   2.4.0\n",
      "moto                          3.1.11\n",
      "multidict                     6.0.2\n",
      "names                         0.3.0\n",
      "nbclient                      0.5.3\n",
      "nbconvert                     6.1.0\n",
      "nbformat                      5.1.3\n",
      "nest-asyncio                  1.5.1\n",
      "notebook                      6.4.0\n",
      "numexpr                       2.7.3\n",
      "numpy                         1.20.3\n",
      "packaging                     21.0\n",
      "pandas                        1.3.0\n",
      "pandocfilters                 1.4.3\n",
      "parso                         0.8.2\n",
      "pexpect                       4.8.0\n",
      "pickleshare                   0.7.5\n",
      "pip                           21.1.3\n",
      "pluggy                        1.0.0\n",
      "prometheus-client             0.11.0\n",
      "prompt-toolkit                3.0.17\n",
      "proto-plus                    1.20.6\n",
      "protobuf                      3.20.1\n",
      "psycopg2                      2.8.6\n",
      "ptyprocess                    0.7.0\n",
      "py                            1.11.0\n",
      "py4j                          0.10.9.5\n",
      "pyarrow                       4.0.1\n",
      "pyasn1                        0.4.8\n",
      "pyasn1-modules                0.2.8\n",
      "pycparser                     2.20\n",
      "Pygments                      2.9.0\n",
      "pyparsing                     2.4.7\n",
      "pyrsistent                    0.17.3\n",
      "pyspark                       3.1.2\n",
      "pytest                        7.1.2\n",
      "pytest-responsemock           1.1.1\n",
      "python-dateutil               2.8.2\n",
      "pytz                          2021.1\n",
      "PyYAML                        6.0\n",
      "pyzmq                         20.0.0\n",
      "qtconsole                     5.1.0\n",
      "QtPy                          1.9.0\n",
      "requests                      2.27.1\n",
      "responses                     0.20.0\n",
      "rsa                           4.8\n",
      "s3fs                          2022.5.0\n",
      "s3transfer                    0.5.2\n",
      "Send2Trash                    1.5.0\n",
      "setuptools                    52.0.0.post20210125\n",
      "six                           1.16.0\n",
      "terminado                     0.9.4\n",
      "testpath                      0.5.0\n",
      "tomli                         2.0.1\n",
      "tornado                       6.1\n",
      "traitlets                     5.0.5\n",
      "typing-extensions             4.3.0\n",
      "urllib3                       1.26.9\n",
      "wcwidth                       0.2.5\n",
      "webencodings                  0.5.1\n",
      "Werkzeug                      2.1.2\n",
      "wheel                         0.36.2\n",
      "widgetsnbextension            3.5.1\n",
      "wrapt                         1.14.1\n",
      "xmltodict                     0.13.0\n",
      "yarl                          1.7.2\n",
      "zipp                          3.5.0\n"
     ]
    }
   ],
   "source": [
    "! pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2269e18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
