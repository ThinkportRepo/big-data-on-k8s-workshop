{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "610d267c",
   "metadata": {},
   "source": [
    "# Demo and Comparission of Big Data File Formats\n",
    "This Notebook runs only on a local Spark Environment, not on Kubernetes\n",
    "\n",
    "## 1. CSV and JSON\n",
    "Old dat formats that are not designed for big data and scaling  \n",
    "**Typical feature:** humand readable\n",
    "\n",
    "## 2. Avro, OCR, Parquet\n",
    "First generation of special big data formats that allow fast writes, fast reads or both  \n",
    "**Typical features:** splittable, compressible, data skipping and predicat pushdown, data schema inclueded\n",
    "\n",
    "\n",
    "\n",
    "## 3. Delta, Iceberg, Hudi\n",
    "Latest generation of big data format that support ACID transaction, audit save transaction logs and time travel  \n",
    "**Typical features:** enhancing first generation format with additonal meta data and read/write procedures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72a3b58b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container {width:100% !important; }<style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#################################################################################\n",
    "# Laden aller relevate Module\n",
    "#################################################################################\n",
    "import pyspark\n",
    "from pyspark.sql.functions import *\n",
    "import json\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from delta import *\n",
    "import delta\n",
    "\n",
    "# use 95% of the screen for jupyter cell\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container {width:100% !important; }<style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3e1e98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first for local usage pip install delta-spark\n",
    "\n",
    "builder = pyspark.sql.SparkSession.builder.appName(\"MyApp\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\") \\\n",
    "    .config(\"spark.jars\",  \"/Users/alor/opt/spark/jars/spark-avro_2.12-3.3.1.jar\") \\\n",
    "    .config(\"spark.driver.extraClassPath\",\"/Users/alor/opt/spark/jars/spark-avro_2.12-3.3.1.jar\") \\\n",
    "    .config(\"spark.executor.extraClassPath\",\"/Users/alor/opt/spark/jars/spark-avro_2.12-3.3.1.jar\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28f7c8e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://macbook-thinkport.fritz.box:4044\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fd67875be20>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27495f26",
   "metadata": {},
   "source": [
    "## Create sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1b8b7c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++ create new dataframe and show schema and data\n",
      "################################################\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- account: string (nullable = true)\n",
      " |-- dt_transaction: date (nullable = true)\n",
      " |-- balance: long (nullable = true)\n",
      "\n",
      "+---+-------+--------------+-------+\n",
      "|id |account|dt_transaction|balance|\n",
      "+---+-------+--------------+-------+\n",
      "|2  |alex   |2019-02-01    |1500   |\n",
      "|3  |alex   |2019-03-01    |1700   |\n",
      "|1  |alex   |2019-01-01    |1000   |\n",
      "|4  |maria  |2020-01-01    |5000   |\n",
      "+---+-------+--------------+-------+\n",
      "\n",
      "+---+-------+--------------+-------+-------------+\n",
      "|id |account|dt_transaction|balance|new          |\n",
      "+---+-------+--------------+-------+-------------+\n",
      "|1  |otto   |2019-10-01    |4444   |neue Spalte 1|\n",
      "+---+-------+--------------+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# initial daten\n",
    "account_data1 = [\n",
    "    (1,\"alex\",\"2019-01-01\",1000),\n",
    "    (2,\"alex\",\"2019-02-01\",1500),\n",
    "    (3,\"alex\",\"2019-03-01\",1700),\n",
    "    (4,\"maria\",\"2020-01-01\",5000)\n",
    "    ]\n",
    "\n",
    "# update mit Änderung und neuem Datensat\n",
    "account_data2 = [\n",
    "    (1,\"alex\",\"2019-03-01\",3300),\n",
    "    (2,\"peter\",\"2021-01-01\",100)\n",
    "    ]\n",
    "\n",
    "# Update mit neuer Spalte\n",
    "account_data3 = [\n",
    "    (1,\"otto\",\"2019-10-01\",4444,\"neue Spalte 1\")\n",
    "]\n",
    "\n",
    "schema = [\"id\",\"account\",\"dt_transaction\",\"balance\"]\n",
    "schema3 = [\"id\",\"account\",\"dt_transaction\",\"balance\",\"new\"]\n",
    "\n",
    "df1 = spark.createDataFrame(data=account_data1, schema = schema).withColumn(\"dt_transaction\",col(\"dt_transaction\").cast(\"date\")).repartition(3)\n",
    "df2 = spark.createDataFrame(data=account_data2, schema = schema).withColumn(\"dt_transaction\",col(\"dt_transaction\").cast(\"date\")).repartition(3)\n",
    "df3 = spark.createDataFrame(data=account_data3, schema = schema3).withColumn(\"dt_transaction\",col(\"dt_transaction\").cast(\"date\")).repartition(3)\n",
    "\n",
    "print(\"++ create new dataframe and show schema and data\")\n",
    "print(\"################################################\")\n",
    "\n",
    "df1.printSchema()\n",
    "df1.show(truncate=False)\n",
    "df3.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6fe705",
   "metadata": {},
   "source": [
    "## CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "916ad0c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Partitions: 3\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Partitions:\", df1.rdd.getNumPartitions())\n",
    "\n",
    "write_csv=(df1\n",
    "           .write\n",
    "           .format(\"csv\")\n",
    "           .mode(\"overwrite\") # append\n",
    "           .save(\"output/csv\")\n",
    "          )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac9d2c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_SUCCESS\r\n",
      "part-00000-a4b2c68c-b0ea-462c-b90b-f1df7ae7432f-c000.csv\r\n",
      "part-00001-a4b2c68c-b0ea-462c-b90b-f1df7ae7432f-c000.csv\r\n",
      "part-00002-a4b2c68c-b0ea-462c-b90b-f1df7ae7432f-c000.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls output/csv/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15fb71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat output/csv/part-00002-ec2256a8-ce7d-4629-b968-78fd32cc337b-c000.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f31d31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      "\n",
      "+---+-----+----------+----+\n",
      "|_c0|  _c1|       _c2| _c3|\n",
      "+---+-----+----------+----+\n",
      "|  2| alex|2019-02-01|1500|\n",
      "|  3| alex|2019-03-01|1700|\n",
      "|  4|maria|2020-01-01|5000|\n",
      "|  1| alex|2019-01-01|1000|\n",
      "+---+-----+----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "read_csv=spark.read.format(\"csv\").load(\"output/csv\")\n",
    "\n",
    "read_csv.printSchema()\n",
    "read_csv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c5a5979",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_csv=(df3\n",
    "           .write\n",
    "           .format(\"csv\")\n",
    "           .mode(\"append\") # append\n",
    "           .save(\"output/csv\")\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a771362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_SUCCESS\r\n",
      "part-00000-1a7710fb-4aad-466f-824f-e01e557c45a1-c000.csv\r\n",
      "part-00000-a4b2c68c-b0ea-462c-b90b-f1df7ae7432f-c000.csv\r\n",
      "part-00001-a4b2c68c-b0ea-462c-b90b-f1df7ae7432f-c000.csv\r\n",
      "part-00002-1a7710fb-4aad-466f-824f-e01e557c45a1-c000.csv\r\n",
      "part-00002-a4b2c68c-b0ea-462c-b90b-f1df7ae7432f-c000.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls output/csv/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349331cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat output/csv/part-00002-ec2256a8-ce7d-4629-b968-78fd32cc337b-c000.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31fd06f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      "\n",
      "+---+-----+----------+----+\n",
      "|_c0|  _c1|       _c2| _c3|\n",
      "+---+-----+----------+----+\n",
      "|  2| alex|2019-02-01|1500|\n",
      "|  3| alex|2019-03-01|1700|\n",
      "|  1| otto|2019-10-01|4444|\n",
      "|  4|maria|2020-01-01|5000|\n",
      "|  1| alex|2019-01-01|1000|\n",
      "+---+-----+----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "read_csv=spark.read.format(\"csv\").load(\"output/csv\")\n",
    "\n",
    "read_csv.printSchema()\n",
    "read_csv.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1b3038",
   "metadata": {},
   "source": [
    "* kein Schema (Typen)\n",
    "* kein anfügen neuer Spalten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55bcbfa",
   "metadata": {},
   "source": [
    "## JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e368413b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Partitions: 3\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Partitions:\", df1.rdd.getNumPartitions())\n",
    "\n",
    "write_json=(df1\n",
    "           .write\n",
    "           .format(\"json\")\n",
    "           .mode(\"overwrite\") # append\n",
    "           .save(\"output/json\")\n",
    "          )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da9898f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_SUCCESS\r\n",
      "part-00000-1fbeb738-5e9f-4030-83fd-7b23803cb6fc-c000.json\r\n",
      "part-00001-1fbeb738-5e9f-4030-83fd-7b23803cb6fc-c000.json\r\n",
      "part-00002-1fbeb738-5e9f-4030-83fd-7b23803cb6fc-c000.json\r\n"
     ]
    }
   ],
   "source": [
    "!ls output/json/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba7f5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat output/json/part-00000-980c6e26-7124-40ba-816d-f8a76e82a796-c000.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7506f71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_csv=(df3\n",
    "           .write\n",
    "           .format(\"json\")\n",
    "           .mode(\"append\") # append\n",
    "           .save(\"output/json\")\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba940d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- account: string (nullable = true)\n",
      " |-- balance: long (nullable = true)\n",
      " |-- dt_transaction: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- new: string (nullable = true)\n",
      "\n",
      "+-------+-------+--------------+---+-------------+\n",
      "|account|balance|dt_transaction| id|          new|\n",
      "+-------+-------+--------------+---+-------------+\n",
      "|   alex|   1500|    2019-02-01|  2|         null|\n",
      "|   alex|   1700|    2019-03-01|  3|         null|\n",
      "|   otto|   4444|    2019-10-01|  1|neue Spalte 1|\n",
      "|  maria|   5000|    2020-01-01|  4|         null|\n",
      "|   alex|   1000|    2019-01-01|  1|         null|\n",
      "+-------+-------+--------------+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "read_json=spark.read.format(\"json\").load(\"output/json\")\n",
    "\n",
    "read_json.printSchema()\n",
    "read_json.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad580792",
   "metadata": {},
   "source": [
    "* Kein Schema\n",
    "* Neue Spalten werden als neues Attribut hinzugefügt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094e8bb0",
   "metadata": {},
   "source": [
    "## Avro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44777375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Partitions: 3\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Partitions:\", df1.rdd.getNumPartitions())\n",
    "\n",
    "write_avro=(df1\n",
    "           .write\n",
    "           .format(\"avro\")\n",
    "           .mode(\"overwrite\") # append\n",
    "           .save(\"output/avro\")\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d82f2e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_SUCCESS\r\n",
      "part-00000-ef4ebba4-291b-47ad-a18b-b9bfe58a3356-c000.avro\r\n",
      "part-00001-ef4ebba4-291b-47ad-a18b-b9bfe58a3356-c000.avro\r\n",
      "part-00002-ef4ebba4-291b-47ad-a18b-b9bfe58a3356-c000.avro\r\n"
     ]
    }
   ],
   "source": [
    "!ls output/avro/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0cf679",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat output/avro/part-00000-5d8db30e-9134-4166-a9d3-f562c0ee0a46-c000.avro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ac72f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- account: string (nullable = true)\n",
      " |-- dt_transaction: date (nullable = true)\n",
      " |-- balance: long (nullable = true)\n",
      "\n",
      "+---+-------+--------------+-------+\n",
      "| id|account|dt_transaction|balance|\n",
      "+---+-------+--------------+-------+\n",
      "|  2|   alex|    2019-02-01|   1500|\n",
      "|  3|   alex|    2019-03-01|   1700|\n",
      "|  4|  maria|    2020-01-01|   5000|\n",
      "|  1|   alex|    2019-01-01|   1000|\n",
      "+---+-------+--------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "read_json=spark.read.format(\"avro\").load(\"output/avro\")\n",
    "read_json.printSchema()\n",
    "read_json.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b1e0ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_avro=(df3\n",
    "           .write\n",
    "           .format(\"avro\")\n",
    "           .mode(\"append\") # append\n",
    "           .save(\"output/avro\")\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "70570799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- account: string (nullable = true)\n",
      " |-- dt_transaction: date (nullable = true)\n",
      " |-- balance: long (nullable = true)\n",
      "\n",
      "+---+-------+--------------+-------+\n",
      "| id|account|dt_transaction|balance|\n",
      "+---+-------+--------------+-------+\n",
      "|  1|   otto|    2019-10-01|   4444|\n",
      "|  2|   alex|    2019-02-01|   1500|\n",
      "|  3|   alex|    2019-03-01|   1700|\n",
      "|  4|  maria|    2020-01-01|   5000|\n",
      "|  1|   alex|    2019-01-01|   1000|\n",
      "+---+-------+--------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "read_json=spark.read.format(\"avro\").load(\"output/avro\")\n",
    "read_json.printSchema()\n",
    "read_json.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b1054d",
   "metadata": {},
   "source": [
    "* Schema erhalten\n",
    "* Schema evolution "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45de295",
   "metadata": {},
   "source": [
    "## Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fb546000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Partitions: 3\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Partitions:\", df1.rdd.getNumPartitions())\n",
    "\n",
    "write_parquet=(df1\n",
    "           .write\n",
    "           .partitionBy(\"account\")\n",
    "           .format(\"parquet\")\n",
    "           .mode(\"overwrite\") # append\n",
    "           .save(\"output/parquet\")\n",
    "          )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "49b6d692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_SUCCESS      \u001b[34maccount=alex\u001b[m\u001b[m  \u001b[34maccount=maria\u001b[m\u001b[m\n",
      "----------------------------\n",
      "part-00000-885ea86e-bdd8-41fc-9498-1cf9099c438c.c000.snappy.parquet\n",
      "part-00001-885ea86e-bdd8-41fc-9498-1cf9099c438c.c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "!ls  output/parquet/\n",
    "!echo \"----------------------------\"\n",
    "!ls output/parquet/account=alex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e99e854",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat output/parquet/account=alex/part-00000-9ce7ae8e-705a-4f93-acc8-d8cca35d5945.c000.snappy.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e00a1e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- dt_transaction: date (nullable = true)\n",
      " |-- balance: long (nullable = true)\n",
      " |-- account: string (nullable = true)\n",
      "\n",
      "+---+--------------+-------+-------+\n",
      "| id|dt_transaction|balance|account|\n",
      "+---+--------------+-------+-------+\n",
      "|  2|    2019-02-01|   1500|   alex|\n",
      "|  3|    2019-03-01|   1700|   alex|\n",
      "|  1|    2019-01-01|   1000|   alex|\n",
      "+---+--------------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "read_parquet=(spark\n",
    "              .read.format(\"parquet\")\n",
    "              .load(\"output/parquet/\")\n",
    "              .filter(col(\"account\")==\"alex\")\n",
    "             )\n",
    "\n",
    "read_parquet.printSchema()\n",
    "read_parquet.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eadd53c",
   "metadata": {},
   "source": [
    "## Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "737c3d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: output/delta: No such file or directory\n",
      "total 0\n",
      "drwxr-xr-x  14 alor  staff  448 Mar 13 13:09 \u001b[34mavro\u001b[m\u001b[m\n",
      "drwxr-xr-x  14 alor  staff  448 Mar 13 13:09 \u001b[34mcsv\u001b[m\u001b[m\n",
      "drwxr-xr-x  14 alor  staff  448 Mar 13 13:09 \u001b[34mjson\u001b[m\u001b[m\n",
      "drwxr-xr-x   6 alor  staff  192 Mar 13 13:10 \u001b[34mparquet\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "# clean old stuff\n",
    "! rm -r output/delta\n",
    "! ls -l output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513d213c",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_delta=(df1\n",
    "           .write\n",
    "           .format(\"delta\")\n",
    "           .option(\"mergeSchema\", \"false\")\n",
    "           .mode(\"overwrite\") # append\n",
    "           .save(\"output/delta\")\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cd6f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls output/delta\n",
    "! echo \"-------------------------------------------------------------------\"\n",
    "! ls output/delta/_delta_log\n",
    "! cat output/delta/_delta_log/00000000000000000000.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a714c074",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_delta=(df2\n",
    "           .write\n",
    "           .format(\"delta\")\n",
    "           .mode(\"append\") # append\n",
    "           .save(\"output/delta\")\n",
    "          )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d72dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_delta=(df3\n",
    "           .write\n",
    "           .format(\"delta\")\n",
    "           .option(\"mergeSchema\", \"true\")\n",
    "           .mode(\"overwrite\") # append\n",
    "           .save(\"output/delta\")\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bf92d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls output/delta\n",
    "! echo \"-------------------------------------------------------------------\"\n",
    "! ls output/delta/_delta_log\n",
    "! cat output/delta/_delta_log/00000000000000000002.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb3480c",
   "metadata": {},
   "outputs": [],
   "source": [
    "deltaTable = DeltaTable.forPath(spark, \"output/delta\")\n",
    "\n",
    "fullHistoryDF = deltaTable.history()    # get the full history of the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac186df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fullHistoryDF.select(\"version\",\"readVersion\",\"timestamp\",\"userId\",\"operation\",\"operationParameters\",\"operationMetrics\",\"userMetadata\").show(truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f89cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_parquet=spark.read.format(\"delta\").load(\"output/delta/\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc61efc",
   "metadata": {},
   "source": [
    "## Time travel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6617afc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.format(\"delta\").option(\"versionAsOf\", \"1\").load(\"output/delta\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55d249d",
   "metadata": {},
   "source": [
    "## Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aea3fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "deltaTable2 = DeltaTable.forPath(spark, \"output/delta\")\n",
    "\n",
    "\n",
    "df2a=df2.withColumn(\"new\",lit(\"test\"))\n",
    "df2a.show()\n",
    "deltaTable2.toDF().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1688108",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt3=(deltaTable2.alias(\"oldData\")\n",
    "      .merge(df2a.alias(\"newData\"),\n",
    "            \"oldData.account = newData.account AND oldData.dt_transaction = newData.dt_transaction\")\n",
    "            .whenMatchedUpdateAll()\n",
    "            .whenNotMatchedInsertAll()\n",
    "      .execute()\n",
    "    )\n",
    "\n",
    "deltaTable2.toDF().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42a0894",
   "metadata": {},
   "outputs": [],
   "source": [
    "result=(deltaTable2\n",
    "        .toDF()\n",
    "        .withColumn(\"month\",month(col(\"dt_transaction\")))\n",
    "        .groupBy(\"account\",\"month\").agg(sum(\"balance\"))\n",
    "        .sort(\"account\",\"month\")\n",
    "       )\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db16769",
   "metadata": {},
   "outputs": [],
   "source": [
    "result=(spark.read\n",
    "        .format(\"delta\")\n",
    "        .option(\"versionAsOf\", \"1\")\n",
    "        .load(\"output/delta\")\n",
    "        .withColumn(\"month\",month(col(\"dt_transaction\")))\n",
    "        .groupBy(\"account\",\"month\").agg(sum(\"balance\"))\n",
    "        .sort(\"account\",\"month\")\n",
    "       )\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe344799",
   "metadata": {},
   "source": [
    "* Schema\n",
    "* Schema evolution\n",
    "* Transaction Log\n",
    "* Time Travel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11faea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814d698d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
