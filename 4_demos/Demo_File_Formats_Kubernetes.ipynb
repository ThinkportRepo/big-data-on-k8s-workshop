{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "610d267c",
   "metadata": {},
   "source": [
    "# Demo and Comparison of Big Data File Formats\n",
    "\n",
    "### CSV and JSON\n",
    "Old data formats that are not designed for big data and scaling  \n",
    "**Typical feature:** human-readable\n",
    "\n",
    "### Avro, ORC, Parquet\n",
    "First generation of special big data formats that allow fast writes, fast reads or both  \n",
    "**Typical features:** splittable, compressible, data skipping and predicat pushdown, data schema included\n",
    "\n",
    "### Delta, Iceberg, Hudi\n",
    "Latest generation of big data formats that support ACID transactions, audit save transaction logs and time travel  \n",
    "**Typical features:** enhancing first generation format with additonal meta data and read/write procedures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592e56dd",
   "metadata": {},
   "source": [
    "## 1. Import\n",
    "\n",
    "#### Import the necessary libraries for data processing using PySpark. Some of the important imports include:\n",
    "- SparkContext and SparkConf from pyspark: these libraries are used to initialize the Spark cluster and set up the configuration for the cluster.\n",
    "- SparkSession and SQLContext from pyspark.sql: These libraries are used for creating and interacting with Spark SQL contexts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0aa4fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "from delta import *\n",
    "\n",
    "import datetime\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import json\n",
    "import csv\n",
    "\n",
    "# use 95% of the screen for jupyter cell\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container {width:100% !important; }<style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3ba21e",
   "metadata": {},
   "source": [
    "## 2. Launch Spark Jupyter and Configuration\n",
    "\n",
    "#### Configure a Spark session for Kubernetes cluster with S3 support\n",
    "### CLUSTER MANAGER\n",
    "- set the Kubernetes master URL as Cluster Manager(“k8s://https://” is NOT a typo, this is how Spark knows the “provider” type)\n",
    "\n",
    "### KUBERNETES\n",
    "- set the namespace that will be used for running the driver and executor pods\n",
    "- set the docker image from which the Worker/Exectutor pods are created\n",
    "- set the Kubernetes service account name and provide the authentication details for the service account (required to create worker pods)\n",
    "\n",
    "### SPARK\n",
    "- set the driver host and the driver port (find name of the driver service with 'kubectl get services' or in the helm chart configuration)\n",
    "- enable Delta Lake, Iceberg, and Hudi support by setting the spark.sql.extensions\n",
    "- configure Hive catalog for Iceberg\n",
    "- enable S3 connector\n",
    "- set the number of worker pods, their memory and cores (HINT: number of possible tasks = cores * executores)\n",
    "\n",
    "### SPARK SESSION\n",
    "- create the Spark session using the SparkSession.builder object\n",
    "- get the Spark context from the created session and set the log level to \"ERROR\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1faf0688",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9f9068",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "appName=\"jupyter-spark\"\n",
    "\n",
    "conf = SparkConf()\n",
    "\n",
    "# CLUSTER MANAGER\n",
    "\n",
    "conf.setMaster(\"k8s://https://kubernetes.default.svc.cluster.local:443\")\n",
    "\n",
    "# CONFIGURE KUBERNETES\n",
    "\n",
    "conf.set(\"spark.kubernetes.namespace\",\"frontend\")\n",
    "conf.set(\"spark.kubernetes.container.image\", \"thinkportgmbh/workshops:spark-3.3.2\")\n",
    "conf.set(\"spark.kubernetes.container.image.pullPolicy\", \"Always\")\n",
    "\n",
    "conf.set(\"spark.kubernetes.authenticate.driver.serviceAccountName\", \"spark\")\n",
    "conf.set(\"spark.kubernetes.authenticate.caCertFile\", \"/var/run/secrets/kubernetes.io/serviceaccount/ca.crt\")\n",
    "conf.set(\"spark.kubernetes.authenticate.oauthTokenFile\", \"/var/run/secrets/kubernetes.io/serviceaccount/token\")\n",
    "\n",
    "# CONFIGURE SPARK\n",
    "\n",
    "conf.set(\"spark.sql.session.timeZone\", \"Europe/Berlin\")\n",
    "conf.set(\"spark.driver.host\", \"jupyter-spark-driver.frontend.svc.cluster.local\")\n",
    "conf.set(\"spark.driver.port\", \"29413\")\n",
    "\n",
    "conf.set(\"spark.jars\", \"/opt/spark/jars/spark-avro_2.12-3.3.2.jar\")\n",
    "conf.set(\"spark.driver.extraClassPath\",\"/opt/spark/jars/spark-avro_2.12-3.3.2.jar\")\n",
    "conf.set(\"spark.executor.extraClassPath\",\"/opt/spark/jars/spark-avro_2.12-3.3.2.jar\")\n",
    "\n",
    "conf.set(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension, org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions, org.apache.spark.sql.hudi.HoodieSparkSessionExtension\")\n",
    "\n",
    "######## Hive als Metastore einbinden\n",
    "conf.set(\"hive.metastore.uris\", \"thrift://hive-metastore.hive.svc.cluster.local:9083\") \n",
    "\n",
    "######## Iceberg configs\n",
    "conf.set(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\")\n",
    "conf.set(\"spark.sql.catalog.ice\",\"org.apache.iceberg.spark.SparkCatalog\") \n",
    "conf.set(\"spark.sql.catalog.ice.type\",\"hive\") \n",
    "conf.set(\"spark.sql.catalog.ice.uri\",\"thrift://hive-metastore.hive.svc.cluster.local:9083\") \n",
    "\n",
    "\n",
    "####### Hudi configs\n",
    "conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "\n",
    "# CONFIGURE S3 CONNECTOR\n",
    "conf.set(\"spark.hadoop.fs.s3a.endpoint\", \"minio.minio.svc.cluster.local:9000\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.access.key\", \"trainadm\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.secret.key\", \"train@thinkport\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "conf.set(\"spark.hadoop.fs.s3.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "\n",
    "# CONFIGURE WORKER (Customize based on workload)\n",
    "\n",
    "conf.set(\"spark.executor.instances\", \"1\")\n",
    "conf.set(\"spark.executor.memory\", \"1G\")\n",
    "conf.set(\"spark.executor.cores\", \"2\")\n",
    "\n",
    "# SPARK SESSION\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .config(conf=conf) \\\n",
    "    .config('spark.sql.session.timeZone', 'Europe/Berlin') \\\n",
    "    .appName(appName)\\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "sc=spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "# get the configuration object to check all the configurations the session was startet with\n",
    "for entry in sc.getConf().getAll():\n",
    "        if entry[0] in [\"spark.app.name\",\"spark.kubernetes.namespace\",\"spark.executor.memory\",\"spark.executor.cores\",\"spark.driver.host\",\"spark.master\",\"spark.sql.extensions\"]:\n",
    "            print(entry[0],\"=\",entry[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27495f26",
   "metadata": {},
   "source": [
    "## Create sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b8b7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial daten\n",
    "account_data1 = [\n",
    "    (1,\"alex\",\"2019-01-01\",1000),\n",
    "    (2,\"alex\",\"2019-02-01\",1500),\n",
    "    (3,\"alex\",\"2019-03-01\",1700),\n",
    "    (4,\"maria\",\"2020-01-01\",5000)\n",
    "    ]\n",
    "\n",
    "# update mit Änderung und neuem Datensat\n",
    "account_data2 = [\n",
    "    (1,\"alex\",\"2019-03-01\",3300),\n",
    "    (2,\"peter\",\"2021-01-01\",100)\n",
    "    ]\n",
    "\n",
    "# Update mit neuer Spalte\n",
    "account_data3 = [\n",
    "    (1,\"otto\",\"2019-10-01\",4444,\"neue Spalte 1\")\n",
    "]\n",
    "\n",
    "schema = [\"id\",\"account\",\"dt_transaction\",\"balance\"]\n",
    "schema3 = [\"id\",\"account\",\"dt_transaction\",\"balance\",\"new\"]\n",
    "\n",
    "df1 = spark.createDataFrame(data=account_data1, schema = schema).withColumn(\"dt_transaction\",col(\"dt_transaction\").cast(\"date\")).repartition(3)\n",
    "df2 = spark.createDataFrame(data=account_data2, schema = schema).withColumn(\"dt_transaction\",col(\"dt_transaction\").cast(\"date\")).repartition(3)\n",
    "df3 = spark.createDataFrame(data=account_data3, schema = schema3).withColumn(\"dt_transaction\",col(\"dt_transaction\").cast(\"date\")).repartition(3)\n",
    "\n",
    "print(\"++ create new dataframe and show schema and data\")\n",
    "print(\"################################################\")\n",
    "\n",
    "# df1.printSchema()\n",
    "df1.show(truncate=False)\n",
    "df3.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379a3b6a",
   "metadata": {},
   "source": [
    "## Configure boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d161c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.client import Config\n",
    "\n",
    "options = {\n",
    "    'endpoint_url': 'http://minio.minio.svc.cluster.local:9000',\n",
    "    'aws_access_key_id': 'trainadm',\n",
    "    'aws_secret_access_key': 'train@thinkport',\n",
    "    'config': Config(signature_version='s3v4'),\n",
    "    'verify': False}\n",
    "\n",
    "s3_resource = boto3.resource('s3', **options)  \n",
    "\n",
    "s3_client = boto3.client('s3', **options)\n",
    "\n",
    "bucket = \"fileformats\"\n",
    "bucket_path=\"s3://\"+bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272d6e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ls(bucket,prefix):\n",
    "    '''List objects from bucket/prefix'''\n",
    "    try:\n",
    "        for obj in s3_resource.Bucket(bucket).objects.filter(Prefix=prefix):\n",
    "            print(obj.key)\n",
    "    except Exception as e: \n",
    "        print(e)\n",
    "    \n",
    "    \n",
    "def cat(bucket,prefix,binary=False):\n",
    "    '''Show content of one or several files with same prefix/wildcard'''\n",
    "    try:\n",
    "        for obj in s3_resource.Bucket(bucket).objects.filter(Prefix=prefix):\n",
    "            print(\"File:\",obj.key)\n",
    "            print(\"----------------------\")\n",
    "            if binary==True:\n",
    "                print(obj.get()['Body'].read())\n",
    "            else: \n",
    "                print(obj.get()['Body'].read().decode())\n",
    "            print(\"######################\")\n",
    "    except Exception as e: \n",
    "        print(e)\n",
    "            \n",
    "def rm(bucket,prefix):\n",
    "    '''Delete everything from bucket/prefix'''\n",
    "    for object in s3_resource.Bucket(bucket).objects.filter(Prefix=prefix):\n",
    "        print(object.key)\n",
    "        s3_client.delete_object(Bucket=bucket, Key=object.key)\n",
    "    print(f\"Deleted files from {bucket}/{prefix}*\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07436ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show everything in bucket\n",
    "ls(bucket,\"\")\n",
    "print(\"#############################\")\n",
    "# show folder\n",
    "ls(bucket,\"csv\")\n",
    "print(\"#############################\")\n",
    "# show subfolder\n",
    "ls(bucket,\"delta/_delta_log/\")\n",
    "print(\"#############################\")\n",
    "print(\"\")\n",
    "# show content of one or several files with same prefix/wildcard\n",
    "cat(bucket,'csv/part')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f96cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm(\"fileformats\", \"csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6fe705",
   "metadata": {},
   "source": [
    "## CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916ad0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of Partitions:\", df1.rdd.getNumPartitions())\n",
    "\n",
    "write_csv=(df1\n",
    "           .write\n",
    "           .format(\"csv\")\n",
    "           .mode(\"overwrite\") # append\n",
    "           .save(f\"s3://{bucket}/csv\")\n",
    "          )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebbc9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls(bucket,\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fb51b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat(bucket,\"csv/part\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f31d31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_csv=spark.read.format(\"csv\").load(f\"s3://{bucket}/csv\")\n",
    "\n",
    "read_csv.printSchema()\n",
    "read_csv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5a5979",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_csv=(df3\n",
    "           .write\n",
    "           .format(\"csv\")\n",
    "           .mode(\"append\") # append\n",
    "           .save(f\"s3://{bucket}/csv\")\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a771362",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls(bucket,\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349331cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat(bucket,\"csv/part\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fd06f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_csv=spark.read.format(\"csv\").load(f\"s3://{bucket}/csv\")\n",
    "\n",
    "read_csv.printSchema()\n",
    "read_csv.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1b3038",
   "metadata": {},
   "source": [
    "* kein Schema (Typen)\n",
    "* kein anfügen neuer Spalten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55bcbfa",
   "metadata": {},
   "source": [
    "## JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e368413b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of Partitions:\", df1.rdd.getNumPartitions())\n",
    "\n",
    "write_json=(df1\n",
    "           .write\n",
    "           .format(\"json\")\n",
    "           .mode(\"overwrite\") # append\n",
    "           .save(f\"s3://{bucket}/json\")\n",
    "          )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9898f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls(bucket,\"json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba7f5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat(bucket,\"json/part\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7506f71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_csv=(df3\n",
    "           .write\n",
    "           .format(\"json\")\n",
    "           .mode(\"append\") # append\n",
    "           .save(f\"s3://{bucket}/json\")\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba940d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_json=spark.read.format(\"json\").load(f\"s3://{bucket}/json\")\n",
    "\n",
    "read_json.printSchema()\n",
    "read_json.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad580792",
   "metadata": {},
   "source": [
    "* Kein Schema\n",
    "* Neue Spalten werden als neues Attribut hinzugefügt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094e8bb0",
   "metadata": {},
   "source": [
    "## Avro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44777375",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of Partitions:\", df1.rdd.getNumPartitions())\n",
    "\n",
    "write_avro=(df1\n",
    "           .write\n",
    "           .format(\"avro\")\n",
    "           .mode(\"overwrite\") # append\n",
    "           .save(f\"s3://{bucket}/avro\")\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ebc8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls(bucket,\"avro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82f2e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat(bucket,\"avro/part\",True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac72f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_json=spark.read.format(\"avro\").load(f\"s3://{bucket}/avro\")\n",
    "read_json.printSchema()\n",
    "read_json.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1e0ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_avro=(df3\n",
    "           .write\n",
    "           .format(\"avro\")\n",
    "           .mode(\"append\") # append\n",
    "           .save(f\"s3://{bucket}/avro\")\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70570799",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_json=spark.read.format(\"avro\").load(f\"s3://{bucket}/avro\")\n",
    "read_json.printSchema()\n",
    "read_json.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b1054d",
   "metadata": {},
   "source": [
    "* Schema erhalten\n",
    "* Schema evolution "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45de295",
   "metadata": {},
   "source": [
    "## Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb546000",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of Partitions:\", df1.rdd.getNumPartitions())\n",
    "\n",
    "write_parquet=(df1\n",
    "           .write\n",
    "           .partitionBy(\"account\")\n",
    "           .format(\"parquet\")\n",
    "           .mode(\"overwrite\") # append\n",
    "           .save(f\"s3://{bucket}/parquet\")\n",
    "          )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9e08a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls(bucket,\"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b6d692",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat(bucket,\"parquet/account=maria\",True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00a1e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_parquet=(spark\n",
    "              .read.format(\"parquet\")\n",
    "              .load(f\"s3://{bucket}/parquet\")\n",
    "              .filter(col(\"account\")==\"alex\")\n",
    "             )\n",
    "\n",
    "read_parquet.printSchema()\n",
    "read_parquet.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eadd53c",
   "metadata": {},
   "source": [
    "## Delta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7d3e7e",
   "metadata": {},
   "source": [
    "- a **storage layer** that runs on top of existing data lakes\n",
    "- supports ACID transactions and data versioning\n",
    "- allows data lineage tracking\n",
    "- provides optimization for streaming workloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513d213c",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_delta=(df1\n",
    "           .write\n",
    "           .format(\"delta\")\n",
    "           .option(\"mergeSchema\", \"false\")\n",
    "           .mode(\"overwrite\") # append\n",
    "           .save(f\"s3://{bucket}/delta\")\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cd6f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls(bucket,\"delta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da732ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls(bucket,\"delta/_delta_log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3dbb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat(bucket,\"delta/_delta_log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a714c074",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_delta=(df2\n",
    "           .write\n",
    "           .format(\"delta\")\n",
    "           .mode(\"append\") # append\n",
    "           .save(f\"s3://{bucket}/delta\")\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d72dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_delta=(df3\n",
    "           .write\n",
    "           .format(\"delta\")\n",
    "           .option(\"mergeSchema\", \"true\")\n",
    "           .mode(\"overwrite\") # append\n",
    "           .save(f\"s3://{bucket}/delta\")\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bf92d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat(bucket,\"delta/_delta_log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb3480c",
   "metadata": {},
   "outputs": [],
   "source": [
    "deltaTable = DeltaTable.forPath(spark, f\"s3://{bucket}/delta\")\n",
    "# --> Vermutlich falsche Delta Version zu Spark\n",
    "fullHistoryDF = deltaTable.history()    # get the full history of the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac186df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fullHistoryDF.select(\"version\",\"readVersion\",\"timestamp\",\"userId\",\"operation\",\"operationParameters\",\"operationMetrics\",\"userMetadata\").show(truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f89cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.format(\"delta\").load(f\"s3://{bucket}/delta\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc61efc",
   "metadata": {},
   "source": [
    "## Delta: Time travel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6617afc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.format(\"delta\").option(\"versionAsOf\", \"1\").load(f\"s3://{bucket}/delta\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55d249d",
   "metadata": {},
   "source": [
    "## Delta: Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aea3fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "deltaTable2 = DeltaTable.forPath(spark, f\"s3://{bucket}/delta\")\n",
    "\n",
    "\n",
    "df2a=df2.withColumn(\"new\",f.lit(\"test\"))\n",
    "df2a.show()\n",
    "deltaTable2.toDF().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1688108",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt3=(deltaTable2.alias(\"oldData\")\n",
    "      .merge(df2a.alias(\"newData\"),\n",
    "            \"oldData.account = newData.account AND oldData.dt_transaction = newData.dt_transaction\")\n",
    "            .whenMatchedUpdateAll()\n",
    "            .whenNotMatchedInsertAll()\n",
    "      .execute()\n",
    "    )\n",
    "\n",
    "deltaTable2.toDF().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42a0894",
   "metadata": {},
   "outputs": [],
   "source": [
    "result=(deltaTable2\n",
    "        .toDF()\n",
    "        .withColumn(\"month\",f.month(col(\"dt_transaction\")))\n",
    "        .groupBy(\"account\",\"month\").agg(f.sum(\"balance\"))\n",
    "        .sort(\"account\",\"month\")\n",
    "       )\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db16769",
   "metadata": {},
   "outputs": [],
   "source": [
    "result=(spark.read\n",
    "        .format(\"delta\")\n",
    "        .option(\"versionAsOf\", \"1\")\n",
    "        .load(f\"s3://{bucket}/delta\")\n",
    "        .withColumn(\"month\",f.month(col(\"dt_transaction\")))\n",
    "        .groupBy(\"account\",\"month\").agg(f.sum(\"balance\"))\n",
    "        .sort(\"account\",\"month\")\n",
    "       )\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe344799",
   "metadata": {},
   "source": [
    "* Schema\n",
    "* Schema evolution\n",
    "* Transaction Log\n",
    "* Time Travel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b85012",
   "metadata": {},
   "source": [
    "## Iceberg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fafcace",
   "metadata": {},
   "source": [
    "- a **table format**\n",
    "- supports schema evolution and provides a portable table metadata format\n",
    "- best suited for analytical workloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb24f4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Current Catalog:\",spark.catalog.currentDatabase())\n",
    "print(\"List Catalogs:\",spark.catalog.listDatabases())\n",
    "print(\"List Tables in current Catalog:\",spark.catalog.listTables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7377b8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Database(name=<db_name>, locationUri='s3a://<bucket>/')\n",
    "spark.sql(f\"CREATE DATABASE iceberg_db LOCATION 's3a://{bucket}/'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b8232c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### show databases and tables in iceberg catalog (only sees iceberg formated tables)\n",
    "# all databases from hive are shown\n",
    "spark.sql(\"SHOW databases from ice\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5010f061",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"show tables from iceberg_db\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cba4b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Delete Iceberg tables: first drop the table \n",
    "#spark.sql(\"drop table iceberg_db.iceberg_table\")\n",
    "#delete_objects(\"aleks-test\", \"iceberg_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafa7b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_iceberg=(df1\n",
    "                  .write\n",
    "                  .format(\"iceberg\")\n",
    "                  .mode(\"overwrite\")\n",
    "                  .saveAsTable(\"iceberg_db.iceberg\")\n",
    "               )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494fb421",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls(bucket,\"iceberg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92013c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "write_iceberg=(df2\n",
    "                   .write\n",
    "                   .format(\"iceberg\")\n",
    "                   .mode(\"append\") # append\n",
    "                   .saveAsTable(\"iceberg_db.iceberg\")\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe763c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls(bucket,\"iceberg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964c7ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat(bucket,\"iceberg/metadata/00000-2346cea3-3db3-46a4-bb55-419ae993156b.metadata.json\",False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10efcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ALTER TABLE myTable ADD COLUMNS (address VARCHAR) - the number of columns in the df3 does not match the schema of the table, so we modify the schema of the existing table\n",
    "spark.sql(\"ALTER TABLE iceberg_db.iceberg ADD COLUMNS (new VARCHAR(50))\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31c068c",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_iceberg=(df3\n",
    "                  .write\n",
    "                  .format(\"iceberg\")\n",
    "                  .mode(\"append\") # append\n",
    "                  .option(\"schema\", schema3)\n",
    "                  .saveAsTable(\"iceberg_db.iceberg\")\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff8d8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read Iceberg table:\n",
    "\n",
    "iceberg_df = spark.read.table(\"iceberg_db.iceberg\")\n",
    "iceberg_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b286f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spark.sql(\"SELECT * FROM iceberg_db.iceberg.history;\").show()\n",
    "spark.sql(\"SELECT * FROM iceberg_db.iceberg.files;\").show()\n",
    "spark.sql(\"SELECT * FROM iceberg_db.iceberg.snapshots;\").show()\n",
    "\n",
    "## alternative syntax example:\n",
    "# spark.read.format(\"iceberg\").load(\"iceberg_db.iceberg_table.files\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6db241",
   "metadata": {},
   "source": [
    "### Iceberg: Time Travel\n",
    "- ```snapshot-id``` selects a specific table snapshot\n",
    "- ```as-of-timestamp``` selects the current snapshot at a timestamp, in milliseconds\n",
    "- ```branch``` selects the head snapshot of the specified branch. Note that currently branch cannot be combined with as-of-timestamp.\n",
    "- ```tag``` selects the snapshot associated with the specified tag. Tags cannot be combined with as-of-timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2d137e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the results of iceberg_table.snapshots get the snapshots IDs\n",
    "snapshot1 = spark.read \\\n",
    "                 .option(\"snapshot-id\", \"2282180466624073266\") \\\n",
    "                 .format(\"iceberg\") \\\n",
    "                 .load(\"iceberg_db.iceberg\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da46097d",
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot2 = spark.read \\\n",
    "                 .option(\"snapshot-id\", \"4263160168885610306\") \\\n",
    "                 .format(\"iceberg\") \\\n",
    "                 .load(\"iceberg_db.iceberg\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abaf72d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsToExpire = f.current_timestamp() - timedelta(minutes=10)\n",
    "print(tsToExpire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60455a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## need iceberg.table\n",
    "## geth nicht verstehe ich. noch nicht??\n",
    "table.expireSnapshots().expireOlderThan(tsToExpire).commit();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94da9d7c",
   "metadata": {},
   "source": [
    "# Hudi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e558c54",
   "metadata": {},
   "source": [
    "- a **storage abstraction layer** \n",
    "- enables data ingestion and query capability on large-scale, evolving datasets\n",
    "- well-suited for real-time streaming workloads and batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ae9de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update partition path, i.e. \"id/dt_transaction\"\n",
    "record_key = \"id\"\n",
    "partition_path = \"id\"\n",
    "\n",
    "hudi_options = {\n",
    "    \"hoodie.table.name\": df1,\n",
    "    \"hoodie.datasource.write.recordkey.field\": record_key,\n",
    "    \"hoodie.datasource.write.partitionpath.field\": partition_path,\n",
    "    \"hoodie.datasource.write.table.name\": df1,\n",
    "    \"hoodie.datasource.write.operation\": \"upsert\",\n",
    "    \"hoodie.datasource.write.precombine.field\": \"ts\",  # This field is used by Hoodie to resolve conflicts between records with the same key (in this case, id) \n",
    "    \"hoodie.upsert.shuffle.parallelism\": 2,\n",
    "    \"hoodie.insert.shuffle.parallelism\": 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad416e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_hudi=(df1.withColumn(\"ts\", f.current_timestamp()).write.format(\"hudi\") # \"ts\" field is a mandatory field in Hoodie that specifies the timestamp of the record, so we add a new column and use simple current_timestamp() function\n",
    "               .options(**hudi_options)\n",
    "               .mode(\"overwrite\")\n",
    "               .save(f\"s3://{bucket}/hudi\")\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2136c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls(bucket,\"hudi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52aa8b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_hudi=(df2.withColumn(\"ts\", f.current_timestamp()).write.format(\"hudi\") # \"ts\" field is a mandatory field in Hoodie that specifies the timestamp of the record, so we add a new column and use simple current_timestamp() function\n",
    "               .options(**hudi_options)\n",
    "               .mode(\"append\")\n",
    "               .save(f\"s3://{bucket}/hudi\")\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1b5e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_hudi=(df3.withColumn(\"ts\", f.current_timestamp()).write.format(\"hudi\") # \"ts\" field is a mandatory field in Hoodie that specifies the timestamp of the record, so we add a new column and use simple current_timestamp() function\n",
    "               .options(**hudi_options)\n",
    "               .mode(\"append\")\n",
    "               .save(f\"s3://{bucket}/hudi\")\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2066ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hudi = spark.read.format(\"hudi\").load(f\"s3://{bucket}/hudi\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814d698d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls(bucket,\"hudi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2c394a",
   "metadata": {},
   "source": [
    "#### Hudi: Time Travel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a428acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the commit time from the Hudi table\n",
    "\n",
    "spark.read.format(\"hudi\")\\\n",
    "     .option(\"as.of.instant\", \"20230515122339203\")\\\n",
    "     .load(f\"s3://{bucket}/hudi\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46611d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "account_data4 = [\n",
    "    (5,\"anna\",\"2020-11-01\",2000,\"neue Spalte 1\")\n",
    "]\n",
    "df4 = spark.createDataFrame(data=account_data4, schema = schema3).withColumn(\"dt_transaction\",col(\"dt_transaction\").cast(\"date\")).repartition(3)\n",
    "\n",
    "write_hudi=(df4.withColumn(\"ts\", f.current_timestamp()).write.format(\"hudi\")\n",
    "               .options(**hudi_options)\n",
    "               .mode(\"append\")\n",
    "               .save(f\"s3://{bucket}/hudi\")\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea78e5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Incremental query:\n",
    "\n",
    "spark.read.format(\"hudi\"). \\\n",
    "  load(f\"s3://{bucket}/hudi\"). \\\n",
    "  createOrReplaceTempView(\"hudi_snapshots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0c8bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "commits = list(map(lambda row: row[0], spark.sql(\"select distinct(_hoodie_commit_time) as commitTime from hudi_snapshots order by commitTime\").limit(10).collect()))\n",
    "print(commits)\n",
    "\n",
    "beginTime = commits[len(commits) - 4] # commit time we are interested in\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c791ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# incrementally query data\n",
    "incremental_read_options = {\n",
    "  'hoodie.datasource.query.type': 'incremental',\n",
    "  'hoodie.datasource.read.begin.instanttime': beginTime,\n",
    "}\n",
    "\n",
    "hudiIncrementalDF = spark.read.format(\"hudi\"). \\\n",
    "  options(**incremental_read_options). \\\n",
    "  load(f\"s3://{bucket}/hudi\")\n",
    "hudiIncrementalDF .createOrReplaceTempView(\"hudi_incremental\")\n",
    "\n",
    "spark.sql(\"select `_hoodie_commit_time`, account, balance, dt_transaction, ts from hudi_incremental\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ac5492",
   "metadata": {},
   "source": [
    "### Hudi: Table maintenance\n",
    "Hudi can run async or inline table services while running Strucrured Streaming query and takes care of cleaning, compaction and clustering. There's no operational overhead for the user.\n",
    "For CoW tables, table services work in inline mode by default.\n",
    "For MoR tables, some async services are enabled by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fcb4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
