{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "610d267c",
   "metadata": {},
   "source": [
    "# Demo and Comparison of Big Data File Formats\n",
    "\n",
    "### CSV and JSON\n",
    "Old data formats that are not designed for big data and scaling  \n",
    "**Typical feature:** human-readable\n",
    "\n",
    "### Avro, ORC, Parquet\n",
    "First generation of special big data formats that allow fast writes, fast reads or both  \n",
    "**Typical features:** splittable, compressible, data skipping and predicat pushdown, data schema included\n",
    "\n",
    "### Delta, Iceberg, Hudi\n",
    "Latest generation of big data formats that support ACID transactions, audit save transaction logs and time travel  \n",
    "**Typical features:** enhancing first generation format with additonal meta data and read/write procedures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592e56dd",
   "metadata": {},
   "source": [
    "## 1. Import\n",
    "\n",
    "#### Import the necessary libraries for data processing using PySpark. Some of the important imports include:\n",
    "- SparkContext and SparkConf from pyspark: these libraries are used to initialize the Spark cluster and set up the configuration for the cluster.\n",
    "- SparkSession and SQLContext from pyspark.sql: These libraries are used for creating and interacting with Spark SQL contexts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f0aa4fda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container {width:100% !important; }<style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "from delta import *\n",
    "\n",
    "import datetime\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import json\n",
    "import csv\n",
    "\n",
    "# use 95% of the screen for jupyter cell\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container {width:100% !important; }<style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "27c15040",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://jupyter-spark-driver.frontend.svc.cluster.local:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>k8s://https://kubernetes.default.svc.cluster.local:443</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>jupyter-spark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fe39b161ac0>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3ba21e",
   "metadata": {},
   "source": [
    "## 2. Launch Spark Jupyter and Configuration\n",
    "\n",
    "#### Configure a Spark session for Kubernetes cluster with S3 support\n",
    "### CLUSTER MANAGER\n",
    "- set the Kubernetes master URL as Cluster Manager(“k8s://https://” is NOT a typo, this is how Spark knows the “provider” type)\n",
    "\n",
    "### KUBERNETES\n",
    "- set the namespace that will be used for running the driver and executor pods\n",
    "- set the docker image from which the Worker/Exectutor pods are created\n",
    "- set the Kubernetes service account name and provide the authentication details for the service account (required to create worker pods)\n",
    "\n",
    "### SPARK\n",
    "- set the driver host and the driver port (find name of the driver service with 'kubectl get services' or in the helm chart configuration)\n",
    "- enable Delta Lake, Iceberg, and Hudi support by setting the spark.sql.extensions\n",
    "- configure Hive catalog for Iceberg\n",
    "- enable S3 connector\n",
    "- set the number of worker pods, their memory and cores (HINT: number of possible tasks = cores * executores)\n",
    "\n",
    "### SPARK SESSION\n",
    "- create the Spark session using the SparkSession.builder object\n",
    "- get the Spark context from the created session and set the log level to \"ERROR\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1faf0688",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f9f9068",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: hive.metastore.uris\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/14 20:09:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.kubernetes.namespace = frontend\n",
      "spark.sql.extensions = io.delta.sql.DeltaSparkSessionExtension, org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions, org.apache.spark.sql.hudi.HoodieSparkSessionExtension\n",
      "spark.master = k8s://https://kubernetes.default.svc.cluster.local:443\n",
      "spark.app.name = jupyter-spark\n",
      "spark.executor.memory = 1G\n",
      "spark.executor.cores = 2\n",
      "spark.driver.host = jupyter-spark-driver.frontend.svc.cluster.local\n"
     ]
    }
   ],
   "source": [
    "appName=\"jupyter-spark\"\n",
    "\n",
    "conf = SparkConf()\n",
    "\n",
    "# CLUSTER MANAGER\n",
    "\n",
    "conf.setMaster(\"k8s://https://kubernetes.default.svc.cluster.local:443\")\n",
    "\n",
    "# CONFIGURE KUBERNETES\n",
    "\n",
    "conf.set(\"spark.kubernetes.namespace\",\"frontend\")\n",
    "conf.set(\"spark.kubernetes.container.image\", \"thinkportgmbh/workshops:spark-3.3.1\")\n",
    "conf.set(\"spark.kubernetes.container.image.pullPolicy\", \"Always\")\n",
    "\n",
    "conf.set(\"spark.kubernetes.authenticate.driver.serviceAccountName\", \"spark\")\n",
    "conf.set(\"spark.kubernetes.authenticate.caCertFile\", \"/var/run/secrets/kubernetes.io/serviceaccount/ca.crt\")\n",
    "conf.set(\"spark.kubernetes.authenticate.oauthTokenFile\", \"/var/run/secrets/kubernetes.io/serviceaccount/token\")\n",
    "\n",
    "# CONFIGURE SPARK\n",
    "\n",
    "conf.set(\"spark.sql.session.timeZone\", \"Europe/Berlin\")\n",
    "conf.set(\"spark.driver.host\", \"jupyter-spark-driver.frontend.svc.cluster.local\")\n",
    "conf.set(\"spark.driver.port\", \"29413\")\n",
    "\n",
    "conf.set(\"spark.jars\", \"/opt/spark/jars/spark-avro_2.12-3.3.1.jar\")\n",
    "conf.set(\"spark.driver.extraClassPath\",\"/opt/spark/jars/spark-avro_2.12-3.3.1.jar\")\n",
    "conf.set(\"spark.executor.extraClassPath\",\"/opt/spark/jars/spark-avro_2.12-3.3.1.jar\")\n",
    "\n",
    "conf.set(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension, org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions, org.apache.spark.sql.hudi.HoodieSparkSessionExtension\")\n",
    "\n",
    "######## Hive als Metastore einbinden\n",
    "conf.set(\"hive.metastore.uris\", \"thrift://hive-metastore.hive.svc.cluster.local:9083\") \n",
    "\n",
    "######## Iceberg configs\n",
    "conf.set(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\")\n",
    "conf.set(\"spark.sql.catalog.ice\",\"org.apache.iceberg.spark.SparkCatalog\") \n",
    "conf.set(\"spark.sql.catalog.ice.type\",\"hive\") \n",
    "conf.set(\"spark.sql.catalog.ice.uri\",\"thrift://hive-metastore.hive.svc.cluster.local:9083\") \n",
    "\n",
    "\n",
    "####### Hudi configs\n",
    "conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "\n",
    "# CONFIGURE S3 CONNECTOR\n",
    "conf.set(\"spark.hadoop.fs.s3a.endpoint\", \"minio.minio.svc.cluster.local:9000\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.access.key\", \"trainadm\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.secret.key\", \"train@thinkport\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "conf.set(\"spark.hadoop.fs.s3.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "\n",
    "# CONFIGURE WORKER (Customize based on workload)\n",
    "\n",
    "conf.set(\"spark.executor.instances\", \"1\")\n",
    "conf.set(\"spark.executor.memory\", \"1G\")\n",
    "conf.set(\"spark.executor.cores\", \"2\")\n",
    "\n",
    "# SPARK SESSION\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .config(conf=conf) \\\n",
    "    .config('spark.sql.session.timeZone', 'Europe/Berlin') \\\n",
    "    .appName(appName)\\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "sc=spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "# get the configuration object to check all the configurations the session was startet with\n",
    "for entry in sc.getConf().getAll():\n",
    "        if entry[0] in [\"spark.app.name\",\"spark.kubernetes.namespace\",\"spark.executor.memory\",\"spark.executor.cores\",\"spark.driver.host\",\"spark.master\",\"spark.sql.extensions\"]:\n",
    "            print(entry[0],\"=\",entry[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27495f26",
   "metadata": {},
   "source": [
    "## Create sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1b8b7c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++ create new dataframe and show schema and data\n",
      "################################################\n"
     ]
    }
   ],
   "source": [
    "# initial daten\n",
    "account_data1 = [\n",
    "    (1,\"alex\",\"2019-01-01\",1000),\n",
    "    (2,\"alex\",\"2019-02-01\",1500),\n",
    "    (3,\"alex\",\"2019-03-01\",1700),\n",
    "    (4,\"maria\",\"2020-01-01\",5000)\n",
    "    ]\n",
    "\n",
    "# update mit Änderung und neuem Datensat\n",
    "account_data2 = [\n",
    "    (1,\"alex\",\"2019-03-01\",3300),\n",
    "    (2,\"peter\",\"2021-01-01\",100)\n",
    "    ]\n",
    "\n",
    "# Update mit neuer Spalte\n",
    "account_data3 = [\n",
    "    (1,\"otto\",\"2019-10-01\",4444,\"neue Spalte 1\")\n",
    "]\n",
    "\n",
    "schema = [\"id\",\"account\",\"dt_transaction\",\"balance\"]\n",
    "schema3 = [\"id\",\"account\",\"dt_transaction\",\"balance\",\"new\"]\n",
    "\n",
    "df1 = spark.createDataFrame(data=account_data1, schema = schema).withColumn(\"dt_transaction\",col(\"dt_transaction\").cast(\"date\")).repartition(3)\n",
    "df2 = spark.createDataFrame(data=account_data2, schema = schema).withColumn(\"dt_transaction\",col(\"dt_transaction\").cast(\"date\")).repartition(3)\n",
    "df3 = spark.createDataFrame(data=account_data3, schema = schema3).withColumn(\"dt_transaction\",col(\"dt_transaction\").cast(\"date\")).repartition(3)\n",
    "\n",
    "print(\"++ create new dataframe and show schema and data\")\n",
    "print(\"################################################\")\n",
    "\n",
    "# df1.printSchema()\n",
    "# df1.show(truncate=False)\n",
    "# df3.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379a3b6a",
   "metadata": {},
   "source": [
    "## Configure boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d161c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.client import Config\n",
    "\n",
    "options = {\n",
    "    'endpoint_url': 'http://minio.minio.svc.cluster.local:9000',\n",
    "    'aws_access_key_id': 'trainadm',\n",
    "    'aws_secret_access_key': 'train@thinkport',\n",
    "    'config': Config(signature_version='s3v4'),\n",
    "    'verify': False}\n",
    "\n",
    "s3_resource = boto3.resource('s3', **options)  \n",
    "\n",
    "s3_client = boto3.client('s3', **options)\n",
    "\n",
    "bucket = \"fileformats\"\n",
    "bucket_path=\"s3://\"+bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "272d6e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ls(bucket,prefix):\n",
    "    '''List objects from bucket/prefix'''\n",
    "    try:\n",
    "        for obj in s3_resource.Bucket(bucket).objects.filter(Prefix=prefix):\n",
    "            print(obj.key)\n",
    "    except Exception as e: \n",
    "        print(e)\n",
    "    \n",
    "    \n",
    "def cat(bucket,prefix,binary=False):\n",
    "    '''Show content of one or several files with same prefix/wildcard'''\n",
    "    try:\n",
    "        for obj in s3_resource.Bucket(bucket).objects.filter(Prefix=prefix):\n",
    "            print(\"File:\",obj.key)\n",
    "            print(\"----------------------\")\n",
    "            if binary==True:\n",
    "                print(obj.get()['Body'].read())\n",
    "            else: \n",
    "                print(obj.get()['Body'].read().decode())\n",
    "            print(\"######################\")\n",
    "    except Exception as e: \n",
    "        print(e)\n",
    "            \n",
    "def rm(bucket,prefix):\n",
    "    '''Delete everything from bucket/prefix'''\n",
    "    for object in s3_resource.Bucket(bucket).objects.filter(Prefix=prefix):\n",
    "        print(object.key)\n",
    "        s3_client.delete_object(Bucket=bucket, Key=object.key)\n",
    "    print(f\"Deleted files from {bucket}/{prefix}*\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e07436ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#############################\n",
      "#############################\n",
      "#############################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show everything in bucket\n",
    "ls(bucket,\"\")\n",
    "print(\"#############################\")\n",
    "# show folder\n",
    "ls(bucket,\"csv\")\n",
    "print(\"#############################\")\n",
    "# show subfolder\n",
    "ls(bucket,\"delta/_delta_log/\")\n",
    "print(\"#############################\")\n",
    "print(\"\")\n",
    "# show content of one or several files with same prefix/wildcard\n",
    "cat(bucket,'csv/part')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f96cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rm(\"aleks-test\", \"iceberg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6fe705",
   "metadata": {},
   "source": [
    "## CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "916ad0c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Partitions: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"Number of Partitions:\", df1.rdd.getNumPartitions())\n",
    "\n",
    "write_csv=(df1\n",
    "           .write\n",
    "           .format(\"csv\")\n",
    "           .mode(\"overwrite\") # append\n",
    "           .save(f\"s3://{bucket}/csv\")\n",
    "          )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cebbc9fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv/_SUCCESS\n",
      "csv/part-00000-20873225-f284-4d1d-88b7-5762e628b2c4-c000.csv\n",
      "csv/part-00001-20873225-f284-4d1d-88b7-5762e628b2c4-c000.csv\n",
      "csv/part-00002-20873225-f284-4d1d-88b7-5762e628b2c4-c000.csv\n"
     ]
    }
   ],
   "source": [
    "ls(bucket,\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82fb51b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: csv/part-00000-20873225-f284-4d1d-88b7-5762e628b2c4-c000.csv\n",
      "----------------------\n",
      "1,alex,2019-01-01,1000\n",
      "\n",
      "######################\n",
      "File: csv/part-00001-20873225-f284-4d1d-88b7-5762e628b2c4-c000.csv\n",
      "----------------------\n",
      "2,alex,2019-02-01,1500\n",
      "4,maria,2020-01-01,5000\n",
      "\n",
      "######################\n",
      "File: csv/part-00002-20873225-f284-4d1d-88b7-5762e628b2c4-c000.csv\n",
      "----------------------\n",
      "3,alex,2019-03-01,1700\n",
      "\n",
      "######################\n"
     ]
    }
   ],
   "source": [
    "cat(bucket,\"csv/part\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f31d31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----------+----+\n",
      "|_c0|  _c1|       _c2| _c3|\n",
      "+---+-----+----------+----+\n",
      "|  2| alex|2019-02-01|1500|\n",
      "|  4|maria|2020-01-01|5000|\n",
      "|  1| alex|2019-01-01|1000|\n",
      "|  3| alex|2019-03-01|1700|\n",
      "+---+-----+----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "read_csv=spark.read.format(\"csv\").load(f\"s3://{bucket}/csv\")\n",
    "\n",
    "read_csv.printSchema()\n",
    "read_csv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c5a5979",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "write_csv=(df3\n",
    "           .write\n",
    "           .format(\"csv\")\n",
    "           .mode(\"append\") # append\n",
    "           .save(f\"s3://{bucket}/csv\")\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a771362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv/_SUCCESS\n",
      "csv/part-00000-20873225-f284-4d1d-88b7-5762e628b2c4-c000.csv\n",
      "csv/part-00000-9fd43bb8-0c13-4e00-8324-babfab453bd2-c000.csv\n",
      "csv/part-00001-20873225-f284-4d1d-88b7-5762e628b2c4-c000.csv\n",
      "csv/part-00001-9fd43bb8-0c13-4e00-8324-babfab453bd2-c000.csv\n",
      "csv/part-00002-20873225-f284-4d1d-88b7-5762e628b2c4-c000.csv\n"
     ]
    }
   ],
   "source": [
    "ls(bucket,\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "349331cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: csv/part-00000-20873225-f284-4d1d-88b7-5762e628b2c4-c000.csv\n",
      "----------------------\n",
      "1,alex,2019-01-01,1000\n",
      "\n",
      "######################\n",
      "File: csv/part-00000-9fd43bb8-0c13-4e00-8324-babfab453bd2-c000.csv\n",
      "----------------------\n",
      "\n",
      "######################\n",
      "File: csv/part-00001-20873225-f284-4d1d-88b7-5762e628b2c4-c000.csv\n",
      "----------------------\n",
      "2,alex,2019-02-01,1500\n",
      "4,maria,2020-01-01,5000\n",
      "\n",
      "######################\n",
      "File: csv/part-00001-9fd43bb8-0c13-4e00-8324-babfab453bd2-c000.csv\n",
      "----------------------\n",
      "1,otto,2019-10-01,4444,neue Spalte 1\n",
      "\n",
      "######################\n",
      "File: csv/part-00002-20873225-f284-4d1d-88b7-5762e628b2c4-c000.csv\n",
      "----------------------\n",
      "3,alex,2019-03-01,1700\n",
      "\n",
      "######################\n"
     ]
    }
   ],
   "source": [
    "cat(bucket,\"csv/part\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "31fd06f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      "\n",
      "+---+-----+----------+----+\n",
      "|_c0|  _c1|       _c2| _c3|\n",
      "+---+-----+----------+----+\n",
      "|  2| alex|2019-02-01|1500|\n",
      "|  4|maria|2020-01-01|5000|\n",
      "|  1| otto|2019-10-01|4444|\n",
      "|  1| alex|2019-01-01|1000|\n",
      "|  3| alex|2019-03-01|1700|\n",
      "+---+-----+----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "read_csv=spark.read.format(\"csv\").load(f\"s3://{bucket}/csv\")\n",
    "\n",
    "read_csv.printSchema()\n",
    "read_csv.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1b3038",
   "metadata": {},
   "source": [
    "* kein Schema (Typen)\n",
    "* kein anfügen neuer Spalten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55bcbfa",
   "metadata": {},
   "source": [
    "## JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e368413b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Partitions: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"Number of Partitions:\", df1.rdd.getNumPartitions())\n",
    "\n",
    "write_json=(df1\n",
    "           .write\n",
    "           .format(\"json\")\n",
    "           .mode(\"overwrite\") # append\n",
    "           .save(f\"s3://{bucket}/json\")\n",
    "          )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "da9898f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "json/_SUCCESS\n",
      "json/part-00000-5b8b02be-f698-4cf6-9320-3aa2642f2997-c000.json\n",
      "json/part-00001-5b8b02be-f698-4cf6-9320-3aa2642f2997-c000.json\n",
      "json/part-00002-5b8b02be-f698-4cf6-9320-3aa2642f2997-c000.json\n"
     ]
    }
   ],
   "source": [
    "ls(bucket,\"json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3ba7f5c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: json/part-00000-5b8b02be-f698-4cf6-9320-3aa2642f2997-c000.json\n",
      "----------------------\n",
      "{\"id\":1,\"account\":\"alex\",\"dt_transaction\":\"2019-01-01\",\"balance\":1000}\n",
      "\n",
      "######################\n",
      "File: json/part-00001-5b8b02be-f698-4cf6-9320-3aa2642f2997-c000.json\n",
      "----------------------\n",
      "{\"id\":2,\"account\":\"alex\",\"dt_transaction\":\"2019-02-01\",\"balance\":1500}\n",
      "{\"id\":4,\"account\":\"maria\",\"dt_transaction\":\"2020-01-01\",\"balance\":5000}\n",
      "\n",
      "######################\n",
      "File: json/part-00002-5b8b02be-f698-4cf6-9320-3aa2642f2997-c000.json\n",
      "----------------------\n",
      "{\"id\":3,\"account\":\"alex\",\"dt_transaction\":\"2019-03-01\",\"balance\":1700}\n",
      "\n",
      "######################\n"
     ]
    }
   ],
   "source": [
    "cat(bucket,\"json/part\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7506f71a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "write_csv=(df3\n",
    "           .write\n",
    "           .format(\"json\")\n",
    "           .mode(\"append\") # append\n",
    "           .save(f\"s3://{bucket}/json\")\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ba940d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- account: string (nullable = true)\n",
      " |-- balance: long (nullable = true)\n",
      " |-- dt_transaction: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- new: string (nullable = true)\n",
      "\n",
      "+-------+-------+--------------+---+-------------+\n",
      "|account|balance|dt_transaction| id|          new|\n",
      "+-------+-------+--------------+---+-------------+\n",
      "|   alex|   1500|    2019-02-01|  2|         null|\n",
      "|  maria|   5000|    2020-01-01|  4|         null|\n",
      "|   otto|   4444|    2019-10-01|  1|neue Spalte 1|\n",
      "|   alex|   1000|    2019-01-01|  1|         null|\n",
      "|   alex|   1700|    2019-03-01|  3|         null|\n",
      "+-------+-------+--------------+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "read_json=spark.read.format(\"json\").load(f\"s3://{bucket}/json\")\n",
    "\n",
    "read_json.printSchema()\n",
    "read_json.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad580792",
   "metadata": {},
   "source": [
    "* Kein Schema\n",
    "* Neue Spalten werden als neues Attribut hinzugefügt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094e8bb0",
   "metadata": {},
   "source": [
    "## Avro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "44777375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Partitions: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"Number of Partitions:\", df1.rdd.getNumPartitions())\n",
    "\n",
    "write_avro=(df1\n",
    "           .write\n",
    "           .format(\"avro\")\n",
    "           .mode(\"overwrite\") # append\n",
    "           .save(f\"s3://{bucket}/avro\")\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a7ebc8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avro/_SUCCESS\n",
      "avro/part-00000-66653003-5264-4bf8-b5c9-64ab10b3c708-c000.avro\n",
      "avro/part-00001-66653003-5264-4bf8-b5c9-64ab10b3c708-c000.avro\n",
      "avro/part-00002-66653003-5264-4bf8-b5c9-64ab10b3c708-c000.avro\n"
     ]
    }
   ],
   "source": [
    "ls(bucket,\"avro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d82f2e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: avro/part-00000-66653003-5264-4bf8-b5c9-64ab10b3c708-c000.avro\n",
      "----------------------\n",
      "b'Obj\\x01\\x06\\x16avro.schema\\xfa\\x03{\"type\":\"record\",\"name\":\"topLevelRecord\",\"fields\":[{\"name\":\"id\",\"type\":[\"long\",\"null\"]},{\"name\":\"account\",\"type\":[\"string\",\"null\"]},{\"name\":\"dt_transaction\",\"type\":[{\"type\":\"int\",\"logicalType\":\"date\"},\"null\"]},{\"name\":\"balance\",\"type\":[\"long\",\"null\"]}]}0org.apache.spark.version\\n3.3.1\\x14avro.codec\\x0csnappy\\x00\\x0b\\xb8\\xac7\\xef\\x1c}i|\\xf7\\x7fi\\x0eA\\xbe\\\\\\x02*\\x0f8\\x00\\x02\\x00\\x08alex\\x00\\xd2\\x97\\x02\\x00\\xd0\\x0f\\x84/\\x04]\\x0b\\xb8\\xac7\\xef\\x1c}i|\\xf7\\x7fi\\x0eA\\xbe\\\\'\n",
      "######################\n",
      "File: avro/part-00001-66653003-5264-4bf8-b5c9-64ab10b3c708-c000.avro\n",
      "----------------------\n",
      "b'Obj\\x01\\x06\\x16avro.schema\\xfa\\x03{\"type\":\"record\",\"name\":\"topLevelRecord\",\"fields\":[{\"name\":\"id\",\"type\":[\"long\",\"null\"]},{\"name\":\"account\",\"type\":[\"string\",\"null\"]},{\"name\":\"dt_transaction\",\"type\":[{\"type\":\"int\",\"logicalType\":\"date\"},\"null\"]},{\"name\":\"balance\",\"type\":[\"long\",\"null\"]}]}0org.apache.spark.version\\n3.3.1\\x14avro.codec\\x0csnappy\\x00\\xe1\\xda\\xca\\xf3\\xf8\\xb2\\x95\\x12\\x81\\xf4\\xf2\\xeb\\xccpk\\xd8\\x04J\\x1fx\\x00\\x04\\x00\\x08alex\\x00\\x90\\x98\\x02\\x00\\xb8\\x17\\x00\\x08\\x00\\nmaria\\x00\\xac\\x9d\\x02\\x00\\x90N\\xbdM\\xe8\\xb0\\xe1\\xda\\xca\\xf3\\xf8\\xb2\\x95\\x12\\x81\\xf4\\xf2\\xeb\\xccpk\\xd8'\n",
      "######################\n",
      "File: avro/part-00002-66653003-5264-4bf8-b5c9-64ab10b3c708-c000.avro\n",
      "----------------------\n",
      "b'Obj\\x01\\x06\\x16avro.schema\\xfa\\x03{\"type\":\"record\",\"name\":\"topLevelRecord\",\"fields\":[{\"name\":\"id\",\"type\":[\"long\",\"null\"]},{\"name\":\"account\",\"type\":[\"string\",\"null\"]},{\"name\":\"dt_transaction\",\"type\":[{\"type\":\"int\",\"logicalType\":\"date\"},\"null\"]},{\"name\":\"balance\",\"type\":[\"long\",\"null\"]}]}0org.apache.spark.version\\n3.3.1\\x14avro.codec\\x0csnappy\\x00\\xb2\\x14JS\\x14\\x8c\\x05\\x93:\\x98\\xdf\\x8d\\xcf\\xbc\\x18t\\x02*\\x0f8\\x00\\x06\\x00\\x08alex\\x00\\xc8\\x98\\x02\\x00\\xc8\\x1aRXA\\xd8\\xb2\\x14JS\\x14\\x8c\\x05\\x93:\\x98\\xdf\\x8d\\xcf\\xbc\\x18t'\n",
      "######################\n"
     ]
    }
   ],
   "source": [
    "cat(bucket,\"avro/part\",True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8ac72f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- account: string (nullable = true)\n",
      " |-- dt_transaction: date (nullable = true)\n",
      " |-- balance: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 25:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------+-------+\n",
      "| id|account|dt_transaction|balance|\n",
      "+---+-------+--------------+-------+\n",
      "|  2|   alex|    2019-02-01|   1500|\n",
      "|  4|  maria|    2020-01-01|   5000|\n",
      "|  1|   alex|    2019-01-01|   1000|\n",
      "|  3|   alex|    2019-03-01|   1700|\n",
      "+---+-------+--------------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "read_json=spark.read.format(\"avro\").load(f\"s3://{bucket}/avro\")\n",
    "read_json.printSchema()\n",
    "read_json.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1b1e0ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "write_avro=(df3\n",
    "           .write\n",
    "           .format(\"avro\")\n",
    "           .mode(\"append\") # append\n",
    "           .save(f\"s3://{bucket}/avro\")\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "70570799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- account: string (nullable = true)\n",
      " |-- dt_transaction: date (nullable = true)\n",
      " |-- balance: long (nullable = true)\n",
      " |-- new: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------+-------+-------------+\n",
      "| id|account|dt_transaction|balance|          new|\n",
      "+---+-------+--------------+-------+-------------+\n",
      "|  1|   otto|    2019-10-01|   4444|neue Spalte 1|\n",
      "|  2|   alex|    2019-02-01|   1500|         null|\n",
      "|  4|  maria|    2020-01-01|   5000|         null|\n",
      "|  1|   alex|    2019-01-01|   1000|         null|\n",
      "|  3|   alex|    2019-03-01|   1700|         null|\n",
      "+---+-------+--------------+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "read_json=spark.read.format(\"avro\").load(f\"s3://{bucket}/avro\")\n",
    "read_json.printSchema()\n",
    "read_json.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b1054d",
   "metadata": {},
   "source": [
    "* Schema erhalten\n",
    "* Schema evolution "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45de295",
   "metadata": {},
   "source": [
    "## Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fb546000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Partitions: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"Number of Partitions:\", df1.rdd.getNumPartitions())\n",
    "\n",
    "write_parquet=(df1\n",
    "           .write\n",
    "           .partitionBy(\"account\")\n",
    "           .format(\"parquet\")\n",
    "           .mode(\"overwrite\") # append\n",
    "           .save(f\"s3://{bucket}/parquet\")\n",
    "          )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6c9e08a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parquet/_SUCCESS\n",
      "parquet/account=alex/part-00000-2ffc4cd5-064e-491f-bd92-117792787ed3.c000.snappy.parquet\n",
      "parquet/account=alex/part-00001-2ffc4cd5-064e-491f-bd92-117792787ed3.c000.snappy.parquet\n",
      "parquet/account=alex/part-00002-2ffc4cd5-064e-491f-bd92-117792787ed3.c000.snappy.parquet\n",
      "parquet/account=maria/part-00001-2ffc4cd5-064e-491f-bd92-117792787ed3.c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "ls(bucket,\"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "49b6d692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: parquet/account=maria/part-00001-2ffc4cd5-064e-491f-bd92-117792787ed3.c000.snappy.parquet\n",
      "----------------------\n",
      "b'PAR1\\x15\\x00\\x15\\x1c\\x15 \\x15\\xb6\\xf1\\xd0\\xb8\\x02\\x1c\\x15\\x02\\x15\\x00\\x15\\x06\\x15\\x08\\x00\\x00\\x0e4\\x02\\x00\\x00\\x00\\x03\\x01\\x04\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x15\\x00\\x15\\x14\\x15\\x18\\x15\\xf9\\xb6\\xd3\\xce\\x0f\\x1c\\x15\\x02\\x15\\x00\\x15\\x06\\x15\\x08\\x00\\x00\\n$\\x02\\x00\\x00\\x00\\x03\\x01VG\\x00\\x00\\x15\\x00\\x15\\x1c\\x15 \\x15\\xba\\xaa\\xc0\\x92\\x08\\x1c\\x15\\x02\\x15\\x00\\x15\\x06\\x15\\x08\\x00\\x00\\x0e4\\x02\\x00\\x00\\x00\\x03\\x01\\x88\\x13\\x00\\x00\\x00\\x00\\x00\\x00\\x19\\x11\\x02\\x19\\x18\\x08\\x04\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x19\\x18\\x08\\x04\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x15\\x02\\x19\\x16\\x00\\x00\\x19\\x11\\x02\\x19\\x18\\x04VG\\x00\\x00\\x19\\x18\\x04VG\\x00\\x00\\x15\\x02\\x19\\x16\\x00\\x00\\x19\\x11\\x02\\x19\\x18\\x08\\x88\\x13\\x00\\x00\\x00\\x00\\x00\\x00\\x19\\x18\\x08\\x88\\x13\\x00\\x00\\x00\\x00\\x00\\x00\\x15\\x02\\x19\\x16\\x00\\x00\\x19\\x1c\\x16\\x08\\x15N\\x16\\x00\\x00\\x00\\x19\\x1c\\x16V\\x15F\\x16\\x00\\x00\\x00\\x19\\x1c\\x16\\x9c\\x01\\x15N\\x16\\x00\\x00\\x00\\x15\\x02\\x19LH\\x0cspark_schema\\x15\\x06\\x00\\x15\\x04%\\x02\\x18\\x02id\\x00\\x15\\x02%\\x02\\x18\\x0edt_transaction%\\x0cLl\\x00\\x00\\x00\\x15\\x04%\\x02\\x18\\x07balance\\x00\\x16\\x02\\x19\\x1c\\x19<&\\x08\\x1c\\x15\\x04\\x195\\x08\\x06\\x00\\x19\\x18\\x02id\\x15\\x02\\x16\\x02\\x16J\\x16N&\\x08<\\x18\\x08\\x04\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x18\\x08\\x04\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x16\\x00(\\x08\\x04\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x18\\x08\\x04\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x19\\x1c\\x15\\x00\\x15\\x00\\x15\\x02\\x00\\x00\\x16\\x94\\x03\\x15\\x14\\x16\\xea\\x01\\x15>\\x00&V\\x1c\\x15\\x02\\x195\\x08\\x06\\x00\\x19\\x18\\x0edt_transaction\\x15\\x02\\x16\\x02\\x16B\\x16F&V<\\x18\\x04VG\\x00\\x00\\x18\\x04VG\\x00\\x00\\x16\\x00(\\x04VG\\x00\\x00\\x18\\x04VG\\x00\\x00\\x00\\x19\\x1c\\x15\\x00\\x15\\x00\\x15\\x02\\x00\\x00\\x16\\xa8\\x03\\x15\\x14\\x16\\xa8\\x02\\x15.\\x00&\\x9c\\x01\\x1c\\x15\\x04\\x195\\x08\\x06\\x00\\x19\\x18\\x07balance\\x15\\x02\\x16\\x02\\x16J\\x16N&\\x9c\\x01<\\x18\\x08\\x88\\x13\\x00\\x00\\x00\\x00\\x00\\x00\\x18\\x08\\x88\\x13\\x00\\x00\\x00\\x00\\x00\\x00\\x16\\x00(\\x08\\x88\\x13\\x00\\x00\\x00\\x00\\x00\\x00\\x18\\x08\\x88\\x13\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x19\\x1c\\x15\\x00\\x15\\x00\\x15\\x02\\x00\\x00\\x16\\xbc\\x03\\x15\\x16\\x16\\xd6\\x02\\x15>\\x00\\x16\\xd6\\x01\\x16\\x02&\\x08\\x16\\xe2\\x01\\x14\\x00\\x00\\x19,\\x18\\x18org.apache.spark.version\\x18\\x053.3.1\\x00\\x18)org.apache.spark.sql.parquet.row.metadata\\x18\\xdb\\x01{\"type\":\"struct\",\"fields\":[{\"name\":\"id\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"dt_transaction\",\"type\":\"date\",\"nullable\":true,\"metadata\":{}},{\"name\":\"balance\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}}]}\\x00\\x18Jparquet-mr version 1.12.2 (build 77e30c8093386ec52c3cfa6c34b7ef3321322c94)\\x19<\\x1c\\x00\\x00\\x1c\\x00\\x00\\x1c\\x00\\x00\\x00\\xf1\\x02\\x00\\x00PAR1'\n",
      "######################\n"
     ]
    }
   ],
   "source": [
    "cat(bucket,\"parquet/account=maria\",True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e00a1e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- dt_transaction: date (nullable = true)\n",
      " |-- balance: long (nullable = true)\n",
      " |-- account: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+-------+-------+\n",
      "| id|dt_transaction|balance|account|\n",
      "+---+--------------+-------+-------+\n",
      "|  1|    2019-01-01|   1000|   alex|\n",
      "|  2|    2019-02-01|   1500|   alex|\n",
      "|  3|    2019-03-01|   1700|   alex|\n",
      "+---+--------------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "read_parquet=(spark\n",
    "              .read.format(\"parquet\")\n",
    "              .load(f\"s3://{bucket}/parquet\")\n",
    "              .filter(col(\"account\")==\"alex\")\n",
    "             )\n",
    "\n",
    "read_parquet.printSchema()\n",
    "read_parquet.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eadd53c",
   "metadata": {},
   "source": [
    "## Delta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7d3e7e",
   "metadata": {},
   "source": [
    "- a **storage layer** that runs on top of existing data lakes\n",
    "- supports ACID transactions and data versioning\n",
    "- allows data lineage tracking\n",
    "- provides optimization for streaming workloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "513d213c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "write_delta=(df1\n",
    "           .write\n",
    "           .format(\"delta\")\n",
    "           .option(\"mergeSchema\", \"false\")\n",
    "           .mode(\"overwrite\") # append\n",
    "           .save(f\"s3://{bucket}/delta\")\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "70cd6f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta/_delta_log/00000000000000000000.json\n",
      "delta/part-00000-71e8256a-4ae9-4a30-82df-1df4300074fa-c000.snappy.parquet\n",
      "delta/part-00001-740db38d-df85-4d5f-b675-785b8e5a1ff6-c000.snappy.parquet\n",
      "delta/part-00002-751ee4f7-7ea7-4731-9df7-9a881d3a97ac-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "ls(bucket,\"delta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2da732ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta/_delta_log/00000000000000000000.json\n"
     ]
    }
   ],
   "source": [
    "ls(bucket,\"delta/_delta_log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0c3dbb33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: delta/_delta_log/00000000000000000000.json\n",
      "----------------------\n",
      "{\"protocol\":{\"minReaderVersion\":1,\"minWriterVersion\":2}}\n",
      "{\"metaData\":{\"id\":\"cc178cae-6af7-410b-9996-f81ccdeb1ab5\",\"format\":{\"provider\":\"parquet\",\"options\":{}},\"schemaString\":\"{\\\"type\\\":\\\"struct\\\",\\\"fields\\\":[{\\\"name\\\":\\\"id\\\",\\\"type\\\":\\\"long\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"account\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"dt_transaction\\\",\\\"type\\\":\\\"date\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"balance\\\",\\\"type\\\":\\\"long\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}}]}\",\"partitionColumns\":[],\"configuration\":{},\"createdTime\":1684095467225}}\n",
      "{\"add\":{\"path\":\"part-00000-71e8256a-4ae9-4a30-82df-1df4300074fa-c000.snappy.parquet\",\"partitionValues\":{},\"size\":1236,\"modificationTime\":1684095469000,\"dataChange\":true,\"stats\":\"{\\\"numRecords\\\":1,\\\"minValues\\\":{\\\"id\\\":1,\\\"account\\\":\\\"alex\\\",\\\"dt_transaction\\\":\\\"2019-01-01\\\",\\\"balance\\\":1000},\\\"maxValues\\\":{\\\"id\\\":1,\\\"account\\\":\\\"alex\\\",\\\"dt_transaction\\\":\\\"2019-01-01\\\",\\\"balance\\\":1000},\\\"nullCount\\\":{\\\"id\\\":0,\\\"account\\\":0,\\\"dt_transaction\\\":0,\\\"balance\\\":0}}\"}}\n",
      "{\"add\":{\"path\":\"part-00001-740db38d-df85-4d5f-b675-785b8e5a1ff6-c000.snappy.parquet\",\"partitionValues\":{},\"size\":1254,\"modificationTime\":1684095469000,\"dataChange\":true,\"stats\":\"{\\\"numRecords\\\":2,\\\"minValues\\\":{\\\"id\\\":2,\\\"account\\\":\\\"alex\\\",\\\"dt_transaction\\\":\\\"2019-02-01\\\",\\\"balance\\\":1500},\\\"maxValues\\\":{\\\"id\\\":4,\\\"account\\\":\\\"maria\\\",\\\"dt_transaction\\\":\\\"2020-01-01\\\",\\\"balance\\\":5000},\\\"nullCount\\\":{\\\"id\\\":0,\\\"account\\\":0,\\\"dt_transaction\\\":0,\\\"balance\\\":0}}\"}}\n",
      "{\"add\":{\"path\":\"part-00002-751ee4f7-7ea7-4731-9df7-9a881d3a97ac-c000.snappy.parquet\",\"partitionValues\":{},\"size\":1236,\"modificationTime\":1684095473000,\"dataChange\":true,\"stats\":\"{\\\"numRecords\\\":1,\\\"minValues\\\":{\\\"id\\\":3,\\\"account\\\":\\\"alex\\\",\\\"dt_transaction\\\":\\\"2019-03-01\\\",\\\"balance\\\":1700},\\\"maxValues\\\":{\\\"id\\\":3,\\\"account\\\":\\\"alex\\\",\\\"dt_transaction\\\":\\\"2019-03-01\\\",\\\"balance\\\":1700},\\\"nullCount\\\":{\\\"id\\\":0,\\\"account\\\":0,\\\"dt_transaction\\\":0,\\\"balance\\\":0}}\"}}\n",
      "{\"commitInfo\":{\"timestamp\":1684095478099,\"operation\":\"WRITE\",\"operationParameters\":{\"mode\":\"Overwrite\",\"partitionBy\":\"[]\"},\"isolationLevel\":\"Serializable\",\"isBlindAppend\":false,\"operationMetrics\":{\"numFiles\":\"3\",\"numOutputRows\":\"4\",\"numOutputBytes\":\"3726\"},\"engineInfo\":\"Apache-Spark/3.3.1 Delta-Lake/2.1.1\",\"txnId\":\"e821496b-88e7-46d6-a713-b91a791957ed\"}}\n",
      "\n",
      "######################\n"
     ]
    }
   ],
   "source": [
    "cat(bucket,\"delta/_delta_log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a714c074",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "write_delta=(df2\n",
    "           .write\n",
    "           .format(\"delta\")\n",
    "           .mode(\"append\") # append\n",
    "           .save(f\"s3://{bucket}/delta\")\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e8d72dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "write_delta=(df3\n",
    "           .write\n",
    "           .format(\"delta\")\n",
    "           .option(\"mergeSchema\", \"true\")\n",
    "           .mode(\"overwrite\") # append\n",
    "           .save(f\"s3://{bucket}/delta\")\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a5bf92d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: delta/_delta_log/00000000000000000000.json\n",
      "----------------------\n",
      "{\"protocol\":{\"minReaderVersion\":1,\"minWriterVersion\":2}}\n",
      "{\"metaData\":{\"id\":\"cc178cae-6af7-410b-9996-f81ccdeb1ab5\",\"format\":{\"provider\":\"parquet\",\"options\":{}},\"schemaString\":\"{\\\"type\\\":\\\"struct\\\",\\\"fields\\\":[{\\\"name\\\":\\\"id\\\",\\\"type\\\":\\\"long\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"account\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"dt_transaction\\\",\\\"type\\\":\\\"date\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"balance\\\",\\\"type\\\":\\\"long\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}}]}\",\"partitionColumns\":[],\"configuration\":{},\"createdTime\":1684095467225}}\n",
      "{\"add\":{\"path\":\"part-00000-71e8256a-4ae9-4a30-82df-1df4300074fa-c000.snappy.parquet\",\"partitionValues\":{},\"size\":1236,\"modificationTime\":1684095469000,\"dataChange\":true,\"stats\":\"{\\\"numRecords\\\":1,\\\"minValues\\\":{\\\"id\\\":1,\\\"account\\\":\\\"alex\\\",\\\"dt_transaction\\\":\\\"2019-01-01\\\",\\\"balance\\\":1000},\\\"maxValues\\\":{\\\"id\\\":1,\\\"account\\\":\\\"alex\\\",\\\"dt_transaction\\\":\\\"2019-01-01\\\",\\\"balance\\\":1000},\\\"nullCount\\\":{\\\"id\\\":0,\\\"account\\\":0,\\\"dt_transaction\\\":0,\\\"balance\\\":0}}\"}}\n",
      "{\"add\":{\"path\":\"part-00001-740db38d-df85-4d5f-b675-785b8e5a1ff6-c000.snappy.parquet\",\"partitionValues\":{},\"size\":1254,\"modificationTime\":1684095469000,\"dataChange\":true,\"stats\":\"{\\\"numRecords\\\":2,\\\"minValues\\\":{\\\"id\\\":2,\\\"account\\\":\\\"alex\\\",\\\"dt_transaction\\\":\\\"2019-02-01\\\",\\\"balance\\\":1500},\\\"maxValues\\\":{\\\"id\\\":4,\\\"account\\\":\\\"maria\\\",\\\"dt_transaction\\\":\\\"2020-01-01\\\",\\\"balance\\\":5000},\\\"nullCount\\\":{\\\"id\\\":0,\\\"account\\\":0,\\\"dt_transaction\\\":0,\\\"balance\\\":0}}\"}}\n",
      "{\"add\":{\"path\":\"part-00002-751ee4f7-7ea7-4731-9df7-9a881d3a97ac-c000.snappy.parquet\",\"partitionValues\":{},\"size\":1236,\"modificationTime\":1684095473000,\"dataChange\":true,\"stats\":\"{\\\"numRecords\\\":1,\\\"minValues\\\":{\\\"id\\\":3,\\\"account\\\":\\\"alex\\\",\\\"dt_transaction\\\":\\\"2019-03-01\\\",\\\"balance\\\":1700},\\\"maxValues\\\":{\\\"id\\\":3,\\\"account\\\":\\\"alex\\\",\\\"dt_transaction\\\":\\\"2019-03-01\\\",\\\"balance\\\":1700},\\\"nullCount\\\":{\\\"id\\\":0,\\\"account\\\":0,\\\"dt_transaction\\\":0,\\\"balance\\\":0}}\"}}\n",
      "{\"commitInfo\":{\"timestamp\":1684095478099,\"operation\":\"WRITE\",\"operationParameters\":{\"mode\":\"Overwrite\",\"partitionBy\":\"[]\"},\"isolationLevel\":\"Serializable\",\"isBlindAppend\":false,\"operationMetrics\":{\"numFiles\":\"3\",\"numOutputRows\":\"4\",\"numOutputBytes\":\"3726\"},\"engineInfo\":\"Apache-Spark/3.3.1 Delta-Lake/2.1.1\",\"txnId\":\"e821496b-88e7-46d6-a713-b91a791957ed\"}}\n",
      "\n",
      "######################\n",
      "File: delta/_delta_log/00000000000000000001.json\n",
      "----------------------\n",
      "{\"add\":{\"path\":\"part-00000-39f4877c-64c1-4ec1-a81d-b7924249759c-c000.snappy.parquet\",\"partitionValues\":{},\"size\":1236,\"modificationTime\":1684095494000,\"dataChange\":true,\"stats\":\"{\\\"numRecords\\\":1,\\\"minValues\\\":{\\\"id\\\":1,\\\"account\\\":\\\"alex\\\",\\\"dt_transaction\\\":\\\"2019-03-01\\\",\\\"balance\\\":3300},\\\"maxValues\\\":{\\\"id\\\":1,\\\"account\\\":\\\"alex\\\",\\\"dt_transaction\\\":\\\"2019-03-01\\\",\\\"balance\\\":3300},\\\"nullCount\\\":{\\\"id\\\":0,\\\"account\\\":0,\\\"dt_transaction\\\":0,\\\"balance\\\":0}}\"}}\n",
      "{\"add\":{\"path\":\"part-00001-c954c2a6-bea5-4699-a511-c29ed8e137bd-c000.snappy.parquet\",\"partitionValues\":{},\"size\":1243,\"modificationTime\":1684095494000,\"dataChange\":true,\"stats\":\"{\\\"numRecords\\\":1,\\\"minValues\\\":{\\\"id\\\":2,\\\"account\\\":\\\"peter\\\",\\\"dt_transaction\\\":\\\"2021-01-01\\\",\\\"balance\\\":100},\\\"maxValues\\\":{\\\"id\\\":2,\\\"account\\\":\\\"peter\\\",\\\"dt_transaction\\\":\\\"2021-01-01\\\",\\\"balance\\\":100},\\\"nullCount\\\":{\\\"id\\\":0,\\\"account\\\":0,\\\"dt_transaction\\\":0,\\\"balance\\\":0}}\"}}\n",
      "{\"commitInfo\":{\"timestamp\":1684095494957,\"operation\":\"WRITE\",\"operationParameters\":{\"mode\":\"Append\",\"partitionBy\":\"[]\"},\"readVersion\":0,\"isolationLevel\":\"Serializable\",\"isBlindAppend\":true,\"operationMetrics\":{\"numFiles\":\"2\",\"numOutputRows\":\"2\",\"numOutputBytes\":\"2479\"},\"engineInfo\":\"Apache-Spark/3.3.1 Delta-Lake/2.1.1\",\"txnId\":\"956ada13-39ae-4266-b9c5-41fa5d356f19\"}}\n",
      "\n",
      "######################\n",
      "File: delta/_delta_log/00000000000000000002.json\n",
      "----------------------\n",
      "{\"metaData\":{\"id\":\"cc178cae-6af7-410b-9996-f81ccdeb1ab5\",\"format\":{\"provider\":\"parquet\",\"options\":{}},\"schemaString\":\"{\\\"type\\\":\\\"struct\\\",\\\"fields\\\":[{\\\"name\\\":\\\"id\\\",\\\"type\\\":\\\"long\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"account\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"dt_transaction\\\",\\\"type\\\":\\\"date\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"balance\\\",\\\"type\\\":\\\"long\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"new\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}}]}\",\"partitionColumns\":[],\"configuration\":{},\"createdTime\":1684095467225}}\n",
      "{\"add\":{\"path\":\"part-00000-969db887-9986-4fe3-8b20-88ec19a75f2b-c000.snappy.parquet\",\"partitionValues\":{},\"size\":645,\"modificationTime\":1684095504000,\"dataChange\":true,\"stats\":\"{\\\"numRecords\\\":0,\\\"minValues\\\":{},\\\"maxValues\\\":{},\\\"nullCount\\\":{}}\"}}\n",
      "{\"add\":{\"path\":\"part-00001-88057ecc-89d7-4ca1-b1ee-c48f28e66b65-c000.snappy.parquet\",\"partitionValues\":{},\"size\":1528,\"modificationTime\":1684095504000,\"dataChange\":true,\"stats\":\"{\\\"numRecords\\\":1,\\\"minValues\\\":{\\\"id\\\":1,\\\"account\\\":\\\"otto\\\",\\\"dt_transaction\\\":\\\"2019-10-01\\\",\\\"balance\\\":4444,\\\"new\\\":\\\"neue Spalte 1\\\"},\\\"maxValues\\\":{\\\"id\\\":1,\\\"account\\\":\\\"otto\\\",\\\"dt_transaction\\\":\\\"2019-10-01\\\",\\\"balance\\\":4444,\\\"new\\\":\\\"neue Spalte 1\\\"},\\\"nullCount\\\":{\\\"id\\\":0,\\\"account\\\":0,\\\"dt_transaction\\\":0,\\\"balance\\\":0,\\\"new\\\":0}}\"}}\n",
      "{\"remove\":{\"path\":\"part-00001-740db38d-df85-4d5f-b675-785b8e5a1ff6-c000.snappy.parquet\",\"deletionTimestamp\":1684095507259,\"dataChange\":true,\"extendedFileMetadata\":true,\"partitionValues\":{},\"size\":1254}}\n",
      "{\"remove\":{\"path\":\"part-00002-751ee4f7-7ea7-4731-9df7-9a881d3a97ac-c000.snappy.parquet\",\"deletionTimestamp\":1684095507259,\"dataChange\":true,\"extendedFileMetadata\":true,\"partitionValues\":{},\"size\":1236}}\n",
      "{\"remove\":{\"path\":\"part-00001-c954c2a6-bea5-4699-a511-c29ed8e137bd-c000.snappy.parquet\",\"deletionTimestamp\":1684095507259,\"dataChange\":true,\"extendedFileMetadata\":true,\"partitionValues\":{},\"size\":1243}}\n",
      "{\"remove\":{\"path\":\"part-00000-39f4877c-64c1-4ec1-a81d-b7924249759c-c000.snappy.parquet\",\"deletionTimestamp\":1684095507259,\"dataChange\":true,\"extendedFileMetadata\":true,\"partitionValues\":{},\"size\":1236}}\n",
      "{\"remove\":{\"path\":\"part-00000-71e8256a-4ae9-4a30-82df-1df4300074fa-c000.snappy.parquet\",\"deletionTimestamp\":1684095507259,\"dataChange\":true,\"extendedFileMetadata\":true,\"partitionValues\":{},\"size\":1236}}\n",
      "{\"commitInfo\":{\"timestamp\":1684095507260,\"operation\":\"WRITE\",\"operationParameters\":{\"mode\":\"Overwrite\",\"partitionBy\":\"[]\"},\"readVersion\":1,\"isolationLevel\":\"Serializable\",\"isBlindAppend\":false,\"operationMetrics\":{\"numFiles\":\"2\",\"numOutputRows\":\"1\",\"numOutputBytes\":\"2173\"},\"engineInfo\":\"Apache-Spark/3.3.1 Delta-Lake/2.1.1\",\"txnId\":\"36d921df-377b-4cae-a280-1116b52a381e\"}}\n",
      "\n",
      "######################\n"
     ]
    }
   ],
   "source": [
    "cat(bucket,\"delta/_delta_log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "54d86582",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "delta.tables.DeltaTable"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DeltaTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7cb3480c",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling z:io.delta.tables.DeltaTable.forPath. Trace:\npy4j.Py4JException: Method forPath([class org.apache.spark.sql.SparkSession, class java.lang.String, class java.util.HashMap]) does not exist\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:339)\n\tat py4j.Gateway.invoke(Gateway.java:276)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Unknown Source)\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m deltaTable \u001b[38;5;241m=\u001b[39m \u001b[43mDeltaTable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspark\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ms3://\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mbucket\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/delta\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# --> Vermutlich falsche Delta Version zu Spark\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#fullHistoryDF = deltaTable.history()    # get the full history of the table\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/delta/tables.py:387\u001b[0m, in \u001b[0;36mDeltaTable.forPath\u001b[0;34m(cls, sparkSession, path, hadoopConf)\u001b[0m\n\u001b[1;32m    384\u001b[0m jvm: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJVMView\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m sparkSession\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    385\u001b[0m jsparkSession: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJavaObject\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m sparkSession\u001b[38;5;241m.\u001b[39m_jsparkSession  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m--> 387\u001b[0m jdt \u001b[38;5;241m=\u001b[39m \u001b[43mjvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdelta\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtables\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDeltaTable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjsparkSession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhadoopConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DeltaTable(sparkSession, jdt)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/py4j/protocol.py:330\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 330\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    334\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    335\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    336\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name))\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling z:io.delta.tables.DeltaTable.forPath. Trace:\npy4j.Py4JException: Method forPath([class org.apache.spark.sql.SparkSession, class java.lang.String, class java.util.HashMap]) does not exist\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:339)\n\tat py4j.Gateway.invoke(Gateway.java:276)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Unknown Source)\n\n"
     ]
    }
   ],
   "source": [
    "deltaTable = DeltaTable.forPath(spark, f\"s3://{bucket}/delta\")\n",
    "# --> Vermutlich falsche Delta Version zu Spark\n",
    "#fullHistoryDF = deltaTable.history()    # get the full history of the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac186df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fullHistoryDF.select(\"version\",\"readVersion\",\"timestamp\",\"userId\",\"operation\",\"operationParameters\",\"operationMetrics\",\"userMetadata\").show(truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "49f89cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------+-------+-------------+\n",
      "| id|account|dt_transaction|balance|          new|\n",
      "+---+-------+--------------+-------+-------------+\n",
      "|  1|   otto|    2019-10-01|   4444|neue Spalte 1|\n",
      "+---+-------+--------------+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"delta\").load(f\"s3://{bucket}/delta\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc61efc",
   "metadata": {},
   "source": [
    "## Delta: Time travel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6617afc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------+-------+\n",
      "| id|account|dt_transaction|balance|\n",
      "+---+-------+--------------+-------+\n",
      "|  2|   alex|    2019-02-01|   1500|\n",
      "|  4|  maria|    2020-01-01|   5000|\n",
      "|  2|  peter|    2021-01-01|    100|\n",
      "|  3|   alex|    2019-03-01|   1700|\n",
      "|  1|   alex|    2019-03-01|   3300|\n",
      "|  1|   alex|    2019-01-01|   1000|\n",
      "+---+-------+--------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"delta\").option(\"versionAsOf\", \"1\").load(f\"s3://{bucket}/delta\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55d249d",
   "metadata": {},
   "source": [
    "## Delta: Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2aea3fb0",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling z:io.delta.tables.DeltaTable.forPath. Trace:\npy4j.Py4JException: Method forPath([class org.apache.spark.sql.SparkSession, class java.lang.String, class java.util.HashMap]) does not exist\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:339)\n\tat py4j.Gateway.invoke(Gateway.java:276)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Unknown Source)\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m deltaTable2 \u001b[38;5;241m=\u001b[39m \u001b[43mDeltaTable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspark\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ms3://\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mbucket\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/delta\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m df2a\u001b[38;5;241m=\u001b[39mdf2\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnew\u001b[39m\u001b[38;5;124m\"\u001b[39m,lit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      5\u001b[0m df2a\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/delta/tables.py:387\u001b[0m, in \u001b[0;36mDeltaTable.forPath\u001b[0;34m(cls, sparkSession, path, hadoopConf)\u001b[0m\n\u001b[1;32m    384\u001b[0m jvm: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJVMView\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m sparkSession\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    385\u001b[0m jsparkSession: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJavaObject\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m sparkSession\u001b[38;5;241m.\u001b[39m_jsparkSession  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m--> 387\u001b[0m jdt \u001b[38;5;241m=\u001b[39m \u001b[43mjvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdelta\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtables\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDeltaTable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjsparkSession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhadoopConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DeltaTable(sparkSession, jdt)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/py4j/protocol.py:330\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 330\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    334\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    335\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    336\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name))\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling z:io.delta.tables.DeltaTable.forPath. Trace:\npy4j.Py4JException: Method forPath([class org.apache.spark.sql.SparkSession, class java.lang.String, class java.util.HashMap]) does not exist\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:339)\n\tat py4j.Gateway.invoke(Gateway.java:276)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Unknown Source)\n\n"
     ]
    }
   ],
   "source": [
    "deltaTable2 = DeltaTable.forPath(spark, f\"s3://{bucket}/delta\")\n",
    "\n",
    "\n",
    "df2a=df2.withColumn(\"new\",lit(\"test\"))\n",
    "df2a.show()\n",
    "deltaTable2.toDF().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e1688108",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'deltaTable2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dt3\u001b[38;5;241m=\u001b[39m(\u001b[43mdeltaTable2\u001b[49m\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moldData\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m       \u001b[38;5;241m.\u001b[39mmerge(df2a\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnewData\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m      3\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moldData.account = newData.account AND oldData.dt_transaction = newData.dt_transaction\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m             \u001b[38;5;241m.\u001b[39mwhenMatchedUpdateAll()\n\u001b[1;32m      5\u001b[0m             \u001b[38;5;241m.\u001b[39mwhenNotMatchedInsertAll()\n\u001b[1;32m      6\u001b[0m       \u001b[38;5;241m.\u001b[39mexecute()\n\u001b[1;32m      7\u001b[0m     )\n\u001b[1;32m      9\u001b[0m deltaTable2\u001b[38;5;241m.\u001b[39mtoDF()\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'deltaTable2' is not defined"
     ]
    }
   ],
   "source": [
    "dt3=(deltaTable2.alias(\"oldData\")\n",
    "      .merge(df2a.alias(\"newData\"),\n",
    "            \"oldData.account = newData.account AND oldData.dt_transaction = newData.dt_transaction\")\n",
    "            .whenMatchedUpdateAll()\n",
    "            .whenNotMatchedInsertAll()\n",
    "      .execute()\n",
    "    )\n",
    "\n",
    "deltaTable2.toDF().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42a0894",
   "metadata": {},
   "outputs": [],
   "source": [
    "result=(deltaTable2\n",
    "        .toDF()\n",
    "        .withColumn(\"month\",month(col(\"dt_transaction\")))\n",
    "        .groupBy(\"account\",\"month\").agg(sum(\"balance\"))\n",
    "        .sort(\"account\",\"month\")\n",
    "       )\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db16769",
   "metadata": {},
   "outputs": [],
   "source": [
    "result=(spark.read\n",
    "        .format(\"delta\")\n",
    "        .option(\"versionAsOf\", \"1\")\n",
    "        .load(\"output/delta\")\n",
    "        .withColumn(\"month\",month(col(\"dt_transaction\")))\n",
    "        .groupBy(\"account\",\"month\").agg(sum(\"balance\"))\n",
    "        .sort(\"account\",\"month\")\n",
    "       )\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe344799",
   "metadata": {},
   "source": [
    "* Schema\n",
    "* Schema evolution\n",
    "* Transaction Log\n",
    "* Time Travel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b85012",
   "metadata": {},
   "source": [
    "## Iceberg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fafcace",
   "metadata": {},
   "source": [
    "- a **table format**\n",
    "- supports schema evolution and provides a portable table metadata format\n",
    "- best suited for analytical workloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2cb24f4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Catalog: default\n",
      "List Catalogs: [Database(name='default', description='Default Hive database', locationUri='file:/home/hive/warehouse')]\n",
      "List Tables in current Catalog: []\n"
     ]
    }
   ],
   "source": [
    "print(\"Current Catalog:\",spark.catalog.currentDatabase())\n",
    "print(\"List Catalogs:\",spark.catalog.listDatabases())\n",
    "print(\"List Tables in current Catalog:\",spark.catalog.listTables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7377b8ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a Database(name=<db_name>, locationUri='s3a://<bucket>/')\n",
    "spark.sql(f\"CREATE DATABASE iceberg_db LOCATION 's3a://{bucket}/'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "59b8232c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "| namespace|\n",
      "+----------+\n",
      "|   default|\n",
      "|iceberg_db|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### show databases and tables in iceberg catalog (only sees iceberg formated tables)\n",
    "# all databases from hive are shown\n",
    "spark.sql(\"SHOW databases from ice\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5010f061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables from iceberg_db\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cba4b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Delete Iceberg tables: first drop the table \n",
    "#spark.sql(\"drop table iceberg_db.iceberg_table\")\n",
    "#delete_objects(\"aleks-test\", \"iceberg_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "eafa7b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "write_iceberg=(df1\n",
    "                  .write\n",
    "                  .format(\"iceberg\")\n",
    "                  .mode(\"overwrite\")\n",
    "                  .saveAsTable(\"iceberg_db.iceberg\")\n",
    "               )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "494fb421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iceberg/data/00000-545-3b1bf31a-0ecb-4245-8a30-5d6b4de18962-00001.parquet\n",
      "iceberg/data/00000-550-7fa8b7a8-5984-487f-b524-9753864c2808-00001.parquet\n",
      "iceberg/data/00001-546-96c2733b-2dc9-4e04-b06d-1c2cbf6fe159-00001.parquet\n",
      "iceberg/data/00001-551-6cfea253-4caa-4fb6-bc7e-e3d9f956388b-00001.parquet\n",
      "iceberg/data/00002-547-3a8580fe-e207-4cf4-8015-2e6f91d99deb-00001.parquet\n",
      "iceberg/metadata/00000-2346cea3-3db3-46a4-bb55-419ae993156b.metadata.json\n",
      "iceberg/metadata/00001-70cc0e6d-feed-4202-bf2a-587f8dd81bbe.metadata.json\n",
      "iceberg/metadata/5dc0a4c4-3c72-4d16-a200-c299b635415c-m0.avro\n",
      "iceberg/metadata/73a18816-64a1-4119-8453-bb671b8abfb3-m0.avro\n",
      "iceberg/metadata/snap-2282180466624073266-1-73a18816-64a1-4119-8453-bb671b8abfb3.avro\n",
      "iceberg/metadata/snap-4263160168885610306-1-5dc0a4c4-3c72-4d16-a200-c299b635415c.avro\n"
     ]
    }
   ],
   "source": [
    "ls(bucket,\"iceberg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "92013c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "write_iceberg=(df2\n",
    "                   .write\n",
    "                   .format(\"iceberg\")\n",
    "                   .mode(\"append\") # append\n",
    "                   .saveAsTable(\"iceberg_db.iceberg\")\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fe763c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iceberg/data/00000-545-3b1bf31a-0ecb-4245-8a30-5d6b4de18962-00001.parquet\n",
      "iceberg/data/00000-550-7fa8b7a8-5984-487f-b524-9753864c2808-00001.parquet\n",
      "iceberg/data/00001-546-96c2733b-2dc9-4e04-b06d-1c2cbf6fe159-00001.parquet\n",
      "iceberg/data/00001-551-6cfea253-4caa-4fb6-bc7e-e3d9f956388b-00001.parquet\n",
      "iceberg/data/00002-547-3a8580fe-e207-4cf4-8015-2e6f91d99deb-00001.parquet\n",
      "iceberg/metadata/00000-2346cea3-3db3-46a4-bb55-419ae993156b.metadata.json\n",
      "iceberg/metadata/00001-70cc0e6d-feed-4202-bf2a-587f8dd81bbe.metadata.json\n",
      "iceberg/metadata/5dc0a4c4-3c72-4d16-a200-c299b635415c-m0.avro\n",
      "iceberg/metadata/73a18816-64a1-4119-8453-bb671b8abfb3-m0.avro\n",
      "iceberg/metadata/snap-2282180466624073266-1-73a18816-64a1-4119-8453-bb671b8abfb3.avro\n",
      "iceberg/metadata/snap-4263160168885610306-1-5dc0a4c4-3c72-4d16-a200-c299b635415c.avro\n"
     ]
    }
   ],
   "source": [
    "ls(bucket,\"iceberg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "964c7ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: iceberg/metadata/00000-2346cea3-3db3-46a4-bb55-419ae993156b.metadata.json\n",
      "----------------------\n",
      "{\n",
      "  \"format-version\" : 1,\n",
      "  \"table-uuid\" : \"c1a84965-5daa-434b-8f3a-3f27cd91ac55\",\n",
      "  \"location\" : \"s3a://fileformats//iceberg\",\n",
      "  \"last-updated-ms\" : 1684152769085,\n",
      "  \"last-column-id\" : 4,\n",
      "  \"schema\" : {\n",
      "    \"type\" : \"struct\",\n",
      "    \"schema-id\" : 0,\n",
      "    \"fields\" : [ {\n",
      "      \"id\" : 1,\n",
      "      \"name\" : \"id\",\n",
      "      \"required\" : false,\n",
      "      \"type\" : \"long\"\n",
      "    }, {\n",
      "      \"id\" : 2,\n",
      "      \"name\" : \"account\",\n",
      "      \"required\" : false,\n",
      "      \"type\" : \"string\"\n",
      "    }, {\n",
      "      \"id\" : 3,\n",
      "      \"name\" : \"dt_transaction\",\n",
      "      \"required\" : false,\n",
      "      \"type\" : \"date\"\n",
      "    }, {\n",
      "      \"id\" : 4,\n",
      "      \"name\" : \"balance\",\n",
      "      \"required\" : false,\n",
      "      \"type\" : \"long\"\n",
      "    } ]\n",
      "  },\n",
      "  \"current-schema-id\" : 0,\n",
      "  \"schemas\" : [ {\n",
      "    \"type\" : \"struct\",\n",
      "    \"schema-id\" : 0,\n",
      "    \"fields\" : [ {\n",
      "      \"id\" : 1,\n",
      "      \"name\" : \"id\",\n",
      "      \"required\" : false,\n",
      "      \"type\" : \"long\"\n",
      "    }, {\n",
      "      \"id\" : 2,\n",
      "      \"name\" : \"account\",\n",
      "      \"required\" : false,\n",
      "      \"type\" : \"string\"\n",
      "    }, {\n",
      "      \"id\" : 3,\n",
      "      \"name\" : \"dt_transaction\",\n",
      "      \"required\" : false,\n",
      "      \"type\" : \"date\"\n",
      "    }, {\n",
      "      \"id\" : 4,\n",
      "      \"name\" : \"balance\",\n",
      "      \"required\" : false,\n",
      "      \"type\" : \"long\"\n",
      "    } ]\n",
      "  } ],\n",
      "  \"partition-spec\" : [ ],\n",
      "  \"default-spec-id\" : 0,\n",
      "  \"partition-specs\" : [ {\n",
      "    \"spec-id\" : 0,\n",
      "    \"fields\" : [ ]\n",
      "  } ],\n",
      "  \"last-partition-id\" : 999,\n",
      "  \"default-sort-order-id\" : 0,\n",
      "  \"sort-orders\" : [ {\n",
      "    \"order-id\" : 0,\n",
      "    \"fields\" : [ ]\n",
      "  } ],\n",
      "  \"properties\" : {\n",
      "    \"owner\" : \"root\"\n",
      "  },\n",
      "  \"current-snapshot-id\" : 2282180466624073266,\n",
      "  \"refs\" : {\n",
      "    \"main\" : {\n",
      "      \"snapshot-id\" : 2282180466624073266,\n",
      "      \"type\" : \"branch\"\n",
      "    }\n",
      "  },\n",
      "  \"snapshots\" : [ {\n",
      "    \"snapshot-id\" : 2282180466624073266,\n",
      "    \"timestamp-ms\" : 1684152769085,\n",
      "    \"summary\" : {\n",
      "      \"operation\" : \"append\",\n",
      "      \"spark.app.id\" : \"spark-671357a054d742b2babacac61c4c8506\",\n",
      "      \"added-data-files\" : \"3\",\n",
      "      \"added-records\" : \"4\",\n",
      "      \"added-files-size\" : \"3689\",\n",
      "      \"changed-partition-count\" : \"1\",\n",
      "      \"total-records\" : \"4\",\n",
      "      \"total-files-size\" : \"3689\",\n",
      "      \"total-data-files\" : \"3\",\n",
      "      \"total-delete-files\" : \"0\",\n",
      "      \"total-position-deletes\" : \"0\",\n",
      "      \"total-equality-deletes\" : \"0\"\n",
      "    },\n",
      "    \"manifest-list\" : \"s3a://fileformats/iceberg/metadata/snap-2282180466624073266-1-73a18816-64a1-4119-8453-bb671b8abfb3.avro\",\n",
      "    \"schema-id\" : 0\n",
      "  } ],\n",
      "  \"statistics\" : [ ],\n",
      "  \"snapshot-log\" : [ {\n",
      "    \"timestamp-ms\" : 1684152769085,\n",
      "    \"snapshot-id\" : 2282180466624073266\n",
      "  } ],\n",
      "  \"metadata-log\" : [ ]\n",
      "}\n",
      "######################\n"
     ]
    }
   ],
   "source": [
    "cat(bucket,\"iceberg/metadata/00000-2346cea3-3db3-46a4-bb55-419ae993156b.metadata.json\",False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a10efcf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ALTER TABLE myTable ADD COLUMNS (address VARCHAR) - the number of columns in the df3 does not match the schema of the table, so we modify the schema of the existing table\n",
    "spark.sql(\"ALTER TABLE iceberg_db.iceberg ADD COLUMNS (new VARCHAR(50))\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a31c068c",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_iceberg=(df3\n",
    "                  .write\n",
    "                  .format(\"iceberg\")\n",
    "                  .mode(\"append\") # append\n",
    "                  .option(\"schema\", schema3)\n",
    "                  .saveAsTable(\"iceberg_db.iceberg\")\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2ff8d8a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- account: string (nullable = true)\n",
      " |-- dt_transaction: date (nullable = true)\n",
      " |-- balance: long (nullable = true)\n",
      " |-- new: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Read Iceberg table:\n",
    "\n",
    "iceberg_df = spark.read.table(\"iceberg_db.iceberg\")\n",
    "iceberg_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "53b286f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "|     made_current_at|        snapshot_id|          parent_id|is_current_ancestor|\n",
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "|2023-05-15 14:12:...|2282180466624073266|               null|               true|\n",
      "|2023-05-15 14:13:...|4263160168885610306|2282180466624073266|               true|\n",
      "|2023-05-15 14:16:...|3806458280092384569|4263160168885610306|               true|\n",
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-----------+-------+------------+------------------+--------------------+--------------------+--------------------+----------------+--------------------+--------------------+------------+-------------+------------+-------------+\n",
      "|content|           file_path|file_format|spec_id|record_count|file_size_in_bytes|        column_sizes|        value_counts|   null_value_counts|nan_value_counts|        lower_bounds|        upper_bounds|key_metadata|split_offsets|equality_ids|sort_order_id|\n",
      "+-------+--------------------+-----------+-------+------------+------------------+--------------------+--------------------+--------------------+----------------+--------------------+--------------------+------------+-------------+------------+-------------+\n",
      "|      0|s3a://fileformats...|    PARQUET|      0|           1|              1534|{1 -> 52, 2 -> 55...|{1 -> 1, 2 -> 1, ...|{1 -> 0, 2 -> 0, ...|              {}|{1 -> \u0001\u0000\u0000\u0000\u0000\u0000\u0000\u0000, 2...|{1 -> \u0001\u0000\u0000\u0000\u0000\u0000\u0000\u0000, 2...|        null|          [4]|        null|            0|\n",
      "|      0|s3a://fileformats...|    PARQUET|      0|           1|              1226|{1 -> 52, 2 -> 55...|{1 -> 1, 2 -> 1, ...|{1 -> 0, 2 -> 0, ...|              {}|{1 -> \u0001\u0000\u0000\u0000\u0000\u0000\u0000\u0000, 2...|{1 -> \u0001\u0000\u0000\u0000\u0000\u0000\u0000\u0000, 2...|        null|          [4]|        null|            0|\n",
      "|      0|s3a://fileformats...|    PARQUET|      0|           1|              1232|{1 -> 51, 2 -> 56...|{1 -> 1, 2 -> 1, ...|{1 -> 0, 2 -> 0, ...|              {}|{1 -> \u0002\u0000\u0000\u0000\u0000\u0000\u0000\u0000, 2...|{1 -> \u0002\u0000\u0000\u0000\u0000\u0000\u0000\u0000, 2...|        null|          [4]|        null|            0|\n",
      "|      0|s3a://fileformats...|    PARQUET|      0|           1|              1227|{1 -> 52, 2 -> 55...|{1 -> 1, 2 -> 1, ...|{1 -> 0, 2 -> 0, ...|              {}|{1 -> \u0001\u0000\u0000\u0000\u0000\u0000\u0000\u0000, 2...|{1 -> \u0001\u0000\u0000\u0000\u0000\u0000\u0000\u0000, 2...|        null|          [4]|        null|            0|\n",
      "|      0|s3a://fileformats...|    PARQUET|      0|           2|              1236|{1 -> 55, 2 -> 63...|{1 -> 2, 2 -> 2, ...|{1 -> 0, 2 -> 0, ...|              {}|{1 -> \u0002\u0000\u0000\u0000\u0000\u0000\u0000\u0000, 2...|{1 -> \u0004\u0000\u0000\u0000\u0000\u0000\u0000\u0000, 2...|        null|          [4]|        null|            0|\n",
      "|      0|s3a://fileformats...|    PARQUET|      0|           1|              1226|{1 -> 52, 2 -> 55...|{1 -> 1, 2 -> 1, ...|{1 -> 0, 2 -> 0, ...|              {}|{1 -> \u0003\u0000\u0000\u0000\u0000\u0000\u0000\u0000, 2...|{1 -> \u0003\u0000\u0000\u0000\u0000\u0000\u0000\u0000, 2...|        null|          [4]|        null|            0|\n",
      "+-------+--------------------+-----------+-------+------------+------------------+--------------------+--------------------+--------------------+----------------+--------------------+--------------------+------------+-------------+------------+-------------+\n",
      "\n",
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "|        committed_at|        snapshot_id|          parent_id|operation|       manifest_list|             summary|\n",
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "|2023-05-15 14:12:...|2282180466624073266|               null|   append|s3a://fileformats...|{spark.app.id -> ...|\n",
      "|2023-05-15 14:13:...|4263160168885610306|2282180466624073266|   append|s3a://fileformats...|{spark.app.id -> ...|\n",
      "|2023-05-15 14:16:...|3806458280092384569|4263160168885610306|   append|s3a://fileformats...|{spark.app.id -> ...|\n",
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark.sql(\"SELECT * FROM iceberg_db.iceberg.history;\").show()\n",
    "spark.sql(\"SELECT * FROM iceberg_db.iceberg.files;\").show()\n",
    "spark.sql(\"SELECT * FROM iceberg_db.iceberg.snapshots;\").show()\n",
    "\n",
    "## alternative syntax example:\n",
    "# spark.read.format(\"iceberg\").load(\"iceberg_db.iceberg_table.files\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6db241",
   "metadata": {},
   "source": [
    "### Iceberg: Time Travel\n",
    "- ```snapshot-id``` selects a specific table snapshot\n",
    "- ```as-of-timestamp``` selects the current snapshot at a timestamp, in milliseconds\n",
    "- ```branch``` selects the head snapshot of the specified branch. Note that currently branch cannot be combined with as-of-timestamp.\n",
    "- ```tag``` selects the snapshot associated with the specified tag. Tags cannot be combined with as-of-timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6c2d137e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 107:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------+-------+\n",
      "| id|account|dt_transaction|balance|\n",
      "+---+-------+--------------+-------+\n",
      "|  1|   alex|    2019-01-01|   1000|\n",
      "|  2|   alex|    2019-02-01|   1500|\n",
      "|  4|  maria|    2020-01-01|   5000|\n",
      "|  3|   alex|    2019-03-01|   1700|\n",
      "+---+-------+--------------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# from the results of iceberg_table.snapshots get the snapshots IDs\n",
    "snapshot1 = spark.read \\\n",
    "                 .option(\"snapshot-id\", \"2282180466624073266\") \\\n",
    "                 .format(\"iceberg\") \\\n",
    "                 .load(\"iceberg_db.iceberg\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "da46097d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------+-------+\n",
      "| id|account|dt_transaction|balance|\n",
      "+---+-------+--------------+-------+\n",
      "|  1|   alex|    2019-01-01|   1000|\n",
      "|  2|   alex|    2019-02-01|   1500|\n",
      "|  4|  maria|    2020-01-01|   5000|\n",
      "|  3|   alex|    2019-03-01|   1700|\n",
      "|  1|   alex|    2019-03-01|   3300|\n",
      "|  2|  peter|    2021-01-01|    100|\n",
      "+---+-------+--------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "snapshot2 = spark.read \\\n",
    "                 .option(\"snapshot-id\", \"4263160168885610306\") \\\n",
    "                 .format(\"iceberg\") \\\n",
    "                 .load(\"iceberg_db.iceberg\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "abaf72d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column<'(current_timestamp() - INTERVAL '0 00:10:00' DAY TO SECOND)'>\n"
     ]
    }
   ],
   "source": [
    "tsToExpire = f.current_timestamp() - timedelta(minutes=10)\n",
    "print(tsToExpire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "60455a0f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'table' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[82], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## need iceberg.table\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtable\u001b[49m\u001b[38;5;241m.\u001b[39mexpireSnapshots()\u001b[38;5;241m.\u001b[39mexpireOlderThan(tsToExpire)\u001b[38;5;241m.\u001b[39mcommit();\n",
      "\u001b[0;31mNameError\u001b[0m: name 'table' is not defined"
     ]
    }
   ],
   "source": [
    "## need iceberg.table\n",
    "## geth nicht verstehe ich. noch nicht??\n",
    "table.expireSnapshots().expireOlderThan(tsToExpire).commit();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94da9d7c",
   "metadata": {},
   "source": [
    "# Hudi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e558c54",
   "metadata": {},
   "source": [
    "- a **storage abstraction layer** \n",
    "- enables data ingestion and query capability on large-scale, evolving datasets\n",
    "- well-suited for real-time streaming workloads and batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "99ae9de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update partition path, i.e. \"id/dt_transaction\"\n",
    "record_key = \"id\"\n",
    "partition_path = \"id\"\n",
    "\n",
    "hudi_options = {\n",
    "    \"hoodie.table.name\": df1,\n",
    "    \"hoodie.datasource.write.recordkey.field\": record_key,\n",
    "    \"hoodie.datasource.write.partitionpath.field\": partition_path,\n",
    "    \"hoodie.datasource.write.table.name\": df1,\n",
    "    \"hoodie.datasource.write.operation\": \"upsert\",\n",
    "    \"hoodie.datasource.write.precombine.field\": \"ts\",  # This field is used by Hoodie to resolve conflicts between records with the same key (in this case, id) \n",
    "    \"hoodie.upsert.shuffle.parallelism\": 2,\n",
    "    \"hoodie.insert.shuffle.parallelism\": 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1ad416e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "write_hudi=(df1.withColumn(\"ts\", f.current_timestamp()).write.format(\"hudi\") # \"ts\" field is a mandatory field in Hoodie that specifies the timestamp of the record, so we add a new column and use simple current_timestamp() function\n",
    "               .options(**hudi_options)\n",
    "               .mode(\"overwrite\")\n",
    "               .save(f\"s3://{bucket}/hudi\")\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e2136c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hudi/.hoodie/.aux/.bootstrap/.fileids/\n",
      "hudi/.hoodie/.aux/.bootstrap/.partitions/\n",
      "hudi/.hoodie/.schema/\n",
      "hudi/.hoodie/.temp/\n",
      "hudi/.hoodie/20230515122157032.commit\n",
      "hudi/.hoodie/20230515122157032.commit.requested\n",
      "hudi/.hoodie/20230515122157032.inflight\n",
      "hudi/.hoodie/archived/\n",
      "hudi/.hoodie/hoodie.properties\n",
      "hudi/.hoodie/metadata/.hoodie/.aux/.bootstrap/.fileids/\n",
      "hudi/.hoodie/metadata/.hoodie/.aux/.bootstrap/.partitions/\n",
      "hudi/.hoodie/metadata/.hoodie/.schema/\n",
      "hudi/.hoodie/metadata/.hoodie/.temp/\n",
      "hudi/.hoodie/metadata/.hoodie/00000000000000.deltacommit\n",
      "hudi/.hoodie/metadata/.hoodie/00000000000000.deltacommit.inflight\n",
      "hudi/.hoodie/metadata/.hoodie/00000000000000.deltacommit.requested\n",
      "hudi/.hoodie/metadata/.hoodie/20230515122157032.deltacommit\n",
      "hudi/.hoodie/metadata/.hoodie/20230515122157032.deltacommit.inflight\n",
      "hudi/.hoodie/metadata/.hoodie/20230515122157032.deltacommit.requested\n",
      "hudi/.hoodie/metadata/.hoodie/archived/\n",
      "hudi/.hoodie/metadata/.hoodie/hoodie.properties\n",
      "hudi/.hoodie/metadata/files/.files-0000_00000000000000.log.1_0-0-0\n",
      "hudi/.hoodie/metadata/files/.files-0000_00000000000000.log.2_0-138-601\n",
      "hudi/.hoodie/metadata/files/.hoodie_partition_metadata\n",
      "hudi/1/.hoodie_partition_metadata\n",
      "hudi/1/cf0f8517-23b9-4415-ac61-57bdbfe69965-0_0-127-589_20230515122157032.parquet\n",
      "hudi/2/.hoodie_partition_metadata\n",
      "hudi/2/0cd93c57-fb35-4ff7-83ac-566835d662a0-0_1-127-590_20230515122157032.parquet\n",
      "hudi/3/.hoodie_partition_metadata\n",
      "hudi/3/d3728799-997a-412c-ae8d-bfcb35c89ea6-0_2-127-591_20230515122157032.parquet\n",
      "hudi/4/.hoodie_partition_metadata\n",
      "hudi/4/ee5e9af9-3f2d-471c-943d-26cc7607b950-0_3-127-592_20230515122157032.parquet\n"
     ]
    }
   ],
   "source": [
    "ls(bucket,\"hudi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "52aa8b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# WARNING: Unable to get Instrumentation. Dynamic Attach failed. You may add this JAR as -javaagent manually, or supply -Djdk.attach.allowAttachSelf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 151:>                                                        (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# WARNING: Unable to attach Serviceability Agent. Unable to attach even with module exceptions: [org.apache.hudi.org.openjdk.jol.vm.sa.SASupportException: Sense failed., org.apache.hudi.org.openjdk.jol.vm.sa.SASupportException: Sense failed., org.apache.hudi.org.openjdk.jol.vm.sa.SASupportException: Sense failed.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "write_hudi=(df2.withColumn(\"ts\", f.current_timestamp()).write.format(\"hudi\") # \"ts\" field is a mandatory field in Hoodie that specifies the timestamp of the record, so we add a new column and use simple current_timestamp() function\n",
    "               .options(**hudi_options)\n",
    "               .mode(\"append\")\n",
    "               .save(f\"s3://{bucket}/hudi\")\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ed1b5e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "write_hudi=(df3.withColumn(\"ts\", f.current_timestamp()).write.format(\"hudi\") # \"ts\" field is a mandatory field in Hoodie that specifies the timestamp of the record, so we add a new column and use simple current_timestamp() function\n",
    "               .options(**hudi_options)\n",
    "               .mode(\"append\")\n",
    "               .save(f\"s3://{bucket}/hudi\")\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6b2066ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+------------------+----------------------+--------------------+-------+--------------+-------+-------------+--------------------+---+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|account|dt_transaction|balance|          new|                  ts| id|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-------+--------------+-------+-------------+--------------------+---+\n",
      "|  20230515122339203|20230515122339203...|                 1|                     1|cf0f8517-23b9-441...|   otto|    2019-10-01|   4444|neue Spalte 1|2023-05-15 14:23:...|  1|\n",
      "|  20230515122157032|20230515122157032...|                 4|                     4|ee5e9af9-3f2d-471...|  maria|    2020-01-01|   5000|         null|2023-05-15 14:21:...|  4|\n",
      "|  20230515122321620|20230515122321620...|                 2|                     2|0cd93c57-fb35-4ff...|  peter|    2021-01-01|    100|         null|2023-05-15 14:23:...|  2|\n",
      "|  20230515122157032|20230515122157032...|                 3|                     3|d3728799-997a-412...|   alex|    2019-03-01|   1700|         null|2023-05-15 14:21:...|  3|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-------+--------------+-------+-------------+--------------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_hudi = spark.read.format(\"hudi\").load(f\"s3://{bucket}/hudi\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "814d698d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hudi/.hoodie/.aux/.bootstrap/.fileids/\n",
      "hudi/.hoodie/.aux/.bootstrap/.partitions/\n",
      "hudi/.hoodie/.schema/\n",
      "hudi/.hoodie/.temp/\n",
      "hudi/.hoodie/20230515122157032.commit\n",
      "hudi/.hoodie/20230515122157032.commit.requested\n",
      "hudi/.hoodie/20230515122157032.inflight\n",
      "hudi/.hoodie/20230515122321620.commit\n",
      "hudi/.hoodie/20230515122321620.commit.requested\n",
      "hudi/.hoodie/20230515122321620.inflight\n",
      "hudi/.hoodie/20230515122339203.commit\n",
      "hudi/.hoodie/20230515122339203.commit.requested\n",
      "hudi/.hoodie/20230515122339203.inflight\n",
      "hudi/.hoodie/archived/\n",
      "hudi/.hoodie/hoodie.properties\n",
      "hudi/.hoodie/metadata/.hoodie/.aux/.bootstrap/.fileids/\n",
      "hudi/.hoodie/metadata/.hoodie/.aux/.bootstrap/.partitions/\n",
      "hudi/.hoodie/metadata/.hoodie/.schema/\n",
      "hudi/.hoodie/metadata/.hoodie/.temp/\n",
      "hudi/.hoodie/metadata/.hoodie/00000000000000.deltacommit\n",
      "hudi/.hoodie/metadata/.hoodie/00000000000000.deltacommit.inflight\n",
      "hudi/.hoodie/metadata/.hoodie/00000000000000.deltacommit.requested\n",
      "hudi/.hoodie/metadata/.hoodie/20230515122157032.deltacommit\n",
      "hudi/.hoodie/metadata/.hoodie/20230515122157032.deltacommit.inflight\n",
      "hudi/.hoodie/metadata/.hoodie/20230515122157032.deltacommit.requested\n",
      "hudi/.hoodie/metadata/.hoodie/20230515122321620.deltacommit\n",
      "hudi/.hoodie/metadata/.hoodie/20230515122321620.deltacommit.inflight\n",
      "hudi/.hoodie/metadata/.hoodie/20230515122321620.deltacommit.requested\n",
      "hudi/.hoodie/metadata/.hoodie/20230515122339203.deltacommit\n",
      "hudi/.hoodie/metadata/.hoodie/20230515122339203.deltacommit.inflight\n",
      "hudi/.hoodie/metadata/.hoodie/20230515122339203.deltacommit.requested\n",
      "hudi/.hoodie/metadata/.hoodie/archived/\n",
      "hudi/.hoodie/metadata/.hoodie/hoodie.properties\n",
      "hudi/.hoodie/metadata/files/.files-0000_00000000000000.log.1_0-0-0\n",
      "hudi/.hoodie/metadata/files/.files-0000_00000000000000.log.2_0-138-601\n",
      "hudi/.hoodie/metadata/files/.files-0000_00000000000000.log.3_0-175-641\n",
      "hudi/.hoodie/metadata/files/.files-0000_00000000000000.log.4_0-212-674\n",
      "hudi/.hoodie/metadata/files/.hoodie_partition_metadata\n",
      "hudi/1/.hoodie_partition_metadata\n",
      "hudi/1/cf0f8517-23b9-4415-ac61-57bdbfe69965-0_0-127-589_20230515122157032.parquet\n",
      "hudi/1/cf0f8517-23b9-4415-ac61-57bdbfe69965-0_0-164-633_20230515122321620.parquet\n",
      "hudi/1/cf0f8517-23b9-4415-ac61-57bdbfe69965-0_0-201-668_20230515122339203.parquet\n",
      "hudi/2/.hoodie_partition_metadata\n",
      "hudi/2/0cd93c57-fb35-4ff7-83ac-566835d662a0-0_1-127-590_20230515122157032.parquet\n",
      "hudi/2/0cd93c57-fb35-4ff7-83ac-566835d662a0-0_1-164-634_20230515122321620.parquet\n",
      "hudi/3/.hoodie_partition_metadata\n",
      "hudi/3/d3728799-997a-412c-ae8d-bfcb35c89ea6-0_2-127-591_20230515122157032.parquet\n",
      "hudi/4/.hoodie_partition_metadata\n",
      "hudi/4/ee5e9af9-3f2d-471c-943d-26cc7607b950-0_3-127-592_20230515122157032.parquet\n"
     ]
    }
   ],
   "source": [
    "ls(bucket,\"hudi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2c394a",
   "metadata": {},
   "source": [
    "#### Hudi: Time Travel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1a428acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+------------------+----------------------+--------------------+-------+--------------+-------+-------------+--------------------+---+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|account|dt_transaction|balance|          new|                  ts| id|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-------+--------------+-------+-------------+--------------------+---+\n",
      "|  20230515122339203|20230515122339203...|                 1|                     1|cf0f8517-23b9-441...|   otto|    2019-10-01|   4444|neue Spalte 1|2023-05-15 14:23:...|  1|\n",
      "|  20230515122157032|20230515122157032...|                 4|                     4|ee5e9af9-3f2d-471...|  maria|    2020-01-01|   5000|         null|2023-05-15 14:21:...|  4|\n",
      "|  20230515122321620|20230515122321620...|                 2|                     2|0cd93c57-fb35-4ff...|  peter|    2021-01-01|    100|         null|2023-05-15 14:23:...|  2|\n",
      "|  20230515122157032|20230515122157032...|                 3|                     3|d3728799-997a-412...|   alex|    2019-03-01|   1700|         null|2023-05-15 14:21:...|  3|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-------+--------------+-------+-------------+--------------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Get the commit time from the Hudi table\n",
    "\n",
    "spark.read.format(\"hudi\")\\\n",
    "     .option(\"as.of.instant\", \"20230515122339203\")\\\n",
    "     .load(f\"s3://{bucket}/hudi\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "46611d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "account_data4 = [\n",
    "    (5,\"anna\",\"2020-11-01\",2000,\"neue Spalte 1\")\n",
    "]\n",
    "df4 = spark.createDataFrame(data=account_data4, schema = schema3).withColumn(\"dt_transaction\",col(\"dt_transaction\").cast(\"date\")).repartition(3)\n",
    "\n",
    "write_hudi=(df4.withColumn(\"ts\", f.current_timestamp()).write.format(\"hudi\")\n",
    "               .options(**hudi_options)\n",
    "               .mode(\"append\")\n",
    "               .save(f\"s3://{bucket}/hudi\")\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ea78e5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Incremental query:\n",
    "\n",
    "spark.read.format(\"hudi\"). \\\n",
    "  load(f\"s3://{bucket}/hudi\"). \\\n",
    "  createOrReplaceTempView(\"hudi_snapshots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "fc0c8bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 276:============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20230515122157032', '20230515122321620', '20230515122339203', '20230515123027657']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "commits = list(map(lambda row: row[0], spark.sql(\"select distinct(_hoodie_commit_time) as commitTime from hudi_snapshots order by commitTime\").limit(10).collect()))\n",
    "print(commits)\n",
    "\n",
    "beginTime = commits[len(commits) - 4] # commit time we are interested in\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9c791ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+-------+--------------+--------------------------+\n",
      "|_hoodie_commit_time|account|balance|dt_transaction|ts                        |\n",
      "+-------------------+-------+-------+--------------+--------------------------+\n",
      "|20230515122339203  |otto   |4444   |2019-10-01    |2023-05-15 14:23:39.428869|\n",
      "|20230515123027657  |anna   |2000   |2020-11-01    |2023-05-15 14:30:27.805069|\n",
      "|20230515122321620  |peter  |100    |2021-01-01    |2023-05-15 14:23:21.809413|\n",
      "+-------------------+-------+-------+--------------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# incrementally query data\n",
    "incremental_read_options = {\n",
    "  'hoodie.datasource.query.type': 'incremental',\n",
    "  'hoodie.datasource.read.begin.instanttime': beginTime,\n",
    "}\n",
    "\n",
    "hudiIncrementalDF = spark.read.format(\"hudi\"). \\\n",
    "  options(**incremental_read_options). \\\n",
    "  load(f\"s3://{bucket}/hudi\")\n",
    "hudiIncrementalDF .createOrReplaceTempView(\"hudi_incremental\")\n",
    "\n",
    "spark.sql(\"select `_hoodie_commit_time`, account, balance, dt_transaction, ts from hudi_incremental\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ac5492",
   "metadata": {},
   "source": [
    "### Hudi: Table maintenance\n",
    "Hudi can run async or inline table services while running Strucrured Streaming query and takes care of cleaning, compaction and clustering. There's no operational overhead for the user.\n",
    "For CoW tables, table services work in inline mode by default.\n",
    "For MoR tables, some async services are enabled by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fcb4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
