{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "349d8c01",
   "metadata": {},
   "source": [
    "# Demo and Comparission of Big Data File Formats\n",
    "This Notebook runs only on a local Spark Environment, not on Kubernetes\n",
    "\n",
    "## 1. CSV and JSON\n",
    "Old dat formats that are not designed for big data and scaling  \n",
    "**Typical feature:** humand readable\n",
    "\n",
    "## 2. Avro, OCR, Parquet\n",
    "First generation of special big data formats that allow fast writes, fast reads or both  \n",
    "**Typical features:** splittable, compressible, data skipping and predicat pushdown, data schema inclueded\n",
    "\n",
    "\n",
    "\n",
    "## 3. Delta, Iceberg, Hudi\n",
    "Latest generation of big data format that support ACID transaction, audit save transaction logs and time travel  \n",
    "**Typical features:** enhancing first generation format with additonal meta data and read/write procedures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4b9472c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container {width:100% !important; }<style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#################################################################################\n",
    "# Load all relevant Python Modules\n",
    "#################################################################################\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import *\n",
    "import json\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from delta import *\n",
    "#import delta\n",
    "\n",
    "# use 95% of the screen for jupyter cell\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container {width:100% !important; }<style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5fc41f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf()\n",
    "\n",
    "# configure Avro Connector\n",
    "conf.set(\"spark.jars\", \"/Users/alor/opt/spark/jars/spark-sql-kafka-0-10_2.12-3.3.1.jar, /Users/alor/opt/spark/jars/kafka-clients-3.3.1.jar, /Users/alor/opt/spark/jars/spark-avro_2.12-3.3.1.jar\")\n",
    "conf.set(\"spark.driver.extraClassPath\",\"/Users/alor/opt/spark/jars/spark-sql-kafka-0-10_2.12-3.3.1.jar, /Users/alor/opt/spark/jars/kafka-clients-3.3.1.jar, /Users/alor/opt/spark/jars/spark-avro_2.12-3.3.1.jar\")\n",
    "conf.set(\"spark.executor.extraClassPath\",\"/Users/alor/opt/spark/jars/spark-sql-kafka-0-10_2.12-3.3.1.jar, /Users/alor/opt/spark/jars/kafka-clients-3.3.1.jar, /Users/alor/opt/spark/jars/spark-avro_2.12-3.3.1.jar\")\n",
    "\n",
    "# configure Delta Connector\n",
    "conf.set(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "conf.set(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "conf.set(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.0.0\")\n",
    "\n",
    "# configure Iceberg Connector\n",
    "conf.set(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\n",
    "conf.set(\"spark.sql.catalog.spark_catalog\",\"org.apache.iceberg.spark.SparkSessionCatalog\")\n",
    "conf.set(\"spark.jars.packages\", \"org.apache.iceberg:iceberg-core:1.2.0\")\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName(\"FileFormatDemo\").config(conf=conf).getOrCreate()\n",
    "\n",
    "sqlContext = SparkSession(spark)\n",
    "#Dont Show warning only error\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d1289301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://macbook-thinkport.fritz.box:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fda293cbaf0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c446a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++ create new dataframe and show schema and data\n",
      "################################################\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- account: string (nullable = true)\n",
      " |-- dt_transaction: date (nullable = true)\n",
      " |-- balance: long (nullable = true)\n",
      "\n",
      "+---+-------+--------------+-------+\n",
      "|id |account|dt_transaction|balance|\n",
      "+---+-------+--------------+-------+\n",
      "|2  |alex   |2019-02-01    |1500   |\n",
      "|3  |alex   |2019-03-01    |1700   |\n",
      "|1  |alex   |2019-01-01    |1000   |\n",
      "|4  |maria  |2020-01-01    |5000   |\n",
      "+---+-------+--------------+-------+\n",
      "\n",
      "+---+-------+--------------+-------+-------------+\n",
      "|id |account|dt_transaction|balance|new          |\n",
      "+---+-------+--------------+-------+-------------+\n",
      "|1  |otto   |2019-10-01    |4444   |neue Spalte 1|\n",
      "+---+-------+--------------+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# initial daten\n",
    "account_data1 = [\n",
    "    (1,\"alex\",\"2019-01-01\",1000),\n",
    "    (2,\"alex\",\"2019-02-01\",1500),\n",
    "    (3,\"alex\",\"2019-03-01\",1700),\n",
    "    (4,\"maria\",\"2020-01-01\",5000)\n",
    "    ]\n",
    "\n",
    "# update mit Ã„nderung und neuem Datensat\n",
    "account_data2 = [\n",
    "    (1,\"alex\",\"2019-03-01\",3300),\n",
    "    (2,\"peter\",\"2021-01-01\",100)\n",
    "    ]\n",
    "\n",
    "# Update mit neuer Spalte\n",
    "account_data3 = [\n",
    "    (1,\"otto\",\"2019-10-01\",4444,\"neue Spalte 1\")\n",
    "]\n",
    "\n",
    "schema = [\"id\",\"account\",\"dt_transaction\",\"balance\"]\n",
    "schema3 = [\"id\",\"account\",\"dt_transaction\",\"balance\",\"new\"]\n",
    "\n",
    "df1 = spark.createDataFrame(data=account_data1, schema = schema).withColumn(\"dt_transaction\",col(\"dt_transaction\").cast(\"date\")).repartition(3)\n",
    "df2 = spark.createDataFrame(data=account_data2, schema = schema).withColumn(\"dt_transaction\",col(\"dt_transaction\").cast(\"date\")).repartition(3)\n",
    "df3 = spark.createDataFrame(data=account_data3, schema = schema3).withColumn(\"dt_transaction\",col(\"dt_transaction\").cast(\"date\")).repartition(3)\n",
    "\n",
    "print(\"++ create new dataframe and show schema and data\")\n",
    "print(\"################################################\")\n",
    "\n",
    "df1.printSchema()\n",
    "df1.show(truncate=False)\n",
    "df3.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9dc0b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Partitions: 3\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Partitions:\", df1.rdd.getNumPartitions())\n",
    "\n",
    "write_avro=(df1\n",
    "           .write\n",
    "           .format(\"avro\")\n",
    "           .mode(\"overwrite\") # append\n",
    "           .save(\"output/avro\")\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf164462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_SUCCESS\r\n",
      "part-00000-3da3e1e6-e463-4236-b8a9-c7b54cd2bd3b-c000.avro\r\n",
      "part-00001-3da3e1e6-e463-4236-b8a9-c7b54cd2bd3b-c000.avro\r\n",
      "part-00002-3da3e1e6-e463-4236-b8a9-c7b54cd2bd3b-c000.avro\r\n"
     ]
    }
   ],
   "source": [
    "!ls output/avro/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "309a164c",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_delta=(df1\n",
    "           .write\n",
    "           .format(\"delta\")\n",
    "           .option(\"mergeSchema\", \"true\")\n",
    "           .mode(\"overwrite\") # append\n",
    "           .save(\"output/delta\")\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d15b379e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m_delta_log\u001b[m\u001b[m\r\n",
      "part-00000-2017faee-4e09-4c52-b5eb-7a268090cf48-c000.snappy.parquet\r\n",
      "part-00001-1c10ce50-4c61-481f-a468-29e9d6c1edc4-c000.snappy.parquet\r\n",
      "part-00002-5693aaf1-8d73-47d4-9c2b-3430cd7336a0-c000.snappy.parquet\r\n"
     ]
    }
   ],
   "source": [
    "!ls output/delta/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54ace436",
   "metadata": {},
   "outputs": [],
   "source": [
    "deltaTable = DeltaTable.forPath(spark, \"output/delta\")\n",
    "\n",
    "fullHistoryDF = deltaTable.history()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "45a88747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-------------------+------+---------+--------------------+--------------------+------------+\n",
      "|version|readVersion|          timestamp|userId|operation| operationParameters|    operationMetrics|userMetadata|\n",
      "+-------+-----------+-------------------+------+---------+--------------------+--------------------+------------+\n",
      "|      0|       null|2023-04-12 09:35:45|  null|    WRITE|{mode -> Overwrit...|{numFiles -> 3, n...|        null|\n",
      "+-------+-----------+-------------------+------+---------+--------------------+--------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fullHistoryDF.select(\"version\",\"readVersion\",\"timestamp\",\"userId\",\"operation\",\"operationParameters\",\"operationMetrics\",\"userMetadata\").show(truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c5978f57",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrameWriter' object has no attribute 'saveTo'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-18f828dc1021>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# write via hive to iceberg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m write=(df1\n\u001b[0m\u001b[1;32m      3\u001b[0m       \u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m       \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"iceberg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m       \u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"overwrite\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrameWriter' object has no attribute 'saveTo'"
     ]
    }
   ],
   "source": [
    "# write via hive to iceberg\n",
    "write=(df1\n",
    "      .write\n",
    "      .format(\"iceberg\")\n",
    "      .mode(\"overwrite\")\n",
    "      .saveTo(\"output/iceberg\")\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f4a5824",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd5b797",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
