{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5765a06e",
   "metadata": {},
   "source": [
    "# Spark Aufgaben\n",
    "\n",
    "1. Read Twitter Streams from Avro\n",
    "2. Do analysis Tasks\n",
    "3. Write Result to Delta several times\n",
    "4. Du some History analysis on it\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eebe3ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24469/3232487455.py:18: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container {width:100% !important; }<style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import Row\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "from delta import *\n",
    "\n",
    "\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "\n",
    "# use 95% of the screen for jupyter cell\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container {width:100% !important; }<style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6d15c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/07 16:51:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.kubernetes.namespace = frontend\n",
      "spark.master = k8s://https://kubernetes.default.svc.cluster.local:443\n",
      "spark.app.name = jupyter-spark\n",
      "spark.executor.memory = 1G\n",
      "spark.executor.cores = 2\n",
      "spark.driver.host = jupyter-spark-driver.frontend.svc.cluster.local\n"
     ]
    }
   ],
   "source": [
    "appName=\"jupyter-spark\"\n",
    "\n",
    "conf = SparkConf()\n",
    "\n",
    "# CLUSTER MANAGER\n",
    "################################################################################\n",
    "# set Kubernetes Master as Cluster Manager(“k8s://https://” is NOT a typo, this is how Spark knows the “provider” type).\n",
    "conf.setMaster(\"k8s://https://kubernetes.default.svc.cluster.local:443\")\n",
    "\n",
    "# CONFIGURE KUBERNETES\n",
    "################################################################################\n",
    "# set the namespace that will be used for running the driver and executor pods.\n",
    "conf.set(\"spark.kubernetes.namespace\",\"frontend\")\n",
    "# set the docker image from which the Worker pods are created\n",
    "conf.set(\"spark.kubernetes.container.image\", \"thinkportgmbh/workshops:spark-3.3.1\")\n",
    "conf.set(\"spark.kubernetes.container.image.pullPolicy\", \"Always\")\n",
    "\n",
    "# set service account to be used\n",
    "conf.set(\"spark.kubernetes.authenticate.driver.serviceAccountName\", \"spark\")\n",
    "# authentication for service account(required to create worker pods):\n",
    "conf.set(\"spark.kubernetes.authenticate.caCertFile\", \"/var/run/secrets/kubernetes.io/serviceaccount/ca.crt\")\n",
    "conf.set(\"spark.kubernetes.authenticate.oauthTokenFile\", \"/var/run/secrets/kubernetes.io/serviceaccount/token\")\n",
    "\n",
    "\n",
    "# CONFIGURE SPARK\n",
    "################################################################################\n",
    "conf.set(\"spark.sql.session.timeZone\", \"Europe/Berlin\")\n",
    "# set driver host. In this case the ingres service for the spark driver\n",
    "# find name of the driver service with 'kubectl get services' or in the helm chart configuration\n",
    "conf.set(\"spark.driver.host\", \"jupyter-spark-driver.frontend.svc.cluster.local\")\n",
    "# set the port, If this port is busy, spark-shell tries to bind to another port.\n",
    "conf.set(\"spark.driver.port\", \"29413\")\n",
    "# add the postgres driver jars into session\n",
    "conf.set(\"spark.jars\", \"/opt/spark/jars/spark-sql-kafka-0-10_2.12-3.3.1.jar, /opt/spark/jars/kafka-clients-3.3.1.jar, /opt/spark/jars/spark-avro_2.12-3.3.1.jar\")\n",
    "#conf.set(\"spark.driver.extraClassPath\",\"/opt/spark/jars/spark-sql-kafka-0-10_2.12-3.3.1.jar, /opt/spark/jars/kafka-clients-3.3.1.jar, /opt/spark/jars/spark-avro_2.12-3.3.1.jar\")\n",
    "conf.set(\"spark.executor.extraClassPath\",\"/opt/spark/jars/spark-sql-kafka-0-10_2.12-3.3.1.jar, /opt/spark/jars/kafka-clients-3.3.1.jar, /opt/spark/jars/spark-avro_2.12-3.3.1.jar\")\n",
    "#conf.set(\"spark.executor.extraLibrary\",\"/opt/spark/jars/spark-sql-kafka-0-10_2.12-3.3.1.jar, /opt/spark/jars/kafka-clients-3.3.1.jar\")\n",
    "\n",
    "# CONFIGURE S3 CONNECTOR\n",
    "conf.set(\"spark.hadoop.fs.s3a.endpoint\", \"minio.minio.svc.cluster.local:9000\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.access.key\", \"trainadm\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.secret.key\", \"train@thinkport\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "conf.set(\"spark.hadoop.fs.s3.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "\n",
    "# conf.set(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.1\")\n",
    "\n",
    "# CONFIGURE WORKER (Customize based on workload)\n",
    "################################################################################\n",
    "# set number of worker pods\n",
    "conf.set(\"spark.executor.instances\", \"1\")\n",
    "# set memory of each worker pod\n",
    "conf.set(\"spark.executor.memory\", \"1G\")\n",
    "# set cpu of each worker pod\n",
    "conf.set(\"spark.executor.cores\", \"2\")\n",
    "# Number of possible tasks = cores * executores\n",
    "\n",
    "# SPARK SESSION\n",
    "################################################################################\n",
    "# and last, create the spark session and pass it the config object\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .config(conf=conf) \\\n",
    "    .config('spark.sql.session.timeZone', 'Europe/Berlin') \\\n",
    "    .appName(appName)\\\n",
    "    .getOrCreate()\n",
    "\n",
    "# also get the spark context\n",
    "sc=spark.sparkContext\n",
    "# change the log level to warning, to see less output\n",
    "sc.setLogLevel('WARN')\n",
    "\n",
    "# get the configuration object to check all the configurations the session was startet with\n",
    "for entry in sc.getConf().getAll():\n",
    "        if entry[0] in [\"spark.app.name\",\"spark.kubernetes.namespace\",\"spark.executor.memory\",\"spark.executor.cores\",\"spark.driver.host\",\"spark.master\"]:\n",
    "            print(entry[0],\"=\",entry[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90b65543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/07 16:52:20 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:=======================================================> (37 + 1) / 38]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df=spark.read.format(\"avro\").load(\"s3a://twitter/avro\")\n",
    "\n",
    "print(df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de995980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.show of DataFrame[tweet_id: string, created_at: timestamp, tweet_message: string, user_name: string, user_location: string, user_follower_count: int, user_friends_count: int, retweet_count: int, language: string, hashtags: array<string>]>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "990cbeff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+--------------------+---------------+--------------------+-------------------+------------------+-------------+--------+--------------------+\n",
      "|           tweet_id|         created_at|       tweet_message|      user_name|       user_location|user_follower_count|user_friends_count|retweet_count|language|            hashtags|\n",
      "+-------------------+-------------------+--------------------+---------------+--------------------+-------------------+------------------+-------------+--------+--------------------+\n",
      "|1600474456073424898|2022-12-07 13:57:00|This #Tesla Cyber...|  Paula_Piccard|            New York|              73768|              9401|            0|      en|                null|\n",
      "|1600474576818954242|2022-12-07 13:57:29|AI Isn't Artifici...|   DTN_Graphics|             Nigeria|                159|               298|            0|      en|                null|\n",
      "|1600474724580003846|2022-12-07 13:58:04|RT @Long_Shot_Ads...|WorldTrendsInfo|       New York, USA|              34779|             15241|            0|      en|                null|\n",
      "|1600475099857301506|2022-12-07 13:59:33|RT @sonu_monika: ...| DigitalMachina|           Bengaluru|                199|               206|            0|     qme|                null|\n",
      "|1600475154257018880|2022-12-07 13:59:46|RT @Khulood_Alman...|      RLDI_Lamy|                null|              16346|             16905|            0|      en|                null|\n",
      "|1600475483480678400|2022-12-07 14:01:05|RT @AjayKap832787...| LiberationWork|                 USA|                486|                12|            0|     qme|[designthinking, ...|\n",
      "|1600475523385151489|2022-12-07 14:01:14|RT @Khulood_Alman...| Khulood_Almani|Kingdom of Saudi ...|              44910|              2259|            0|      en|[Intelligence, Au...|\n",
      "|1600475534689026049|2022-12-07 14:01:17|AIM spoke to @Ree...|Analyticsindiam|    Bengaluru, India|              15350|               483|            0|      en|[quantumcomputing...|\n",
      "|1600475603202981889|2022-12-07 14:01:33|Micro Focus and J...|     jenna_loup|         Houston, TX|                 97|               102|            0|      en|[sustainability, ...|\n",
      "|1600475677743980544|2022-12-07 14:01:51|RT @Khulood_Alman...| Khulood_Almani|Kingdom of Saudi ...|              44910|              2259|            0|      en|[DataScientist, J...|\n",
      "|1600475711029989376|2022-12-07 14:01:59|RT @ghostx1010: E...|   falconX_1010|                null|                  3|               203|            0|      en|[MachineLearning,...|\n",
      "|1600475715505373184|2022-12-07 14:02:00|How to Transform ...|   DivergentCIO|         Kansas City|              29734|             28024|            0|      en|[EmergingTech, AI...|\n",
      "|1600475719779360770|2022-12-07 14:02:01|RT @Khulood_Alman...|  flutterbyamey|        Flutterverse|                855|                22|            0|      en|[IoT, Business, A...|\n",
      "|1600475776108793856|2022-12-07 14:02:15|RT @Khulood_Alman...| Khulood_Almani|Kingdom of Saudi ...|              44910|              2259|            0|      en|[Regulation, Meta...|\n",
      "|1600475778503696384|2022-12-07 14:02:15|RT @Khulood_Alman...|   HAMDANLAVI89| Hafr Al Batin, KSA.|                225|               458|            0|      en|[Data, AI, Python...|\n",
      "|1600475800909717505|2022-12-07 14:02:20|RT @Khulood_Alman...| Khulood_Almani|Kingdom of Saudi ...|              44910|              2259|            0|      en|[DataAnalytics, S...|\n",
      "|1600475803912994816|2022-12-07 14:02:21|RT @Khulood_Alman...|  flutterbyamey|        Flutterverse|                855|                22|            0|      en|[Data, AI, Python...|\n",
      "|1600475805641064448|2022-12-07 14:02:22|Six Ways Artifici...|  terence_mills|New York | London...|              16043|              8615|            0|      en|[AI, AIio, BigDat...|\n",
      "|1600475878600765441|2022-12-07 14:02:39|RT @Khulood_Alman...| Khulood_Almani|Kingdom of Saudi ...|              44910|              2259|            0|      en|[Smart, SmartCity...|\n",
      "|1600475887631212544|2022-12-07 14:02:41|RT @Khulood_Alman...|  flutterbyamey|        Flutterverse|                855|                22|            0|      en|[Smart, SmartCity...|\n",
      "+-------------------+-------------------+--------------------+---------------+--------------------+-------------------+------------------+-------------+--------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"avro\").load(\"s3a://twitter/avro\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518be64b",
   "metadata": {},
   "source": [
    "## Aufgaben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8588d58a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tweet_id: string (nullable = true)\n",
      " |-- created_at: timestamp (nullable = true)\n",
      " |-- tweet_message: string (nullable = true)\n",
      " |-- user_name: string (nullable = true)\n",
      " |-- user_location: string (nullable = true)\n",
      " |-- user_follower_count: integer (nullable = true)\n",
      " |-- user_friends_count: integer (nullable = true)\n",
      " |-- retweet_count: integer (nullable = true)\n",
      " |-- language: string (nullable = true)\n",
      " |-- hashtags: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc238b9",
   "metadata": {},
   "source": [
    "### 1. Zählen der Tweets pro Stunde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1185327b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hourly=(df  \n",
    "        # if stream did run less than an hour take minutes\n",
    "        .withColumn(\"hour\", f.hour(f.col(\"created_at\")))\n",
    "        .groupBy(\"hour\")\n",
    "        .count()\n",
    "        .withColumnRenamed(\"count\",\"total\")\n",
    "        .sort(\"hour\")\n",
    "    )\n",
    "\n",
    "df_hourly.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0d5a8f",
   "metadata": {},
   "source": [
    "### 1. Top 10 User nach Tweet-Anzahl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c2a6ca1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 76:======================================================> (42 + 1) / 43]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|          user|total|\n",
      "+--------------+-----+\n",
      "|   Eli_Krumova|  127|\n",
      "|Khulood_Almani|   82|\n",
      "|       cuongcz|   61|\n",
      "| Soul_Dispatch|   54|\n",
      "|  chidambara09|   50|\n",
      "|       sdogdev|   48|\n",
      "|TechNativeWire|   28|\n",
      "| flutterbyamey|   28|\n",
      "|      mikejo_m|   26|\n",
      "|    TechNative|   22|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_top_user=(df\n",
    "                .groupBy(\"user_name\")\n",
    "                .agg(\n",
    "                    f.count(\"user_name\").alias(\"total\")\n",
    "                    )\n",
    "                .orderBy(f.col(\"total\").desc())\n",
    "                .limit(10)\n",
    "                .withColumnRenamed(\"user_name\",\"user\")\n",
    "                )\n",
    "\n",
    "df_top_user.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58765db1",
   "metadata": {},
   "source": [
    "### Top 5 Hashtags der Top 10 User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0bd4d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:=====================================================>  (36 + 2) / 38]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------+\n",
      "|       hashtags|count(hashtags)|\n",
      "+---------------+---------------+\n",
      "|        bigdata|            226|\n",
      "|             ai|            225|\n",
      "|    datascience|            223|\n",
      "|machinelearning|            177|\n",
      "|      analytics|            153|\n",
      "+---------------+---------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import lower, col\n",
    "# Top 5 Hashtags der Top 10 User\n",
    "# a) reduziere Gesamtdaten auf die Menge der 10 Top User via Join\n",
    "df_top5_per_user=(df_top_user\n",
    "            # filter via join\n",
    "            .join(df,[df_top_user.user==df.user_name],how=\"left\")\n",
    "            # hashtags array in Zeilen Einträge exploden\n",
    "              # groupieren und counten by hashtag\n",
    "            .withColumn(\"hashtags\",explode(\"hashtags\"))\n",
    "            .withColumn(\"hashtags\", lower(col('hashtags')))\n",
    "            .groupBy(\"hashtags\").agg(f.count(\"hashtags\"))\n",
    "                  \n",
    "                  \n",
    "          #  # rückwärts sortieren\n",
    "            .sort(f.col(\"count(hashtags)\").desc())\n",
    "            # top 5 selectieren\n",
    "            .limit(5)\n",
    "            \n",
    "    )\n",
    "    \n",
    "df_top5_per_user.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb68e6c",
   "metadata": {},
   "source": [
    " ### Top 10 Influencer (User mit #BigData-tweets mit den meisten Followern) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79b5113b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 28:======================================================> (37 + 1) / 38]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+\n",
      "|      user_name|follower|\n",
      "+---------------+--------+\n",
      "|     KirkDBorne|  371809|\n",
      "| Ronald_vanLoon|  294773|\n",
      "|         rwang0|  140016|\n",
      "|     sallyeaves|  132261|\n",
      "|    ipfconline1|  128732|\n",
      "| SpirosMargaris|  126185|\n",
      "|DataScienceDojo|  116258|\n",
      "|    IainLJBrown|  113804|\n",
      "|      mvollmer1|  100366|\n",
      "|      JimMarous|   96994|\n",
      "+---------------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_top_influencer=(df\n",
    "                .groupBy(\"user_name\")\n",
    "                .agg(\n",
    "                    f.max(\"user_follower_count\").alias(\"follower\")\n",
    "                    )\n",
    "                .orderBy(f.col(\"follower\").desc())\n",
    "                .limit(10)\n",
    "                )\n",
    "df_top_influencer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e47a7c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+----+-----+\n",
      "|      user_name|follower|user|total|\n",
      "+---------------+--------+----+-----+\n",
      "|     KirkDBorne|  371809|null| null|\n",
      "| Ronald_vanLoon|  294773|null| null|\n",
      "|         rwang0|  140016|null| null|\n",
      "|     sallyeaves|  132261|null| null|\n",
      "|    ipfconline1|  128732|null| null|\n",
      "| SpirosMargaris|  126185|null| null|\n",
      "|DataScienceDojo|  116258|null| null|\n",
      "|    IainLJBrown|  113804|null| null|\n",
      "|      mvollmer1|  100366|null| null|\n",
      "|      JimMarous|   96994|null| null|\n",
      "+---------------+--------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Top 10 Influencer mit den meisten tweets\n",
    "df_withRetweets=(df_top_influencer\n",
    "            # filter via join auf die Top 10 Influencer\n",
    "            .join(df_top_user, [df_top_influencer.user_name==df_top_user.user],how=\"left\")\n",
    "            .orderBy(f.col(\"follower\").desc())\n",
    "            .limit(10)\n",
    "            .orderBy(f.col(\"total\").desc())     \n",
    "           \n",
    "            \n",
    "    )\n",
    "\n",
    "df_withRetweets.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13676250",
   "metadata": {},
   "source": [
    "### Aufgabe der Titel noch fehlt( Filter nach Location der TOP 10 User, Top  Hashtag pro Location)  IN SQL oder Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1d4233",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql=\"\"\"\n",
    "SELECT user_location, word, total FROM (\n",
    "    SELECT user_location, word, total, rank() over(partition by a.user_location order by a.total desc) as rank FROM\n",
    "    (\n",
    "        SELECT user_location,word, count(*) as total\n",
    "        FROM tweets\n",
    "        LATERAL VIEW explode(tweets.hashtags) tweets as word\n",
    "        WHERE word not in (\"BigData\",\"bigdata\",\"\")\n",
    "        AND tweets.user_location IN\n",
    "        (SELECT t.user_location\n",
    "                FROM (\n",
    "                         SELECT user_location, count(*) as total from tweets\n",
    "                         WHERE user_location not in (\"\",\"null\",\"REMOTE\",\"Earth\")\n",
    "                         GROUP BY user_location\n",
    "                         ORDER BY total DESC LIMIT 10\n",
    "                     ) as t\n",
    "        )\n",
    "        GROUP BY user_location, word\n",
    "        ) as a\n",
    "    ) as b\n",
    "WHERE rank=1\n",
    "ORDER BY total DESC LIMIT 10;\n",
    "\"\"\"\n",
    "\n",
    "df.registerTempTable(\"tweets\")\n",
    "\n",
    "df_result = spark.sql(sql)\n",
    "df_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f14e0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3=(df\n",
    "    .select(\"user_location\")\n",
    "    .where(~f.col(\"user_location\").isin(\"\",\"null\",\"REMOTE\",\"Earth\"))\n",
    "    .groupBy(\"user_location\")\n",
    "    .count()\n",
    "    .withColumnRenamed(\"count\",\"location_total\")\n",
    "    .orderBy(f.col(\"location_total\").desc())\n",
    "    .limit(10)\n",
    "    )\n",
    "\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb05cf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4=(df\n",
    "    .select(\"user_location\",\"hashtag\")\n",
    "    .withColumn(\"singletag\",f.explode(f.col(\"hashtag\")))\n",
    "    .groupBy(\"user_location\",\"singletag\")\n",
    "    .count()\n",
    "    .withColumnRenamed(\"count\",\"tags_total\")\n",
    "    )\n",
    "\n",
    "df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132149f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5=(df3.alias(\"a\")\n",
    "    .join(f.broadcast(df4.alias(\"b\")),[df3.user_location==df4.user_location],how=\"left\")\n",
    "    .select(\"a.user_location\",\"a.location_total\",\"b.singletag\",\"b.tags_total\")\n",
    "    .withColumn(\"rank\",f.row_number().over(Window.partitionBy(\"a.user_location\").orderBy(f.col(\"b.tags_total\").desc())))\n",
    "    .filter(f.col(\"rank\")==1)\n",
    "    .sort(f.col(\"location_total\").desc())\n",
    "    .limit(100)\n",
    "    )\n",
    "df5.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17094ce",
   "metadata": {},
   "source": [
    "## Schreiben der Daten nach Parquet und Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f44c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer_parquet=(df\n",
    "                .write.partitionBy(\"tweet_language\")\n",
    "                .mode(\"overwrite\")\n",
    "                .format(\"parquet\")\n",
    "                .save(\"s3a://solution/twitter_parquet\")\n",
    "            )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e819843c",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer_delta=(df\n",
    "                #.where(f.col(\"tweet_language\").isin(\"en\",\"de\"))\n",
    "                .write.partitionBy(\"tweet_language\")\n",
    "                .mode(\"overwrite\")\n",
    "                .format(\"delta\")\n",
    "                .save(\"s3a://solution/twitter_delta\")\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a081f82e",
   "metadata": {},
   "source": [
    "## Delta History and Time Travel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f34013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Delta file in s3 into Delta Table Object\n",
    "dt = DeltaTable.forPath(spark, \"s3a://solution/twitter_delta\")\n",
    "dt.toDF().show(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7862d91",
   "metadata": {},
   "source": [
    "#### Time Travel Aufgabe\n",
    "1. Excecute write to delta several times and check how the history adds new entries  \n",
    "2. Load one of the versions and check all the `tweet_language` (via distinct().show())\n",
    "3. write again delta with activated filter to write only out the `tweet_language` `en` and `de` \n",
    "4. confirm in the history log that less files were written out\n",
    "5. Load the latest version and check that only two entries are available\n",
    "6. Load older versions and confirm that there are still all data available\n",
    "7. Revert old version to latest again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5127f540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the metadata for the full history of the table\n",
    "fullHistoryDF = dt.history()    \n",
    "\n",
    "# get the metadata for the last operation\n",
    "lastOperationDF = dt.history(1) \n",
    "\n",
    "fullHistoryDF.select(\"version\",\"readVersion\",\"timestamp\",\"userId\",\"operation\",\"operationParameters\",\"operationMetrics\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92161ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load latest delta version\n",
    "df_timetravel = spark.read.format(\"delta\").load(\"s3a://solution/twitter_delta\")\n",
    "\n",
    "df_timetravel.select(\"tweet_language\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e31aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load specific historic version\n",
    "df_timetravel = spark.read.format(\"delta\").option(\"versionAsOf\", 2).load(\"s3a://solution/twitter_delta\")\n",
    "\n",
    "df_timetravel.select(\"tweet_language\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e561fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write old version back as latest\n",
    "f_timetravel = (spark\n",
    "                .read.format(\"delta\").option(\"versionAsOf\", 0).load(\"s3a://solution/twitter_delta\")\n",
    "                .write.partitionBy(\"tweet_language\").mode(\"overwrite\").format(\"delta\").save(\"s3a://solution/twitter_delta\")\n",
    "               )\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be4272d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdb31e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
